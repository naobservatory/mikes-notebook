---
title: "Basic detection theory"
description: | 
  Notes exploring the factors that determine our ability to perform _basic detection_ in a WWTP setting. 
  By basic detection, I mean detection based on seeing a sufficient number of distinguishing reads from the pathogen, rather than from a spatiotemporal pattern such as exponential growth.
author:
  - name: Michael R. McLaren
    url: {}
categories:
  - theory
bibliography: ../../_references.bib
# date: 2023-02-06
date: "`r format(Sys.time(), '%Y-%m-%d')`"
draft: false
output:
  distill::distill_article:
    self_contained: false
    dev: svg
    toc: true
    code_folding: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  autodep = TRUE,
  cache.comments = FALSE,
  dpi = 300,
  include = TRUE
)
```

```{r}
library(tidyverse)
library(fs)
library(here)

# plotting helpers
library(cowplot)
library(patchwork)
library(ggbeeswarm)

theme_set(theme_cowplot())
```

<!-- Links

- [Initial Overleaf modeling doc](https://www.overleaf.com/project/62ea87d86acf9339a72ed433)
- [Threat portfolio report](https://docs.google.com/document/d/1_qjSuCUBAKBBVV04FzEdt4XYZB6qZpCu1hs27YdVfLs/)

-->

# Model and approximate dynamics

General assumptions

* Local outbreak --- cases come in from elsewhere (or a local release), then spread locally
* Local wastewater treatment plant (WWTP)
* Assume that the WWTP is a good proxy for the local population
* Consider the exponential phase of the outbreak. 

Thus we need to model

* exponential growth (possibly noisy)
* shedding (possibly noisy)
* transport --- possible delay, noise
* sample collection
* measurement via qPCR, amplicon sequencing, metagenomic sequencing


Even under the simplified scenario of a local outbreak and monitoring system (closed system), there is still complication from stochasticity and complex disease time courses.
To make progress we need to look for ways to approximate the dynamics that still gives an accurate picture for the purposes of detection.
Goal of this section is to explain when and why we can approximate the dynamics of cumulative infections, active infections, and the amount of collected NA as growing deterministically and exponentially.

### SEIR model

Define the SEIR model as
\begin{align}
  \frac{dS}{dt} &= -\beta S I \\
  \frac{dE}{dt} &= \beta S I - \theta E \\
  \frac{dI}{dt} &= \theta E - \gamma I \\
  \frac{dR}{dt} &= \gamma I
\end{align}
where $S$ is the number of susceptible, $E$ is the number of exposed, $I$ is the number of infected, and $R$ is the number of recovered; $\beta$ is the transmission rate, $\theta$ is the incubation rate, and $\gamma$ is the recovery rate.
(This parameterization is from @diekmann2012math.)
Early in the pandemic, we can take $S \approx 1$ and $R \approx 0$, giving the approximate system
\begin{align}
  \frac{dE}{dt} &= - \theta E + B I \\
  \frac{dI}{dt} &= \theta E - \gamma I,
\end{align}
(where $B = \beta N$)
which converges to exponential growth at rate given by
\begin{align}
  r = \frac{-(\theta + \gamma) + \sqrt{(\theta - \gamma)^2 + 4 \theta B}}{2}.
\end{align}
(given by the leading eigenvector $\lambda_1$ of the Jacobian matrix) (see @ma2020esti).
We can solve for the equilibrium ratio of exposed to infected, $E/I$, by solving for $d/dt(E/I) = 0$; this gives
\begin{align}
  \frac{E}{I} 
  &= \frac{-(\theta - \gamma) + \sqrt{(\theta - \gamma)^2 + 4 \theta B}}{2 \theta}
\\&= \frac{r + \gamma}{\theta}.
\end{align}
(Alternatively, we could get the equilibrium ratio from the leading eigenvector of the Jacobian; see below).

For $\theta (B - \gamma) \ll (\theta + \gamma)^2$, we can approximate $r$ as
\begin{align}
  r \approx \frac{\theta (\theta - \gamma)}{\theta + \gamma},
\end{align}
which is consistent with the results of @heng2020theaa.
@heng2020theaa focus on an approximation in which the SEIR model's dynamics are approximately the same as the corresponding SIR model, but scaled by a factor of $\alpha = \theta / (\theta + \gamma)$.

More explicitly, the eigenvalues are ([Wolfram Alpha](https://www.wolframalpha.com/input?i=eigenvectors+%7B%7B-theta%2C+beta%7D%2C%7Btheta%2C+-gamma%7D%7D))
\begin{align}
  \lambda_1 &= \frac{-(\theta + \gamma) + \sqrt{(\theta - \gamma)^2 + 4 \theta B}}{2} \\
  \lambda_2 &= \frac{-(\theta + \gamma) - \sqrt{(\theta - \gamma)^2 + 4 \theta B}}{2}.
\end{align}
The eigenvectors are
\begin{align}
  v_1 &= \left(\frac{-(\theta - \gamma) + \sqrt{(\theta - \gamma)^2 + 4 \theta B}}{2\theta}, 1 \right) \\
  v_2 &= \left(\frac{-(\theta - \gamma) - \sqrt{(\theta - \gamma)^2 + 4 \theta B}}{2\theta}, 1 \right).
\end{align}
I believe the (exponential) rate of convergence to the exponential quasi-equilibrium is given by the difference $\lambda_1 - \lambda_2$, 
\begin{align}
  \lambda_1 - \lambda_2 = \sqrt{(\theta - \gamma)^2 + 4 \theta B}.
\end{align}


TODO

- [ ] Reparameterize the below to be in numbers rather than percentages

```{r}
# modified from http://epirecip.es/epicookbook/chapters/seir/r_desolve

# Function to return derivatives of SEIR model
seir_ode <- function(t,Y,par){
  S <- Y[1]
  E <- Y[2]
  I <- Y[3]
  R <- Y[4]
  
  beta <- par[1]
  theta <- par[2]
  gamma <- par[3]
  
  dYdt <- vector(length=3)
  dYdt[1] <- -beta*I*S
  dYdt[2] <- beta*I*S - theta*E
  dYdt[3] <- theta*E - gamma*I
  
  return(list(dYdt))
}

# Set parameter values
theta <- 1/5
gamma <- 1/10
# Set beta from the basic reproduction number, since R0 = beta * N / gamma
R0 <- 3
N <- 1e6
beta <- R0 * gamma / N

# Set initial conditions
E0 <- 1
I0 <- 0
init <- c(N - E0 - I0, E0, I0)
t <- seq(0, 250)
par <- c(beta, theta, gamma)
# Solve system using lsoda
sol <- deSolve::lsoda(init, t, seir_ode, par) %>% 
  as_tibble %>%
  set_names(c("time", "S","E","I")) %>%
  mutate(R = N - (S + E + I))

r_pred = (-(theta + gamma) + sqrt((theta - gamma)^2 + 4 * theta * beta * N)) / 2

x <- sol %>%
  mutate(
    across(everything(), as.numeric),
    I_pred = pmin(exp(r_pred * time), N)
  ) %>%
  pivot_longer(-time)
```

```{r}
x %>%
  filter(time <= 200, name != 'I_pred') %>%
  ggplot(aes(time, value, color = name)) +
  geom_line() +
  labs(y = "Number of individuals")
```

```{r}
x %>%
  filter(time <= 200, name != 'S') %>%
  ggplot(aes(time, value, color = name)) +
  scale_y_log10(limits = c(1, N), breaks = c(1, 1e2, 1e4, 1e6)) +
  geom_line() +
  labs(y = "Number of individuals")
```

Note, I haven't calculated the constant for the asymptotic number of infected. But we can see that we have the correct asymptotic growth rate.

Let's also check our prediction for the asymptotic ratio of $E/I$ in the exponential regime,

```{r}
ratio_pred <- (-(theta - gamma) + sqrt((theta - gamma)^2 + 4 * theta * beta * N))/(2 * theta)
x %>%
  pivot_wider(time) %>%
  ggplot(aes(time, E / I)) +
  scale_y_log10() +
  geom_hline(yintercept = ratio_pred, color = 'grey', 
             size = 0.8, linetype = 2) +
  geom_line() +
  labs(y = "E / I")
```

The simulation starts with 1 exposed individual; hence, the ratio of exposed to infected starts high but quickly drops to the quasi-equilibrium ratio of 1, then eventually starts to decline as the susceptible population declines and causes a drop in the rate of new infections, before appearing to asymptote at a positive value in the waning days of the epidemic.

Possible additions

- [ ] Expression for the peak susceptible, and total infected


```{r, eval = F}
# from https://cran.r-project.org/web/packages/odin/vignettes/discrete.html
library(odin)

## Core equations for transitions between compartments:
update(S) <- S - beta * S * I / N
update(E) <- S - beta * S * I / N
update(I) <- I + beta * S * I / N - gamma * I
update(R) <- R + gamma * I

## Total population size (odin will recompute this at each timestep:
## automatically)
# N <- S + I + R
N <- 1

## Initial states:
initial(S) <- S_ini # will be user-defined
initial(E) <- E_ini # will be user-defined
initial(I) <- I_ini # will be user-defined
initial(R) <- 0

## User defined parameters - default in parentheses:
S_ini <- user(1 - 1e-4)
E_ini <- user(0)
I_ini <- user(1e-4)
beta <- user(0.2)
theta <- user(0.1)
gamma <- user(0.1)
```


### Shedding, and more complex disease time courses

A main issue left unaddressed by the SEIR model is when shedding occurs.
Also, we might not be that happy with the exponential waiting times for the disease stages.
We can address these issues by considering a more complex disease time course which explicitly includes shedding. 
To remain tractable in the present framework, what is important is that it is still safe to assume that we quickly reach approximate exponential growth in the number of infected and that daily shedding is proportional to the number of infected in this asymptotic regime.

Questions to investigate here include

* what are the asymptotic growth rates and shedding proportionality constants?
* related: how does the disease time course affect growth rate and the proportionality constant between shedding and the number of infected?
* what is the time scale for reaching the asymptotic exponential growth regime?

Qualitative model: 

```{r, fig.dim = c(4, 1.5) * 1.5, include = TRUE}
tribble(
  ~name, ~x, ~xend,
  'symptomatic', 8, 16,
  'infectious', 5, 12,
  'shedding', 3, 20
) %>%
  mutate(
    across(name, fct_inorder),
    y = name %>% as.integer
  ) %>%
  ggplot(aes(x = x, xend = xend, color = name, label = name)) +
  theme_minimal_hgrid() +
  theme(legend.position = 'none') +
  scale_x_continuous(breaks = c(0, 10, 20)) +
  expand_limits(x = 0, xend = 20) +
  scale_y_discrete(limits = rev) +
  scale_color_brewer(type = 'qual', palette = 2) +
  labs(x = 'time since exposed (days)', y = NULL) +
  geom_segment(aes(y = name, yend = name), size = 1)
```

Quantitative model would be to have each of these represented by a continuous function of time.
Could further have a (joint) distribution of functions.

Can tackle this by considering the impact on the exponential growth rate and the scaling factors relating exposed to infected to recovered to shedding amounts.
A key interest is the relationship between cumulative exposures (i.e. infected) to currently infectious to shedding.


At large enough numbers, we should be able to approximate the effect of these complex dynamics through effective parameters linking the number of infected, number of infectious, and current rate of shedding; all will be growing asymptotically exponentially.
There are various plausible models here.
As a simple illustration, suppose that....

<!--
For example, consider the following classes,

- $S$: Susceptible: Not yet exposed
- $x_{0, 0}$: Exposed, not yet shedding or infectious
- $x_{1, 0}$: Exposed, Shedding, non-infectious
- $x_{0, 1}$: Exposed, Non-shedding, infectious
- $x_{1, 1}$: Exposed, Shedding, infectious
- $R$: Recovered

We can define a transition matrix between these classes
-->


### Stochasticity

Establishment size above which an outbreak is approximately guaranteed and can ignore stochastic effects; what is this for real epidemics? I'm used to models where the point at which success is assured and which can ignore stochastic effects are the same; is that also true with very skewed offspring/infection distributions?


### Origination of the outbreak

Suppose new infections coming in at rate $M$.
Can understand the point at which we have a deterministic local outbreak and can ignore new imports, based on what we know about this from evolutionary theory.

## Shedding

Simple model used in the modeling literature links cases with shedding by supposing that there is a fixed distribution of shedding as a function of time post symptom onset.
The idea that everyone sheds the same can be misleading; and is particularly sketchy in cases where there is substantial heterogeneity in how people respond to the disease.
Still, it is a fair starting point and useful for illustration.

In our case, we're interested in shedding that occurs even without clinical presentation, so I'll consider that there is a shedding distribution $s(t)$ indicating the amount of shedding from a person $t$ days after infection (exposure).
If the number of exposures is $E(t)$, then the total shedding $S(t)$ at time $t$ is given by the convolution of $E$ with $s$,
\begin{align}
  S(t) = \int_{-\infty}^{\infty} E(u) \; s(t - u) \; du.
\end{align}
Suppose the number of exposed grows exponentially at rate $r$, $E(t) = E_0 e^{rt}$.
We know that $s(t) = 0$ for $t \le 0$, and we further expect $s$ to initially increase, peak at some time $t^*$, and then decay to 0.
Let's take it to be Gamma distributed with a mean of 10 days and a CV of 0.5,

```{r}
shedding <- tibble(mean = 10, cv = 0.5) %>%
  mutate(
    sd = cv * mean,
    var = sd^2,
    scale = var / mean,
    shape = mean / scale
  ) %>%
  crossing(t = seq(0, 30, by = 0.1)) %>%
  mutate(
    density = dgamma(x = t, shape = shape, scale = scale)
  )
shedding %>%
  ggplot(aes(t, density)) +
  labs(x = 'Time (days) after exposure', y = 'Density of shedding') +
  geom_line()
```

Here, total area under the curve is 1, so we can think of the units as in terms of the total shedding over a single infection.

Now let's view the total shedding rate over time, starting from a single exposed and under a growth rate of $r = 0.1$,

\begin{align}
  S(t) = \int_{0}^{t} e^{r u} \; s(t - u) \; du.
\end{align}

```{r}
total_shedding <- function(t, r, gamma_mean, gamma_cv) {
  scale = gamma_cv^2 * gamma_mean
  shape = gamma_cv^(-2)
  f <- function(u) { exp(r * u) * dgamma(t - u, shape = shape, scale = scale) }
  integrate(f, lower = 0, upper = t)
}
# total_shedding(10, 0.1, 10, 0.5)$value
```

```{r}
r <- 0.1
x <- tibble(t = seq(0, 100)) %>%
  mutate(
    Exposed = exp(r * t),
    Shed = map(t, total_shedding, r = r, gamma_mean = 10, gamma_cv = 0.5),
    across(Shed, map_dbl, 'value')
  )
```

```{r}
x %>%
  pivot_longer(-t) %>%
  ggplot(aes(t, value, color = name)) +
  scale_y_log10(limits = c(1e-2, NA)) +
  geom_line()
```

After an initial lag, the shedding rate too grows exponentially, as expected.

We can attempt a rough calculation of the ratio ratio $S/E$:

- The total shedding amount over an infection is 1
- The bulk of the shedding of the infected is roughly between days 5 and 15;
- So $S(t)$ is roughly the sum of contributions from those infected over 5-15 days in the past, and each of them are contributing 1/10. (The mean $\pm$ the standard deviation)
- 5-15 days in the past is (in this case) a period of $-1/2r$ to $-3/2r$; the integral of $e^{rt}$ over that period is $1/(10 r) e^{rt} (e^{-1/2} - e^{-3/2})$
- This suggests a ratio of $(e^{-1/2} - e^{-3/2})/(10 r)$ or `r ((exp(-1/2) - exp(-3/2)) / (10 * r)) %>% round(2)`.

```{r}
ratio_guess <- (exp(-1/2) - exp(-3/2)) / (10 * r)
x %>%
  mutate(Guess = Exposed * ratio_guess) %>%
  pivot_longer(-t) %>%
  ggplot(aes(t, value, color = name)) +
  scale_y_log10(limits = c(1e-2, NA)) +
  geom_line()
```


## Transport and sample collection

NOTE: much of this is structurally the same as for shedding


Basic model: see Will's pdf as starting point.
Important complications --- shedding will be taking place all over the city, at different times, randomly.
The signal from a given shedding event may spread out over days.
But in the regimes where we can detect things, guess that we can capture these complications by an effective recovery parameter.

Complications: rain and/or other large perturbations to the WW system.
Can we capture this with our main noise parameter?

As a way to get intuition, suppose that the amount being shed into the WW system at time $t$ is given by $S(t)$ and that the fraction of NA collected in the sample a time $t$ after it is shed is given by $w(t)$.
The amount of NA collected at time $t$, $W(t)$, is given by the convolution of $S$ and $w$,
\begin{align}
  W(t) = \int_{-\infty}^{\infty} S(u) \; w(t - u) \; du.
\end{align}
Our main interest is in $W(t)$ when the shedding amount grows exponentially at rate $r$, $S(t) = S_0 e^{rt}$.
We know that $w(t) = 0$ for $t \le 0$, and we further expect $w$ to initially increase, peak at some time $t^*$, and then decay to 0.
For WWTPs, I expect the peak to be on the order of a day or less ($t^* \lesssim 1$), and the timescale of the decay to be on the order of days, e.g. the half life around 0.5 to 3 days, but I haven't looked into this carefully and this intuition is mostly based on the Helsinki study.

Intuition for shape of $w(t)$:

* Could try modeling as flow with turbulent diffusion -> model concentration with Brownian motion with a constant 'drift' term representing the average flow rate. Should be fairly easy to simulate and understand the timescale and tail shape with formal solution or heuristics for this approximation.
* Complication is that the transit length will vary by shedding location, and that the flow rate will vary by shedding location as well as along the transport path. The diffusion rate would also vary.
* May be a long tail of arrival due to low-turnover reservoirs in the system where things might hang out; however, I don't expect these to be very important contributors in an exponential growth scenario.
* Perturbances such as a rain storm may also be important.

It still seems useful to consider the simple model where there is a given transit length $l$.
Brownian motion at rate $\sigma$ with drift at rate $v$, where the transit length is $l$, gives^[https://www.randomservices.org/random/brown/Drift.html] that the concentration at time $t$ is normally distributed with mean $vt$ and standard deviation $\sigma \sqrt{t}$.
The concentration at the sampling point (location $l$ from the shedding location) is
\begin{align}
  w_l(t) = \frac{1}{\sqrt{2 \pi \sigma^2 t}} \exp \left[\frac{(l - v t)^2}{2\sigma^2 t} \right].
\end{align}
The concentration peak occurs at $t^* = l / v$.

TODO: Determine the decay behavior

```{r, include = T}
v = 1
l = 1
sigma = 0.2
t_star = l/v
sigma_star = sigma * sqrt(t_star)

tibble(
  t = seq(0, 2, by = 0.01)
) %>%
  mutate(
    w = dnorm(l, mean = v * t, sd = sigma * sqrt(t))
  ) %>%
  ggplot(aes(t, w)) +
  geom_line() +
  geom_vline(xintercept = t_star, color ='grey') +
  annotate(geom = 'segment', color = 'grey',
           x = t_star - sigma_star,
           xend = t_star + sigma_star,
           y = 1, yend = 1)
```

Note that the distribution is skewed, with more weight on the right side of the peak.

Let's now combine this with the shedding function $S(t)$ to consider the total amount of NA collected at $t$,

\begin{align}
  W(t) &= \int_{-\infty}^{\infty} S(u) \; w(t - u) \; du \\
       &= \int_{-\infty}^{\infty} S_0 \frac{1}{\sqrt{2 \pi \sigma^2 (t-u)}}
          \exp \left[ru + \frac{(l - v (t-u))^2}{2\sigma^2 (t-u)}\right] \; du
\end{align}

My guess is we can do an ok job of understanding the behavior with a heuristic approximation, where we consider the lag time $t^* = l/v$ and the spread at this time approximated by the standard deviation $\sigma \sqrt{t^*}$.

## Sample collection

# Detection methods

## qPCR

## Amplicon sequencing

## Metagenomic sequencing

# Detection under deterministic exponential growth

I will assume a very simple model, but which can represent more complex cases so long as the outbreak has has established, and reached the asymptotic exponential growth rate in the number of infections, cumulative infections, and shedding amount, by using 'effective parameters'.
Motivation: Mathematical simplicity for developing intuition; also, I assume that detection has a negligible chance of occurring before the outbreak reaches the exponential phase.

## Epidemic dynamics

Notation warning: Here, I'm taking $I$ to be the number of infections, which includes exposed but non-yet-infectious individuals.
In other words, this would be $E + I$ in the SEIR model.

Assume deterministic exponential growth of the current number of infected, $I(t)$, from some initial number $I_0 = I(0)$. 
The number of infected at time $t$ (measured in days) has derivative
\begin{align}
  \frac{dI}{dt} &= (b - d) I = r I,
\end{align}
where $r > 0$ is the exponential growth rate.
I further assume that $r$ can be broken into separate 'birth' and 'death' rates, $b$ and $d$, describing the rate at which infections beget new infections, and which infected individuals recover.
Under this model, the number of infections at time $t$ grows as 
\begin{align}
  I(t) &\approx I_0 e^{rt}.
\end{align}

The cumulative number of infection-days at day $t$ is
\begin{align}
  \int_0^t I(u)\;du 
    &= \int_0^t I_0 e^{ru}\;du
  \\&= \frac{I_0}{r} \left(e^{rt} - 1 \right) 
  \\&= \frac{1}{r} \left[I(t) - I(0) \right]
  \\&\approx \frac{1}{r} I(t) \quad \text{for $t \gg 1/r$}.
\end{align}

We can calculate the cumulative number of infections $C(t)$ at day $t$ by noting that new infections occur at rate $b I(t)$; hence
\begin{align}
  C(t) 
    &= \int_0^t b I(u)\;du 
  \\&= \int_0^t b I_0 e^{ru}\;du 
  \\&= \frac{b I_0}{r} \left(e^{rt} - 1 \right) 
  \\&= \frac{b}{r} \left[I(t) - I(0) \right] 
  \\&\approx \frac{b}{r} I(t) \quad \text{for $t \gg 1/r$}.
\end{align}
The cumulative number of infections is $b$ times the cumulative number of infection-days, and is approximately $b/r$ the number of current infections.
Note: Arguably, we should be adding the initial number of infected, $I_0$, in which case we get
\begin{align}
  C(t) 
    &= \int_0^t b I(u)\;du  + I_0
  \\&= \frac{b}{r} \left[I(t) - I_0 \right] + I_0
  \\&= \frac{b}{r} I(t) - \frac{d}{r} I_0.
  \\&\approx \frac{b}{r} I(t) \quad \text{for $I(t) \gg I_0$}.
\end{align}
I will typically assume $I_0$ is a tiny fraction of all current and cumulative infections in the regimes of interest, so this distinction shouldn't matter.
The condition $I(t) \gg I_0$ will be true for $t \gg 1/r$.

Define _cumulative incidence_ $c(t)$ as the fraction of the population that have been infected by time $t$,
\begin{align}
  c(t) 
    &= C(t) / N
  \\&\approx \frac{b}{r} i(t) \quad \text{for $t \gg 1/r$}.
\end{align}

How long is the exponential approximation good for? 
Suppose there is lasting immunity, such that each person can only be infected once.
In that case, we expect the rate of new infections per capita, $b$, to be in proportional to $1 - c(t)$; i.e., $b(t) = [1 - c(t)] b_0$.
The exponential approximation is good so long as $c(t) \ll 1$.
A good rule of thumb could be to take the point when $c(t) \approx 1/e$ as the end of the exponential phase.
This occurs when the currently infected fraction of the population $i(t)$ reaches $\tfrac{r}{b} e^{-1}$, or when $t \approx \tfrac{1}{r} [\log (\tfrac{r}{i_0 b}) - 1]$.
(Needs to be checked)

The peak of the epidemic (max value of $I$) will occur when $b$ decreases to $d$.
This occurs when $c(t) = 1 - \tfrac{d}{b_0} = \tfrac{r_0}{b_0}$.
The number of susceptible (those not infected) is $1 - c = d/b_0$, matching the standard SIR result (e.g. equation 4a [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7997702/)).

## Shedding and sequencing

Suppose the number of infected at time $t$ is then
\begin{align}
  I(t) &\approx I_0 e^{rt}.
\end{align}
where $I_0$ is the initial number of infected.
Let $N$ be the total population (assumed fixed) and $i(t) = I(t) / N$ be the fraction of the population that is infected.

Assume a constant amount of background shedding in each person regardless of infection status.
Suppose a background shedding rate of $s_0$ of background microbiome in each person, regardless of infection status.
Let $s$ be the relative rate of shedding of pathogen to background in an infected person, so that $s s_0$ is the rate of pathogen shedding in an infected person (note potential name clash with the fraction of the population that is susceptible).
Let $B$ be the relative measurement effiency of the pathogen to the background (MGS bias).
In the regime I consider below, where total threat reads are a small fraction of total reads, we can use $B$ to also account for the fact that perhaps only a small fraction of reads from the pathogen are useful for detecting it as a novel threat.

The expected fraction (i.e. proportion) of the threat in the MGS reads in a sample collected on day $t$ is
\begin{align}
E [ P(t) ]
  &\approx \frac{I(t) s s_0 B}{I(t) s s_0  B + N s_0 } 
\\&= \frac{I(t) s B}{I(t) s B + N } 
\\&= \frac{i(t) s B}{i(t) s B + 1 } 
\\&\approx i(t) s B.
\end{align}
The first approximation comes from treating $I(t)$ as constant over the 24h the sample is collected (more on this below).
The second approximation comes from assuming that the pathogen is always a small fraction of the reads ($i s B \ll 1$).
One way for this condition to arise is that it holds even for a sample from an infected individual or an entirely infected population (i.e., $s B \ll 1$).
I expect that to be true for subtle pathogens, but not necessary all pathogens (e.g., gastroenteric pathogens).
For these latter cases I expect $i(t) \ll 1$ and for the broader condition $i s B \ll 1$ to still hold.

To determine the absolute copy number or concentration of the pathogen in the sample, we'd need to say something about the overall shedding amount and water system. However, to analyze MGS detection, it is enough to consider the relative abundance as above.

The expected number of sequencing reads of the threat on day $t$ is
\begin{align}
E [ M(t) ] = E [ P(t) ] \cdot \mathcal M \approx i(t) s B \mathcal M.
\end{align}
where $\mathcal M$ is the total sequencing depth, which I'm treating as a value that is determined by the experimenter and is the same for each day.

More generally, if the sample is collected over a particular period of time $\Delta t$, we might want to model the pathogen contribution to the sample as an integral of shedding over that period,
\begin{align}
E [ M(t) ] 
  &= s B \mathcal M \int_{t-\Delta t}{t} i(u) \; du
\\&= s B \mathcal M \frac{1}{r} i_0 e^{rt} \left[1 - e^{-r \Delta t} \right]
\\&= s B \mathcal M \frac{1}{r} i(t) \left[1 - e^{-r \Delta t} \right].
\\&\approx i(t) (\Delta t) s B \mathcal M \quad \text{for $r \Delta t \ll 1$}.
\end{align}

## Detection

Warning: The below assumes that $r \ll 1$, so that we can ignore the growth in $i(t)$ over the sampling period when computing the contribution of pathogen to the sample.

Suppose there is a threshold number of reads (of the pathogen or the distinguishing subsequence), $M^*$, we need to see in a single day to detect the threat (e.g. 5).
I'll take $\Delta t = 1$ (24-h composite sampling) and suppose that $r \ll 1$, so that we can ignore growth over the sampling window (otherwise, just multiply the number of expected reads by the constant factor $(1 - e^r)/r$).
As a first approximation, let's ignore noise in shedding and sequencing measurement, and define detection as occurring when $E [M(t^*)] = M^*$.
Detection then occurs at $i(t) s B \mathcal M = M^*$ or
\begin{align}
  i(t^*) = \frac{M^*}{s B \mathcal M}.
\end{align}
In the exponential regime where $i(t) = i_0 e^{rt}$ and cumulative incidence is $c(t) \approx \tfrac{b}{r} i(t)$, we have that 
\begin{align}
  t^* = \frac{1}{r} \log \frac{M^*}{i_0 s B \mathcal M}
\end{align}
\begin{align}
  c(t^*) = \frac{b}{r} \cdot \frac{M^*}{s B \mathcal M}.
\end{align}

These results explain the pattern seen in the example simulations presented by Charlie Whittaker last December, in which we saw that the time of detection decreases logarithmically $\mathcal M$ and cumulative incidence at time of detection decreases as $1 / \mathcal M$.

Note that the detection threshold and efficiency appear together as $M^* / B$; this means we'll get the same answer whether we think of 

- $M^*$ is the number of reads hitting a particular identifying subsequence, and $B$ contains a factor $x < 1$ to account for the fact that only a fraction $x$ of the pathogen's reads hit the subsequence
- $M^*$ is the number of reads hitting the pathogen, and we increase it by a factor $1/x$ to account for the fact that only $x$ of the reads are useful for identification; $B$ does not contain the factor $x$.

However, when we start to consider noise in the read counts, we need to be more careful about this.

### Cumulative reads over a given period

What if it is sufficient to see $R^*$ reads cumulatively over the last $T$ days?
Call $Q$ the cumulative reads over the last $T$ days.
Then
\begin{align}
E [ Q(t) ] 
  &= s B \mathcal M \int_{t-T}{t} i(u) \; du
\\&= s B \mathcal M i(t) \frac{1}{r} \left[1 - e^{-r T} \right].
\end{align}
Setting $T = t$ corresponds to all reads since time $0$; in that case, we have that $E [ Q(t) ]$ equals $s B \mathcal M$ times the cumulative infection hours, or approximately
\begin{align}
E [ Q(t) ] 
  &\approx s B \mathcal M \frac{i(t)}{r}
  \\&\approx s B \mathcal M \frac{c(t)}{b}
\end{align}
Considering the factor $1 - e^{-rT}$ indicates that it is only the samples from the past few characteristic growth periods that contributes significantly to the total reads.

How much better do we do, detection wise, when we only require cumulative reads to reach $M^*$?
Detection now occurs at $E[Q(t)] = M^*$ or
\begin{align}
  i(t^*) &= \frac{r M^*}{s B \mathcal M} \\
  c(t^*) &= b \cdot \frac{M^*}{s B \mathcal M};
\end{align}
these are both a factor $r$ greater than before.
As expected, the effect on time is only logarithmic,
\begin{align}
  t^* = \frac{1}{r} \log \frac{r M^*}{i_0 s B \mathcal M}.
\end{align}
The effect overall is as if we increased any of $\mathcal M$, $s$, or $B$ by a factor of $1/r$.


# Connecting to empirical data

## Jeff's analysis of SARS2 in @rothman2021rnav

Jeff estimates [here](https://docs.google.com/document/d/1XVeLyc1Peh59mILGtie8bB2eC7Mke7Byq77tiqs7Ih8/edit#heading=h.dshnx0efqzl4) that the proportion of SARS2 reads in a sample in the @rothman2021rnav data is 3e-7 when 1e-3 of the catchment is infected.
That corresponds to $P = 3\cdot 10^{-7}$ when $i = 10^{-3}$, so that $sB = P(t) / i(t) = 3 \cdot 10^{-4} \approx 10^{-3.5}$.
We would therefore need to sequence a single sample to a depth of $10^{8.5}$ to see 10 reads of SARS2 when $10^{-4}$ of the population is infected.

(CHECK).
