[
  {
    "path": "posts/2023-02-06-basic-detection-theory/",
    "title": "Basic detection theory",
    "description": "Notes exploring the factors that determine our ability to perform _basic detection_ in a WWTP setting. \nBy basic detection, I mean detection based on seeing a sufficient number of distinguishing reads from the pathogen, rather than from a spatiotemporal pattern such as exponential growth.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-03-03",
    "categories": [
      "theory"
    ],
    "contents": "\n\nContents\nModel and approximate dynamics\nSEIR model\nShedding, and more complex disease time courses\nStochasticity\nOrigination of the outbreak\nShedding\nTransport and sample collection\nSample collection\n\nDetection methods\nqPCR\nAmplicon sequencing\nMetagenomic sequencing\n\nDetection under deterministic exponential growth\nEpidemic dynamics\nShedding and sequencing\nDetection\nCumulative reads over a given period\n\n\nConnecting to empirical data\nJeff’s analysis of SARS2 in Rothman et al. (2021)\n\n\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(here)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n\n\nModel and approximate dynamics\nGeneral assumptions\nLocal outbreak — cases come in from elsewhere (or a local release), then spread locally\nLocal wastewater treatment plant (WWTP)\nAssume that the WWTP is a good proxy for the local population\nConsider the exponential phase of the outbreak.\nThus we need to model\nexponential growth (possibly noisy)\nshedding (possibly noisy)\ntransport — possible delay, noise\nsample collection\nmeasurement via qPCR, amplicon sequencing, metagenomic sequencing\nEven under the simplified scenario of a local outbreak and monitoring system (closed system), there is still complication from stochasticity and complex disease time courses.\nTo make progress we need to look for ways to approximate the dynamics that still gives an accurate picture for the purposes of detection.\nGoal of this section is to explain when and why we can approximate the dynamics of cumulative infections, active infections, and the amount of collected NA as growing deterministically and exponentially.\nSEIR model\nDefine the SEIR model as\n\\[\\begin{align}\n  \\frac{dS}{dt} &= -\\beta S I \\\\\n  \\frac{dE}{dt} &= \\beta S I - \\theta E \\\\\n  \\frac{dI}{dt} &= \\theta E - \\gamma I \\\\\n  \\frac{dR}{dt} &= \\gamma I\n\\end{align}\\]\nwhere \\(S\\) is the number of susceptible, \\(E\\) is the number of exposed, \\(I\\) is the number of infected, and \\(R\\) is the number of recovered; \\(\\beta\\) is the transmission rate, \\(\\theta\\) is the incubation rate, and \\(\\gamma\\) is the recovery rate.\n(This parameterization is from Diekmann, Heesterbeek, and Britton (2012).)\nEarly in the pandemic, we can take \\(S \\approx 1\\) and \\(R \\approx 0\\), giving the approximate system\n\\[\\begin{align}\n  \\frac{dE}{dt} &= - \\theta E + B I \\\\\n  \\frac{dI}{dt} &= \\theta E - \\gamma I,\n\\end{align}\\]\n(where \\(B = \\beta N\\))\nwhich converges to exponential growth at rate given by\n\\[\\begin{align}\n  r = \\frac{-(\\theta + \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2}.\n\\end{align}\\]\n(given by the leading eigenvector \\(\\lambda_1\\) of the Jacobian matrix) (see Ma (2020)).\nWe can solve for the equilibrium ratio of exposed to infected, \\(E/I\\), by solving for \\(d/dt(E/I) = 0\\); this gives\n\\[\\begin{align}\n  \\frac{E}{I}\n  &= \\frac{-(\\theta - \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2 \\theta}\n\\\\&= \\frac{r + \\gamma}{\\theta}.\n\\end{align}\\]\n(Alternatively, we could get the equilibrium ratio from the leading eigenvector of the Jacobian; see below).\nFor \\(\\theta (B - \\gamma) \\ll (\\theta + \\gamma)^2\\), we can approximate \\(r\\) as\n\\[\\begin{align}\n  r \\approx \\frac{\\theta (\\theta - \\gamma)}{\\theta + \\gamma},\n\\end{align}\\]\nwhich is consistent with the results of Heng and Althaus (2020).\nHeng and Althaus (2020) focus on an approximation in which the SEIR model’s dynamics are approximately the same as the corresponding SIR model, but scaled by a factor of \\(\\alpha = \\theta / (\\theta + \\gamma)\\).\nMore explicitly, the eigenvalues are (Wolfram Alpha)\n\\[\\begin{align}\n  \\lambda_1 &= \\frac{-(\\theta + \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2} \\\\\n  \\lambda_2 &= \\frac{-(\\theta + \\gamma) - \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2}.\n\\end{align}\\]\nThe eigenvectors are\n\\[\\begin{align}\n  v_1 &= \\left(\\frac{-(\\theta - \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2\\theta}, 1 \\right) \\\\\n  v_2 &= \\left(\\frac{-(\\theta - \\gamma) - \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2\\theta}, 1 \\right).\n\\end{align}\\]\nI believe the (exponential) rate of convergence to the exponential quasi-equilibrium is given by the difference \\(\\lambda_1 - \\lambda_2\\),\n\\[\\begin{align}\n  \\lambda_1 - \\lambda_2 = \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}.\n\\end{align}\\]\nTODO\nReparameterize the below to be in numbers rather than percentages\n\n\nShow code\n\n# modified from http://epirecip.es/epicookbook/chapters/seir/r_desolve\n\n# Function to return derivatives of SEIR model\nseir_ode <- function(t,Y,par){\n  S <- Y[1]\n  E <- Y[2]\n  I <- Y[3]\n  R <- Y[4]\n  \n  beta <- par[1]\n  theta <- par[2]\n  gamma <- par[3]\n  \n  dYdt <- vector(length=3)\n  dYdt[1] <- -beta*I*S\n  dYdt[2] <- beta*I*S - theta*E\n  dYdt[3] <- theta*E - gamma*I\n  \n  return(list(dYdt))\n}\n\n# Set parameter values\ntheta <- 1/5\ngamma <- 1/10\n# Set beta from the basic reproduction number, since R0 = beta * N / gamma\nR0 <- 3\nN <- 1e6\nbeta <- R0 * gamma / N\n\n# Set initial conditions\nE0 <- 1\nI0 <- 0\ninit <- c(N - E0 - I0, E0, I0)\nt <- seq(0, 250)\npar <- c(beta, theta, gamma)\n# Solve system using lsoda\nsol <- deSolve::lsoda(init, t, seir_ode, par) %>% \n  as_tibble %>%\n  set_names(c(\"time\", \"S\",\"E\",\"I\")) %>%\n  mutate(R = N - (S + E + I))\n\nr_pred = (-(theta + gamma) + sqrt((theta - gamma)^2 + 4 * theta * beta * N)) / 2\n\nx <- sol %>%\n  mutate(\n    across(everything(), as.numeric),\n    I_pred = pmin(exp(r_pred * time), N)\n  ) %>%\n  pivot_longer(-time)\n\n\n\n\nShow code\n\nx %>%\n  filter(time <= 200, name != 'I_pred') %>%\n  ggplot(aes(time, value, color = name)) +\n  geom_line() +\n  labs(y = \"Number of individuals\")\n\n\n\n\n\nShow code\n\nx %>%\n  filter(time <= 200, name != 'S') %>%\n  ggplot(aes(time, value, color = name)) +\n  scale_y_log10(limits = c(1, N), breaks = c(1, 1e2, 1e4, 1e6)) +\n  geom_line() +\n  labs(y = \"Number of individuals\")\n\n\n\nNote, I haven’t calculated the constant for the asymptotic number of infected. But we can see that we have the correct asymptotic growth rate.\nLet’s also check our prediction for the asymptotic ratio of \\(E/I\\) in the exponential regime,\n\n\nShow code\n\nratio_pred <- (-(theta - gamma) + sqrt((theta - gamma)^2 + 4 * theta * beta * N))/(2 * theta)\nx %>%\n  pivot_wider(time) %>%\n  ggplot(aes(time, E / I)) +\n  scale_y_log10() +\n  geom_hline(yintercept = ratio_pred, color = 'grey', \n             size = 0.8, linetype = 2) +\n  geom_line() +\n  labs(y = \"E / I\")\n\n\n\nThe simulation starts with 1 exposed individual; hence, the ratio of exposed to infected starts high but quickly drops to the quasi-equilibrium ratio of 1, then eventually starts to decline as the susceptible population declines and causes a drop in the rate of new infections, before appearing to asymptote at a positive value in the waning days of the epidemic.\nPossible additions\nExpression for the peak susceptible, and total infected\n\n\nShow code\n\n# from https://cran.r-project.org/web/packages/odin/vignettes/discrete.html\nlibrary(odin)\n\n## Core equations for transitions between compartments:\nupdate(S) <- S - beta * S * I / N\nupdate(E) <- S - beta * S * I / N\nupdate(I) <- I + beta * S * I / N - gamma * I\nupdate(R) <- R + gamma * I\n\n## Total population size (odin will recompute this at each timestep:\n## automatically)\n# N <- S + I + R\nN <- 1\n\n## Initial states:\ninitial(S) <- S_ini # will be user-defined\ninitial(E) <- E_ini # will be user-defined\ninitial(I) <- I_ini # will be user-defined\ninitial(R) <- 0\n\n## User defined parameters - default in parentheses:\nS_ini <- user(1 - 1e-4)\nE_ini <- user(0)\nI_ini <- user(1e-4)\nbeta <- user(0.2)\ntheta <- user(0.1)\ngamma <- user(0.1)\n\n\nShedding, and more complex disease time courses\nA main issue left unaddressed by the SEIR model is when shedding occurs.\nAlso, we might not be that happy with the exponential waiting times for the disease stages.\nWe can address these issues by considering a more complex disease time course which explicitly includes shedding.\nTo remain tractable in the present framework, what is important is that it is still safe to assume that we quickly reach approximate exponential growth in the number of infected and that daily shedding is proportional to the number of infected in this asymptotic regime.\nQuestions to investigate here include\nwhat are the asymptotic growth rates and shedding proportionality constants?\nrelated: how does the disease time course affect growth rate and the proportionality constant between shedding and the number of infected?\nwhat is the time scale for reaching the asymptotic exponential growth regime?\nQualitative model:\n\n\nShow code\n\ntribble(\n  ~name, ~x, ~xend,\n  'symptomatic', 8, 16,\n  'infectious', 5, 12,\n  'shedding', 3, 20\n) %>%\n  mutate(\n    across(name, fct_inorder),\n    y = name %>% as.integer\n  ) %>%\n  ggplot(aes(x = x, xend = xend, color = name, label = name)) +\n  theme_minimal_hgrid() +\n  theme(legend.position = 'none') +\n  scale_x_continuous(breaks = c(0, 10, 20)) +\n  expand_limits(x = 0, xend = 20) +\n  scale_y_discrete(limits = rev) +\n  scale_color_brewer(type = 'qual', palette = 2) +\n  labs(x = 'time since exposed (days)', y = NULL) +\n  geom_segment(aes(y = name, yend = name), size = 1)\n\n\n\nQuantitative model would be to have each of these represented by a continuous function of time.\nCould further have a (joint) distribution of functions.\nCan tackle this by considering the impact on the exponential growth rate and the scaling factors relating exposed to infected to recovered to shedding amounts.\nA key interest is the relationship between cumulative exposures (i.e. infected) to currently infectious to shedding.\nAt large enough numbers, we should be able to approximate the effect of these complex dynamics through effective parameters linking the number of infected, number of infectious, and current rate of shedding; all will be growing asymptotically exponentially.\nThere are various plausible models here.\nAs a simple illustration, suppose that….\n\nStochasticity\nEstablishment size above which an outbreak is approximately guaranteed and can ignore stochastic effects; what is this for real epidemics? I’m used to models where the point at which success is assured and which can ignore stochastic effects are the same; is that also true with very skewed offspring/infection distributions?\nOrigination of the outbreak\nSuppose new infections coming in at rate \\(M\\).\nCan understand the point at which we have a deterministic local outbreak and can ignore new imports, based on what we know about this from evolutionary theory.\nShedding\nSimple model used in the modeling literature links cases with shedding by supposing that there is a fixed distribution of shedding as a function of time post symptom onset.\nThe idea that everyone sheds the same can be misleading; and is particularly sketchy in cases where there is substantial heterogeneity in how people respond to the disease.\nStill, it is a fair starting point and useful for illustration.\nIn our case, we’re interested in shedding that occurs even without clinical presentation, so I’ll consider that there is a shedding distribution \\(s(t)\\) indicating the amount of shedding from a person \\(t\\) days after infection (exposure).\nIf the number of exposures is \\(E(t)\\), then the total shedding \\(S(t)\\) at time \\(t\\) is given by the convolution of \\(E\\) with \\(s\\),\n\\[\\begin{align}\n  S(t) = \\int_{-\\infty}^{\\infty} E(u) \\; s(t - u) \\; du.\n\\end{align}\\]\nSuppose the number of exposed grows exponentially at rate \\(r\\), \\(E(t) = E_0 e^{rt}\\).\nWe know that \\(s(t) = 0\\) for \\(t \\le 0\\), and we further expect \\(s\\) to initially increase, peak at some time \\(t^*\\), and then decay to 0.\nLet’s take it to be Gamma distributed with a mean of 10 days and a CV of 0.5,\n\n\nShow code\n\nshedding <- tibble(mean = 10, cv = 0.5) %>%\n  mutate(\n    sd = cv * mean,\n    var = sd^2,\n    scale = var / mean,\n    shape = mean / scale\n  ) %>%\n  crossing(t = seq(0, 30, by = 0.1)) %>%\n  mutate(\n    density = dgamma(x = t, shape = shape, scale = scale)\n  )\nshedding %>%\n  ggplot(aes(t, density)) +\n  labs(x = 'Time (days) after exposure', y = 'Density of shedding') +\n  geom_line()\n\n\n\nHere, total area under the curve is 1, so we can think of the units as in terms of the total shedding over a single infection.\nNow let’s view the total shedding rate over time, starting from a single exposed and under a growth rate of \\(r = 0.1\\),\n\\[\\begin{align}\n  S(t) = \\int_{0}^{t} e^{r u} \\; s(t - u) \\; du.\n\\end{align}\\]\n\n\nShow code\n\ntotal_shedding <- function(t, r, gamma_mean, gamma_cv) {\n  scale = gamma_cv^2 * gamma_mean\n  shape = gamma_cv^(-2)\n  f <- function(u) { exp(r * u) * dgamma(t - u, shape = shape, scale = scale) }\n  integrate(f, lower = 0, upper = t)\n}\n# total_shedding(10, 0.1, 10, 0.5)$value\n\n\n\n\nShow code\n\nr <- 0.1\nx <- tibble(t = seq(0, 100)) %>%\n  mutate(\n    Exposed = exp(r * t),\n    Shed = map(t, total_shedding, r = r, gamma_mean = 10, gamma_cv = 0.5),\n    across(Shed, map_dbl, 'value')\n  )\n\n\n\n\nShow code\n\nx %>%\n  pivot_longer(-t) %>%\n  ggplot(aes(t, value, color = name)) +\n  scale_y_log10(limits = c(1e-2, NA)) +\n  geom_line()\n\n\n\nAfter an initial lag, the shedding rate too grows exponentially, as expected.\nWe can attempt a rough calculation of the ratio ratio \\(S/E\\):\nThe total shedding amount over an infection is 1\nThe bulk of the shedding of the infected is roughly between days 5 and 15;\nSo \\(S(t)\\) is roughly the sum of contributions from those infected over 5-15 days in the past, and each of them are contributing 1/10. (The mean \\(\\pm\\) the standard deviation)\n5-15 days in the past is (in this case) a period of \\(-1/2r\\) to \\(-3/2r\\); the integral of \\(e^{rt}\\) over that period is \\(1/(10 r) e^{rt} (e^{-1/2} - e^{-3/2})\\)\nThis suggests a ratio of \\((e^{-1/2} - e^{-3/2})/(10 r)\\) or 0.38.\n\n\nShow code\n\nratio_guess <- (exp(-1/2) - exp(-3/2)) / (10 * r)\nx %>%\n  mutate(Guess = Exposed * ratio_guess) %>%\n  pivot_longer(-t) %>%\n  ggplot(aes(t, value, color = name)) +\n  scale_y_log10(limits = c(1e-2, NA)) +\n  geom_line()\n\n\n\nTransport and sample collection\nNOTE: much of this is structurally the same as for shedding\nBasic model: see Will’s pdf as starting point.\nImportant complications — shedding will be taking place all over the city, at different times, randomly.\nThe signal from a given shedding event may spread out over days.\nBut in the regimes where we can detect things, guess that we can capture these complications by an effective recovery parameter.\nComplications: rain and/or other large perturbations to the WW system.\nCan we capture this with our main noise parameter?\nAs a way to get intuition, suppose that the amount being shed into the WW system at time \\(t\\) is given by \\(S(t)\\) and that the fraction of NA collected in the sample a time \\(t\\) after it is shed is given by \\(w(t)\\).\nThe amount of NA collected at time \\(t\\), \\(W(t)\\), is given by the convolution of \\(S\\) and \\(w\\),\n\\[\\begin{align}\n  W(t) = \\int_{-\\infty}^{\\infty} S(u) \\; w(t - u) \\; du.\n\\end{align}\\]\nOur main interest is in \\(W(t)\\) when the shedding amount grows exponentially at rate \\(r\\), \\(S(t) = S_0 e^{rt}\\).\nWe know that \\(w(t) = 0\\) for \\(t \\le 0\\), and we further expect \\(w\\) to initially increase, peak at some time \\(t^*\\), and then decay to 0.\nFor WWTPs, I expect the peak to be on the order of a day or less (\\(t^* \\lesssim 1\\)), and the timescale of the decay to be on the order of days, e.g. the half life around 0.5 to 3 days, but I haven’t looked into this carefully and this intuition is mostly based on the Helsinki study.\nIntuition for shape of \\(w(t)\\):\nCould try modeling as flow with turbulent diffusion -> model concentration with Brownian motion with a constant ‘drift’ term representing the average flow rate. Should be fairly easy to simulate and understand the timescale and tail shape with formal solution or heuristics for this approximation.\nComplication is that the transit length will vary by shedding location, and that the flow rate will vary by shedding location as well as along the transport path. The diffusion rate would also vary.\nMay be a long tail of arrival due to low-turnover reservoirs in the system where things might hang out; however, I don’t expect these to be very important contributors in an exponential growth scenario.\nPerturbances such as a rain storm may also be important.\nIt still seems useful to consider the simple model where there is a given transit length \\(l\\).\nBrownian motion at rate \\(\\sigma\\) with drift at rate \\(v\\), where the transit length is \\(l\\), gives1 that the concentration at time \\(t\\) is normally distributed with mean \\(vt\\) and standard deviation \\(\\sigma \\sqrt{t}\\).\nThe concentration at the sampling point (location \\(l\\) from the shedding location) is\n\\[\\begin{align}\n  w_l(t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 t}} \\exp \\left[\\frac{(l - v t)^2}{2\\sigma^2 t} \\right].\n\\end{align}\\]\nThe concentration peak occurs at \\(t^* = l / v\\).\nTODO: Determine the decay behavior\n\n\nShow code\n\nv = 1\nl = 1\nsigma = 0.2\nt_star = l/v\nsigma_star = sigma * sqrt(t_star)\n\ntibble(\n  t = seq(0, 2, by = 0.01)\n) %>%\n  mutate(\n    w = dnorm(l, mean = v * t, sd = sigma * sqrt(t))\n  ) %>%\n  ggplot(aes(t, w)) +\n  geom_line() +\n  geom_vline(xintercept = t_star, color ='grey') +\n  annotate(geom = 'segment', color = 'grey',\n           x = t_star - sigma_star,\n           xend = t_star + sigma_star,\n           y = 1, yend = 1)\n\n\n\nNote that the distribution is skewed, with more weight on the right side of the peak.\nLet’s now combine this with the shedding function \\(S(t)\\) to consider the total amount of NA collected at \\(t\\),\n\\[\\begin{align}\n  W(t) &= \\int_{-\\infty}^{\\infty} S(u) \\; w(t - u) \\; du \\\\\n       &= \\int_{-\\infty}^{\\infty} S_0 \\frac{1}{\\sqrt{2 \\pi \\sigma^2 (t-u)}}\n          \\exp \\left[ru + \\frac{(l - v (t-u))^2}{2\\sigma^2 (t-u)}\\right] \\; du\n\\end{align}\\]\nMy guess is we can do an ok job of understanding the behavior with a heuristic approximation, where we consider the lag time \\(t^* = l/v\\) and the spread at this time approximated by the standard deviation \\(\\sigma \\sqrt{t^*}\\).\nSample collection\nDetection methods\nqPCR\nAmplicon sequencing\nMetagenomic sequencing\nDetection under deterministic exponential growth\nI will assume a very simple model, but which can represent more complex cases so long as the outbreak has has established, and reached the asymptotic exponential growth rate in the number of infections, cumulative infections, and shedding amount, by using ‘effective parameters’.\nMotivation: Mathematical simplicity for developing intuition; also, I assume that detection has a negligible chance of occurring before the outbreak reaches the exponential phase.\nEpidemic dynamics\nNotation warning: Here, I’m taking \\(I\\) to be the number of infections, which includes exposed but non-yet-infectious individuals.\nIn other words, this would be \\(E + I\\) in the SEIR model.\nAssume deterministic exponential growth of the current number of infected, \\(I(t)\\), from some initial number \\(I_0 = I(0)\\).\nThe number of infected at time \\(t\\) (measured in days) has derivative\n\\[\\begin{align}\n  \\frac{dI}{dt} &= (b - d) I = r I,\n\\end{align}\\]\nwhere \\(r > 0\\) is the exponential growth rate.\nI further assume that \\(r\\) can be broken into separate ‘birth’ and ‘death’ rates, \\(b\\) and \\(d\\), describing the rate at which infections beget new infections, and which infected individuals recover.\nUnder this model, the number of infections at time \\(t\\) grows as\n\\[\\begin{align}\n  I(t) &\\approx I_0 e^{rt}.\n\\end{align}\\]\nThe cumulative number of infection-days at day \\(t\\) is\n\\[\\begin{align}\n  \\int_0^t I(u)\\;du\n    &= \\int_0^t I_0 e^{ru}\\;du\n  \\\\&= \\frac{I_0}{r} \\left(e^{rt} - 1 \\right)\n  \\\\&= \\frac{1}{r} \\left[I(t) - I(0) \\right]\n  \\\\&\\approx \\frac{1}{r} I(t) \\quad \\text{for $t \\gg 1/r$}.\n\\end{align}\\]\nWe can calculate the cumulative number of infections \\(C(t)\\) at day \\(t\\) by noting that new infections occur at rate \\(b I(t)\\); hence\n\\[\\begin{align}\n  C(t)\n    &= \\int_0^t b I(u)\\;du\n  \\\\&= \\int_0^t b I_0 e^{ru}\\;du\n  \\\\&= \\frac{b I_0}{r} \\left(e^{rt} - 1 \\right)\n  \\\\&= \\frac{b}{r} \\left[I(t) - I(0) \\right]\n  \\\\&\\approx \\frac{b}{r} I(t) \\quad \\text{for $t \\gg 1/r$}.\n\\end{align}\\]\nThe cumulative number of infections is \\(b\\) times the cumulative number of infection-days, and is approximately \\(b/r\\) the number of current infections.\nNote: Arguably, we should be adding the initial number of infected, \\(I_0\\), in which case we get\n\\[\\begin{align}\n  C(t)\n    &= \\int_0^t b I(u)\\;du  + I_0\n  \\\\&= \\frac{b}{r} \\left[I(t) - I_0 \\right] + I_0\n  \\\\&= \\frac{b}{r} I(t) - \\frac{d}{r} I_0.\n  \\\\&\\approx \\frac{b}{r} I(t) \\quad \\text{for $I(t) \\gg I_0$}.\n\\end{align}\\]\nI will typically assume \\(I_0\\) is a tiny fraction of all current and cumulative infections in the regimes of interest, so this distinction shouldn’t matter.\nThe condition \\(I(t) \\gg I_0\\) will be true for \\(t \\gg 1/r\\).\nDefine cumulative incidence \\(c(t)\\) as the fraction of the population that have been infected by time \\(t\\),\n\\[\\begin{align}\n  c(t)\n    &= C(t) / N\n  \\\\&\\approx \\frac{b}{r} i(t) \\quad \\text{for $t \\gg 1/r$}.\n\\end{align}\\]\nHow long is the exponential approximation good for?\nSuppose there is lasting immunity, such that each person can only be infected once.\nIn that case, we expect the rate of new infections per capita, \\(b\\), to be in proportional to \\(1 - c(t)\\); i.e., \\(b(t) = [1 - c(t)] b_0\\).\nThe exponential approximation is good so long as \\(c(t) \\ll 1\\).\nA good rule of thumb could be to take the point when \\(c(t) \\approx 1/e\\) as the end of the exponential phase.\nThis occurs when the currently infected fraction of the population \\(i(t)\\) reaches \\(\\tfrac{r}{b} e^{-1}\\), or when \\(t \\approx \\tfrac{1}{r} [\\log (\\tfrac{r}{i_0 b}) - 1]\\).\n(Needs to be checked)\nThe peak of the epidemic (max value of \\(I\\)) will occur when \\(b\\) decreases to \\(d\\).\nThis occurs when \\(c(t) = 1 - \\tfrac{d}{b_0} = \\tfrac{r_0}{b_0}\\).\nThe number of susceptible (those not infected) is \\(1 - c = d/b_0\\), matching the standard SIR result (e.g. equation 4a here).\nShedding and sequencing\nSuppose the number of infected at time \\(t\\) is then\n\\[\\begin{align}\n  I(t) &\\approx I_0 e^{rt}.\n\\end{align}\\]\nwhere \\(I_0\\) is the initial number of infected.\nLet \\(N\\) be the total population (assumed fixed) and \\(i(t) = I(t) / N\\) be the fraction of the population that is infected.\nAssume a constant amount of background shedding in each person regardless of infection status.\nSuppose a background shedding rate of \\(s_0\\) of background microbiome in each person, regardless of infection status.\nLet \\(s\\) be the relative rate of shedding of pathogen to background in an infected person, so that \\(s s_0\\) is the rate of pathogen shedding in an infected person (note potential name clash with the fraction of the population that is susceptible).\nLet \\(B\\) be the relative measurement effiency of the pathogen to the background (MGS bias).\nIn the regime I consider below, where total threat reads are a small fraction of total reads, we can use \\(B\\) to also account for the fact that perhaps only a small fraction of reads from the pathogen are useful for detecting it as a novel threat.\nThe expected fraction (i.e. proportion) of the threat in the MGS reads in a sample collected on day \\(t\\) is\n\\[\\begin{align}\nE [ P(t) ]\n  &\\approx \\frac{I(t) s s_0 B}{I(t) s s_0  B + N s_0 }\n\\\\&= \\frac{I(t) s B}{I(t) s B + N }\n\\\\&= \\frac{i(t) s B}{i(t) s B + 1 }\n\\\\&\\approx i(t) s B.\n\\end{align}\\]\nThe first approximation comes from treating \\(I(t)\\) as constant over the 24h the sample is collected (more on this below).\nThe second approximation comes from assuming that the pathogen is always a small fraction of the reads (\\(i s B \\ll 1\\)).\nOne way for this condition to arise is that it holds even for a sample from an infected individual or an entirely infected population (i.e., \\(s B \\ll 1\\)).\nI expect that to be true for subtle pathogens, but not necessary all pathogens (e.g., gastroenteric pathogens).\nFor these latter cases I expect \\(i(t) \\ll 1\\) and for the broader condition \\(i s B \\ll 1\\) to still hold.\nTo determine the absolute copy number or concentration of the pathogen in the sample, we’d need to say something about the overall shedding amount and water system. However, to analyze MGS detection, it is enough to consider the relative abundance as above.\nThe expected number of sequencing reads of the threat on day \\(t\\) is\n\\[\\begin{align}\nE [ M(t) ] = E [ P(t) ] \\cdot \\mathcal M \\approx i(t) s B \\mathcal M.\n\\end{align}\\]\nwhere \\(\\mathcal M\\) is the total sequencing depth, which I’m treating as a value that is determined by the experimenter and is the same for each day.\nMore generally, if the sample is collected over a particular period of time \\(\\Delta t\\), we might want to model the pathogen contribution to the sample as an integral of shedding over that period,\n\\[\\begin{align}\nE [ M(t) ]\n  &= s B \\mathcal M \\int_{t-\\Delta t}{t} i(u) \\; du\n\\\\&= s B \\mathcal M \\frac{1}{r} i_0 e^{rt} \\left[1 - e^{-r \\Delta t} \\right]\n\\\\&= s B \\mathcal M \\frac{1}{r} i(t) \\left[1 - e^{-r \\Delta t} \\right].\n\\\\&\\approx i(t) (\\Delta t) s B \\mathcal M \\quad \\text{for $r \\Delta t \\ll 1$}.\n\\end{align}\\]\nDetection\nWarning: The below assumes that \\(r \\ll 1\\), so that we can ignore the growth in \\(i(t)\\) over the sampling period when computing the contribution of pathogen to the sample.\nSuppose there is a threshold number of reads (of the pathogen or the distinguishing subsequence), \\(M^*\\), we need to see in a single day to detect the threat (e.g. 5).\nI’ll take \\(\\Delta t = 1\\) (24-h composite sampling) and suppose that \\(r \\ll 1\\), so that we can ignore growth over the sampling window (otherwise, just multiply the number of expected reads by the constant factor \\((1 - e^r)/r\\)).\nAs a first approximation, let’s ignore noise in shedding and sequencing measurement, and define detection as occurring when \\(E [M(t^*)] = M^*\\).\nDetection then occurs at \\(i(t) s B \\mathcal M = M^*\\) or\n\\[\\begin{align}\n  i(t^*) = \\frac{M^*}{s B \\mathcal M}.\n\\end{align}\\]\nIn the exponential regime where \\(i(t) = i_0 e^{rt}\\) and cumulative incidence is \\(c(t) \\approx \\tfrac{b}{r} i(t)\\), we have that\n\\[\\begin{align}\n  t^* = \\frac{1}{r} \\log \\frac{M^*}{i_0 s B \\mathcal M}\n\\end{align}\\]\n\\[\\begin{align}\n  c(t^*) = \\frac{b}{r} \\cdot \\frac{M^*}{s B \\mathcal M}.\n\\end{align}\\]\nThese results explain the pattern seen in the example simulations presented by Charlie Whittaker last December, in which we saw that the time of detection decreases logarithmically \\(\\mathcal M\\) and cumulative incidence at time of detection decreases as \\(1 / \\mathcal M\\).\nNote that the detection threshold and efficiency appear together as \\(M^* / B\\); this means we’ll get the same answer whether we think of\n\\(M^*\\) is the number of reads hitting a particular identifying subsequence, and \\(B\\) contains a factor \\(x < 1\\) to account for the fact that only a fraction \\(x\\) of the pathogen’s reads hit the subsequence\n\\(M^*\\) is the number of reads hitting the pathogen, and we increase it by a factor \\(1/x\\) to account for the fact that only \\(x\\) of the reads are useful for identification; \\(B\\) does not contain the factor \\(x\\).\nHowever, when we start to consider noise in the read counts, we need to be more careful about this.\nCumulative reads over a given period\nWhat if it is sufficient to see \\(R^*\\) reads cumulatively over the last \\(T\\) days?\nCall \\(Q\\) the cumulative reads over the last \\(T\\) days.\nThen\n\\[\\begin{align}\nE [ Q(t) ]\n  &= s B \\mathcal M \\int_{t-T}{t} i(u) \\; du\n\\\\&= s B \\mathcal M i(t) \\frac{1}{r} \\left[1 - e^{-r T} \\right].\n\\end{align}\\]\nSetting \\(T = t\\) corresponds to all reads since time \\(0\\); in that case, we have that \\(E [ Q(t) ]\\) equals \\(s B \\mathcal M\\) times the cumulative infection hours, or approximately\n\\[\\begin{align}\nE [ Q(t) ]\n  &\\approx s B \\mathcal M \\frac{i(t)}{r}\n  \\\\&\\approx s B \\mathcal M \\frac{c(t)}{b}\n\\end{align}\\]\nConsidering the factor \\(1 - e^{-rT}\\) indicates that it is only the samples from the past few characteristic growth periods that contributes significantly to the total reads.\nHow much better do we do, detection wise, when we only require cumulative reads to reach \\(M^*\\)?\nDetection now occurs at \\(E[Q(t)] = M^*\\) or\n\\[\\begin{align}\n  i(t^*) &= \\frac{r M^*}{s B \\mathcal M} \\\\\n  c(t^*) &= b \\cdot \\frac{M^*}{s B \\mathcal M};\n\\end{align}\\]\nthese are both a factor \\(r\\) greater than before.\nAs expected, the effect on time is only logarithmic,\n\\[\\begin{align}\n  t^* = \\frac{1}{r} \\log \\frac{r M^*}{i_0 s B \\mathcal M}.\n\\end{align}\\]\nThe effect overall is as if we increased any of \\(\\mathcal M\\), \\(s\\), or \\(B\\) by a factor of \\(1/r\\).\nConnecting to empirical data\nJeff’s analysis of SARS2 in Rothman et al. (2021)\nJeff estimates here that the proportion of SARS2 reads in a sample in the Rothman et al. (2021) data is 3e-7 when 1e-3 of the catchment is infected.\nThat corresponds to \\(P = 3\\cdot 10^{-7}\\) when \\(i = 10^{-3}\\), so that \\(sB = P(t) / i(t) = 3 \\cdot 10^{-4} \\approx 10^{-3.5}\\).\nWe would therefore need to sequence a single sample to a depth of \\(10^{8.5}\\) to see 10 reads of SARS2 when \\(10^{-4}\\) of the population is infected.\n(CHECK).\n\n\n\nDiekmann, Odo, Hans Heesterbeek, and Tom Britton. 2012. Mathematical Tools for Understanding Infectious Disease Dynamics. First. Princeton University Press. https://doi.org/10.23943/princeton/9780691155395.001.0001.\n\n\nHeng, Kevin, and Christian L. Althaus. 2020. “The Approximately Universal Shapes of Epidemic Curves in the Susceptible-Exposed-Infectious-Recovered (SEIR) Model.” Scientific Reports 10 (1): 19365. https://doi.org/10.1038/s41598-020-76563-8.\n\n\nMa, Junling. 2020. “Estimating Epidemic Exponential Growth Rate and Basic Reproduction Number.” Infectious Disease Modelling 5 (January): 129–41. https://doi.org/10.1016/j.idm.2019.12.009.\n\n\nRothman, Jason A., Theresa B. Loveless, Joseph Kapcia, Eric D. Adams, Joshua A. Steele, Amity G. Zimmer-Faust, Kylie Langlois, et al. 2021. “RNA Viromics of Southern California Wastewater and Detection of SARS-CoV-2 Single-Nucleotide Variants.” Applied and Environmental Microbiology 87 (23): e01448–21. https://doi.org/10.1128/AEM.01448-21.\n\n\nhttps://www.randomservices.org/random/brown/Drift.html↩︎\n",
    "preview": "posts/2023-02-06-basic-detection-theory/basic-detection-theory_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2023-03-03T20:14:20+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-effect-of-noise-on-egd/",
    "title": "Examining the effect of noise on Exponential Growth Detection",
    "description": {},
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "EGD",
      "R",
      "theory"
    ],
    "contents": "\n\nContents\nBackground\nAnalysis\nNext steps\nDiscussion\nSession info\n\n\n\n# set of data analysis and viz tools\nlibrary(tidyverse)\nlibrary(furrr)\nplan(multisession, workers = 3)\n# file system helpers\nlibrary(fs)\n# specifying locations within a project\nlibrary(here)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\nlibrary(ggdist)\ntheme_set(theme_cowplot())\n\n# stats\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\nlibrary(broom.mixed)\n\n\nBackground\nWe’ve done some examination of the ability to detect exponential increase when there is no noise beyond the Poisson error process associated with sequencing.\nBut we know there is additional noise due to processes such as shedding, sample collection, and sample processing.\nQuestions this analysis considers:\nHow does increasing the amount of noise in the relative abundance of pathogen in the sequencing data reduce our power to estimate its growth rate or infer that it is exponentially increasing?\nHow much additional sequencing effort is required to make up for a given increase in noise/dispersion? this is a naive question — we know from statistical principles that you can’t really make up for more noise upstream of sequencing by doing more sequencing; you need to make new measurements that have independent noise. But I consider it here for illustration purposes.\nAnalysis\nAssume a Gamma-Poisson (also known as Negative Binomial) model for both the real data and the statistical inference model.\nI will start with a simple model.\nFor now, I will assume that the fraction of the population that is infected grows exponentially without any noise.\n\\[\\begin{align}\n  i(t) = i(0) \\exp(r t),\n\\end{align}\\]\nand that number of reads of the pathogen in the sample from time \\(t\\), \\(M(t)\\), has expectation\n\\[\\begin{align}\n  E[M(t)] &= i(t) \\cdot s B \\mathcal M\n        \\\\&\\equiv i(s) \\cdot a,\n\\end{align}\\]\nwhere\n\\(s\\) is the rate of pathogen shedding in infected, relative to background microbiome\n\\(B\\) is the measurement efficiency (bias), relative to background microbiome\n\\(\\mathcal M\\) is the total number of sequencing reads\nThis formula for the mean read count of the pathogen is derived elsewhere.\nFor simplicity I’m treating all of \\(s\\), \\(B\\), and \\(\\mathcal M\\) as fixed parameters.\nSince all of these parameters are constants that multiply together, it is convenient for later calculations to define \\(a \\equiv sB\\mathcal M\\).\nTODO\nExplain nuance regarding noise etc\nConsider using more nuanced model for expected pathogen reads based on integral of \\(i(t)\\).\nI assume a Negative Binomial model for the number of reads of the pathogen in the sample from time \\(t\\), using the ‘alternative parameterization’ used described in the Stan docs; however, I’ll follow Rstanarm and use \\(\\theta\\) in place of \\(\\phi\\) for the reciprocal dispersion parameter.\nIf \\(E[M] = \\mu\\), then the variance of \\(M\\) is\n\\[\\begin{align}\n  \\text{Var}[M]\n    &= \\mu + \\frac{\\mu^2}{\\theta}\n  \\\\&= \\mu \\left(1 + \\frac{\\mu}{\\theta} \\right),\n\\end{align}\\]\nand the coefficient of variation is\n\\[\\begin{align}\n  \\text{CV}[M]\n    &= \\frac{\\sqrt{\\text{Var}[M]}}{E[M]}\n  \\\\&= \\sqrt{\\frac{1}{\\mu} + \\frac{1}{\\theta}}.\n\\end{align}\\]\n(speculative) Intuitively, the coefficient of variation is the measure of noise relevant for our power to infer exponential growth rate.\nThe CV is Poisson-like when \\(\\mu \\ll \\theta\\) and Gamma-like when \\(\\mu \\gg \\theta\\).\nIn the Poisson regime, we can reduce the CV by increasing sequencing effort and hence increasing \\(\\mu\\);\nhowever, once \\(\\mu \\gg \\theta\\), increasing the sequencing effort will have a negligible benefit.\nInstead, we need to collect additional samples and/or re-measure existing samples so as to effectively average out the extra-Poisson noise.\nI will simulate using stats::rnbinom with the parameterization matching\nThe size argument in rnbinom is the (reciprocal) dispersion parameter \\(\\phi\\) in stan docs, but which is called \\(\\theta\\) in rstanarm::neg_binomial_2.\n\n\n# Total time of monitoring, in days\ntotal_time <- 20\nsampling_days <- seq(0, total_time - 1, by = 1)\n# Initial fraction of population infected\ni_0 <- 1e-4\n# Growth rate of infections to correspond to 4 doublings\ndoubling_time <- 5\nr <- log(2) / doubling_time\n# Multiplier a s.t. expected number of reads spans 1 over the range\na <- 0.4 / i_0\n# a * i_0 * exp(r * sampling_days)\n\n# reciprocal-dispersion parameter; smaller values = lower variance\ntheta_sim <- 5e-1\n\nset.seed(42)\nsim <- tibble(t = sampling_days) %>%\n  mutate(\n    i_t = i_0 * exp(r * t),\n    M_t_expected = a * i_t,\n    # lambda_t = rgamma(n(), shape = 1, rate = 1 / M_t_expected),\n    # M_t = rpois(n(), lambda_t)\n    M_t = rnbinom(n(), size = theta_sim, mu = M_t_expected)\n  )\n\n\n\n\nsim %>%\n  ggplot(aes(t)) +\n    geom_line(aes(y = M_t_expected)) +\n    geom_point(aes(y = M_t))\n\n\n\nNow we want to repeat the simulations many times for a range of values of \\(\\theta\\) and \\(a\\).\n\n\nsimulate_monitoring <- function(theta, a) {\n  tibble(t = sampling_days) %>%\n    mutate(\n      i_t = i_0 * exp(r * t),\n      M_t_expected = a * i_t,\n      M_t = rnbinom(n(), size = theta, mu = M_t_expected)\n    )\n}\n\nset.seed(42)\n\nsims <- crossing(\n  theta = c(3e-2, 1e-1, 3e-1, 1e0),\n  # a = 4000 * c(1, 3, 10, 30),\n  a = 4000 * c(1, 10, 100),\n  rep = 1:40\n) %>%\n  mutate(\n    data = map2(theta, a, simulate_monitoring),\n    M_total = map_dbl(data, ~ sum(.x$M_t))\n  )\n\n\n\n\nsims %>%\n  unnest(data) %>%\n  ggplot(aes(t, M_t)) +\n  facet_grid(a ~ theta, scales = 'free_y') +\n  geom_line(aes(group = rep))\n\n\n\nCHECK: Do any simulations have no observations?\n\n\nsims %>%\n  count(theta, M_total == 0)\n\n# A tibble: 5 × 3\n  theta `M_total == 0`     n\n  <dbl> <lgl>          <int>\n1  0.03 FALSE            113\n2  0.03 TRUE               7\n3  0.1  FALSE            120\n4  0.3  FALSE            120\n5  1    FALSE            120\n\nYes! We need to handle these separately, since they will cause errors when we try to fit the GLM.\n\n\nFALSE\n\n[1] FALSE\n\nLet’s try fitting on a subset, for now using the ‘optimizing’ algorithm to speed things up.\nWe must restrict ourselves to cases where the total count is at least 1, to be able to fit.\nWe might consider restricting ourselves to a higher count than this.\nTODO: disable centering the predictors; otherwise, need to change our intercept prior to be based on the midpoint of the simulation.\n\n\nsims_fit <- sims %>%\n  # filter(rep <= 2, M_total > 0) %>%\n  filter(M_total > 0) %>%\n  mutate(\n    prior_intercept_mean = log(a * i_0),\n    # Note: For testing with 'optimizing', we can fit in parallel\n    fit = map(data, ~stan_glm(\n        M_t ~ t,, \n        data = .x,\n        family = neg_binomial_2,\n        prior = normal(0, 0.5, autoscale = FALSE),\n        # prior_intercept = normal(prior_intercept_mean, 2.5, autoscale = FALSE),\n        prior_aux = exponential(2, autoscale = FALSE),\n        algorithm = 'optimizing',\n    ))\n  )\n\n\nTODO\nLook at a single model and its fit against the ‘real’ data.\nConsider the prior on theta, and explicitly code it in. For the purposes of this study, I can make the prior accurately reflect the actual range of theta values that I’m using. In fact, it would even make sense to set the prior tightly around the correct value of theta, which should also speed up the inference.\nreconsider the prior on the intercept; could set to a range like with theta though that doesn’t make too much sense if we think of the fact that we’re modeling an increase in sequencing effort and so we have extra info that the mean should increase accordingly\n\n\nfit <- sims_fit %>% pull(fit) %>% pluck(1)\nfit\n\nstan_glm\n family:       neg_binomial_2 [log]\n formula:      M_t ~ t\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept)  2.9    2.0  \nt           -0.4    0.3  \n\nAuxiliary parameter(s):\n                      Median MAD_SD\nreciprocal_dispersion 0.1    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nfit %>% tidy(conf.int = TRUE)\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    2.86      1.96    -0.536    6.58  \n2 t             -0.429     0.272   -0.914    0.0584\n\nfit %>% prior_summary\n\nPriors for model '.' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n ~ normal(location = 0, scale = 0.5)\n\nAuxiliary (reciprocal_dispersion)\n ~ exponential(rate = 2)\n------\nSee help('prior_summary.stanreg') for more details\n\nOur goal is to assess how dispersion impacts our ability to infer the exponential trend.\nOne way we can do that is plot credible intervals for the growth rate \\(r\\), for all simulations, group by theta, against the actual growth rate.\nAn easy way to get CIs is with broom.mixed::tidy(),\n\n\nfit %>% tidy(conf.int = TRUE, conf.level = 0.9)\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    2.86      1.96    -0.536    6.58  \n2 t             -0.429     0.272   -0.914    0.0584\n\nBesides correctly inferring the growth rate, we are also interested in the our posterior that the sequence is increasing (possibly above a certain rate of increase; here I’ll just consider a rate above 0).\nWe can do this for a single fit like\n\n\n# post <- fit %>% as.matrix %>% as_tibble %>% janitor::clean_names() %>%\n  # glimpse\nfit %>% as.matrix %>% {mean(.[, 't'] > 0)}\n\n[1] 0.075\n\n\n\nx <- sims_fit %>%\n  mutate(\n    prob_increasing = map_dbl(fit, ~ .x %>% as.matrix %>% {mean(.[, 't'] > 0)}),\n    fit = map(fit, tidy, conf.int = TRUE, conf.level = 0.9),\n  ) %>%\n  unnest(fit) %>%\n  filter(term == 't')\n\n\n\n\nx %>%\n  ggplot(aes(y = rep, x = estimate)) +\n  facet_grid(a ~ theta) +\n  geom_pointinterval(\n    aes(xmin = conf.low, xmax = conf.high),\n    fatten_point = 1\n  ) +\n  geom_vline(xintercept = 0, color = 'grey') +\n  geom_vline(xintercept = r, linetype = 2, color = 'darkred')\n\n\n\nNote that for lambda=0.03, some intervals are missing; these are cases where the dispersion was so high that we never saw the pathogen.\nIn those cases, our estimate of the growth rate is simply the prior; perhaps we can show that?\nFrom this graph, it looks like a 10X increase in dispersion requires more than a 10X increase in sequencing effort to achieve the same power.\nLet’s check how calibrated these CIs are, by comparing the proportion of fits that contain the true value of \\(r\\) against the expected 90%.\nNote, the case where \\(\\theta = 0.03\\) is currently not adjusted for the missing data.\n\n\nx %>%\n  mutate(\n    true_value_in_ci = r >= conf.low & r <= conf.high\n  ) %>%\n  summarize(.by = c(theta, a),\n    proportion = mean(true_value_in_ci)\n  )\n\n# A tibble: 12 × 3\n   theta      a proportion\n   <dbl>  <dbl>      <dbl>\n 1  0.03   4000      0.853\n 2  0.03  40000      0.744\n 3  0.03 400000      0.8  \n 4  0.1    4000      0.775\n 5  0.1   40000      0.825\n 6  0.1  400000      0.725\n 7  0.3    4000      0.85 \n 8  0.3   40000      0.85 \n 9  0.3  400000      0.875\n10  1      4000      0.85 \n11  1     40000      0.925\n12  1    400000      0.95 \n\nThese seem to be fairly calibrated; would need to do a binomial test to look for evidence of deviation.\nNow let’s look at the power to detect exponential growth, by considering the posterior probability that \\(r>0\\).\n\n\nx %>%\n  ggplot(aes(y = as.factor(theta), x = prob_increasing)) +\n  facet_wrap(~a, labeller = label_both) +\n  labs(x = 'Pr(r > 0)', y = 'reciprocal dispersion, theta') +\n  geom_dots(binwidth = 0.01)\n\n\n  # stat_dotsinterval()\n\n\nTODO: in revising, do the above calculation at the same time as the inteval extraction. Could also do the interval in the same manner, from the posterior.\n\n\nx %>%\n  ggplot(aes(y = as.factor(a), x = prob_increasing)) +\n  facet_wrap(~theta, labeller = label_both) +\n  labs(x = 'Pr(r > 0)', y = 'reciprocal dispersion, theta') +\n  geom_dots(binwidth = 0.01)\n\n\n  # stat_dotsinterval()\n\n\nHow often do we infer that \\(r>0\\) is more likely than not? Or with 80% certainty?\nNote: We have not filled in the missing rows where there were no observations; in these cases, we cannot infer increase.\n\n\nx %>%\n  summarize(.by = c(theta, a),\n    prob_0.5 = mean(prob_increasing > 0.5),\n    prob_0.6 = mean(prob_increasing > 0.6),\n    prob_0.8 = mean(prob_increasing > 0.8),\n  )\n\n# A tibble: 12 × 5\n   theta      a prob_0.5 prob_0.6 prob_0.8\n   <dbl>  <dbl>    <dbl>    <dbl>    <dbl>\n 1  0.03   4000    0.618    0.588    0.324\n 2  0.03  40000    0.513    0.513    0.282\n 3  0.03 400000    0.55     0.525    0.325\n 4  0.1    4000    0.775    0.75     0.525\n 5  0.1   40000    0.7      0.65     0.475\n 6  0.1  400000    0.8      0.675    0.575\n 7  0.3    4000    0.95     0.9      0.85 \n 8  0.3   40000    0.9      0.9      0.825\n 9  0.3  400000    0.95     0.95     0.825\n10  1      4000    0.975    0.975    0.95 \n11  1     40000    1        1        1    \n12  1    400000    1        1        1    \n\nNext steps\nwrite out the model, and compute the coefficient of variation under it.\nwrite some background\nmake the model more concrete, perhaps by framing as having a random relative abundance and Poisson sampling.\nstart with a lower value of a, s.t. can see that sequencing depth matters until we’re seeing a large enough expected count\ninvestigate the errors during fitting\nDiscussion\nIncreasing the sequencing depth cannot make up for an increase in overdispersion.\nThis makes sense — once we’re in a regime where the expected read count is above 0 and there is lots of overdispersion relative to Poisson, sequencing more doesn’t help much; it just helps us get a more precise measurement of the latent noisy (relative) abundance, and what we need is to reduce noise in that latent abundance.\nTo do this, we need to measure more samples with independent relative abundances.\nWe can reduce the noise from sample processing by processing the same sample repeatedly; however, for other noise sources we’d need to collect new samples from different sources or from more days.\nNote that because I use the correct model to fit the data, increasing the dispersion does not make the fit overconfident; the credible intervals are still tending to cover the true value of r the expected 90% of the time.\nIn contrast, if I fit using Poisson regression instead, I expect the fit to be overconfident, that is that the credible intervals will be too small and we’ll be missing the true r more than 90% of the time, for the cases there theta is significantly less than 1.\nSession info\n\nClick for session info\n\n\nsessioninfo::session_info()\n\n─ Session info ─────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       Arch Linux\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-02-09\n pandoc   3.0 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ─────────────────────────────────────────────────────────────\n package        * version  date (UTC) lib source\n assertthat       0.2.1    2019-03-21 [1] CRAN (R 4.0.0)\n backports        1.4.1    2021-12-13 [1] CRAN (R 4.1.2)\n base64enc        0.1-3    2015-07-28 [1] CRAN (R 4.0.0)\n bayesplot        1.9.0    2022-03-10 [1] CRAN (R 4.2.0)\n beeswarm         0.4.0    2021-06-01 [1] CRAN (R 4.1.0)\n boot             1.3-28   2021-05-03 [2] CRAN (R 4.2.2)\n broom            1.0.1    2022-08-29 [1] CRAN (R 4.2.1)\n broom.mixed    * 0.2.9.4  2022-04-17 [1] CRAN (R 4.2.0)\n bslib            0.4.1    2022-11-02 [1] CRAN (R 4.2.2)\n cachem           1.0.6    2021-08-19 [1] CRAN (R 4.1.1)\n callr            3.7.3    2022-11-02 [1] CRAN (R 4.2.1)\n cellranger       1.1.0    2016-07-27 [1] CRAN (R 4.0.0)\n cli              3.4.1    2022-09-23 [1] CRAN (R 4.2.1)\n codetools        0.2-18   2020-11-04 [2] CRAN (R 4.2.2)\n colorspace       2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n colourpicker     1.2.0    2022-10-28 [1] CRAN (R 4.2.1)\n cowplot        * 1.1.1    2021-08-27 [1] Github (wilkelab/cowplot@555c9ae)\n crayon           1.5.2    2022-09-29 [1] CRAN (R 4.2.1)\n crosstalk        1.2.0    2021-11-04 [1] CRAN (R 4.1.2)\n DBI              1.1.3    2022-06-18 [1] CRAN (R 4.2.1)\n dbplyr           2.2.1    2022-06-27 [1] CRAN (R 4.2.1)\n digest           0.6.30   2022-10-18 [1] CRAN (R 4.2.1)\n distill          1.5.2    2022-11-10 [1] Github (rstudio/distill@9c1a1a2)\n distributional   0.3.1    2022-09-02 [1] CRAN (R 4.2.1)\n downlit          0.4.2    2022-07-05 [1] CRAN (R 4.2.1)\n dplyr          * 1.1.0    2023-01-29 [1] CRAN (R 4.2.2)\n DT               0.26     2022-10-19 [1] CRAN (R 4.2.1)\n dygraphs         1.1.1.6  2018-07-11 [1] CRAN (R 4.0.2)\n ellipsis         0.3.2    2021-04-29 [1] CRAN (R 4.1.0)\n evaluate         0.18     2022-11-07 [1] CRAN (R 4.2.2)\n fansi            1.0.3    2022-03-24 [1] CRAN (R 4.2.1)\n farver           2.1.1    2022-07-06 [1] CRAN (R 4.2.1)\n fastmap          1.1.0    2021-01-25 [1] CRAN (R 4.0.4)\n forcats        * 0.5.2    2022-08-19 [1] CRAN (R 4.2.1)\n fs             * 1.5.2    2021-12-08 [1] CRAN (R 4.1.2)\n furrr          * 0.3.1    2022-08-15 [1] CRAN (R 4.2.1)\n future         * 1.28.0   2022-09-02 [1] CRAN (R 4.2.1)\n gargle           1.2.1    2022-09-08 [1] CRAN (R 4.2.1)\n generics         0.1.3    2022-07-05 [1] CRAN (R 4.2.1)\n ggbeeswarm     * 0.6.0    2017-08-07 [1] CRAN (R 4.0.0)\n ggdist         * 3.2.0    2022-07-19 [1] CRAN (R 4.2.1)\n ggplot2        * 3.3.6    2022-05-03 [1] CRAN (R 4.2.0)\n ggridges         0.5.4    2022-09-26 [1] CRAN (R 4.2.1)\n globals          0.16.1   2022-08-28 [1] CRAN (R 4.2.1)\n glue             1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n googledrive      2.0.0    2021-07-08 [1] CRAN (R 4.1.0)\n googlesheets4    1.0.1    2022-08-13 [1] CRAN (R 4.2.1)\n gridExtra        2.3      2017-09-09 [1] CRAN (R 4.0.2)\n gtable           0.3.1    2022-09-01 [1] CRAN (R 4.2.1)\n gtools           3.9.3    2022-07-11 [1] CRAN (R 4.2.1)\n haven            2.5.1    2022-08-22 [1] CRAN (R 4.2.1)\n here           * 1.0.1    2020-12-13 [1] CRAN (R 4.0.5)\n highr            0.9      2021-04-16 [1] CRAN (R 4.1.0)\n hms              1.1.2    2022-08-19 [1] CRAN (R 4.2.1)\n htmltools        0.5.3    2022-07-18 [1] CRAN (R 4.2.1)\n htmlwidgets      1.5.4    2021-09-08 [1] CRAN (R 4.1.1)\n httpuv           1.6.6    2022-09-08 [1] CRAN (R 4.2.1)\n httr             1.4.4    2022-08-17 [1] CRAN (R 4.2.1)\n igraph           1.3.5    2022-09-22 [1] CRAN (R 4.2.1)\n inline           0.3.19   2021-05-31 [1] CRAN (R 4.1.0)\n jquerylib        0.1.4    2021-04-26 [1] CRAN (R 4.1.0)\n jsonlite         1.8.3    2022-10-21 [1] CRAN (R 4.2.1)\n knitr            1.40     2022-08-24 [1] CRAN (R 4.2.1)\n labeling         0.4.2    2020-10-20 [1] CRAN (R 4.0.3)\n later            1.3.0    2021-08-18 [1] CRAN (R 4.1.1)\n lattice          0.20-45  2021-09-22 [2] CRAN (R 4.2.2)\n lifecycle        1.0.3    2022-10-07 [1] CRAN (R 4.2.1)\n listenv          0.8.0    2019-12-05 [1] CRAN (R 4.0.0)\n lme4             1.1-31   2022-11-01 [1] CRAN (R 4.2.1)\n loo              2.5.1    2022-03-24 [1] CRAN (R 4.2.0)\n lubridate        1.9.0    2022-11-06 [1] CRAN (R 4.2.2)\n magrittr         2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n markdown         1.3      2022-10-29 [1] CRAN (R 4.2.1)\n MASS             7.3-58.1 2022-08-03 [1] CRAN (R 4.2.1)\n Matrix           1.5-1    2022-09-13 [1] CRAN (R 4.2.1)\n matrixStats      0.62.0   2022-04-19 [1] CRAN (R 4.2.0)\n memoise          2.0.1    2021-11-26 [1] CRAN (R 4.1.2)\n mime             0.12     2021-09-28 [1] CRAN (R 4.1.1)\n miniUI           0.1.1.1  2018-05-18 [1] CRAN (R 4.0.2)\n minqa            1.2.5    2022-10-19 [1] CRAN (R 4.2.1)\n modelr           0.1.9    2022-08-19 [1] CRAN (R 4.2.1)\n munsell          0.5.0    2018-06-12 [1] CRAN (R 4.0.0)\n nlme             3.1-160  2022-10-10 [2] CRAN (R 4.2.2)\n nloptr           2.0.3    2022-05-26 [1] CRAN (R 4.2.0)\n nvimcom        * 0.9-142  2022-12-22 [1] local\n parallelly       1.32.1   2022-07-21 [1] CRAN (R 4.2.1)\n patchwork      * 1.1.2    2022-08-19 [1] CRAN (R 4.2.1)\n pillar           1.8.1    2022-08-19 [1] CRAN (R 4.2.1)\n pkgbuild         1.3.1    2021-12-20 [1] CRAN (R 4.1.2)\n pkgconfig        2.0.3    2019-09-22 [1] CRAN (R 4.0.0)\n plyr             1.8.7    2022-03-24 [1] CRAN (R 4.2.0)\n prettyunits      1.1.1    2020-01-24 [1] CRAN (R 4.0.0)\n processx         3.8.0    2022-10-26 [1] CRAN (R 4.2.1)\n promises         1.2.0.1  2021-02-11 [1] CRAN (R 4.0.4)\n ps               1.7.2    2022-10-26 [1] CRAN (R 4.2.1)\n purrr          * 0.3.5    2022-10-06 [1] CRAN (R 4.2.1)\n R6               2.5.1    2021-08-19 [1] CRAN (R 4.1.1)\n Rcpp           * 1.0.9    2022-07-08 [1] CRAN (R 4.2.1)\n RcppParallel     5.1.5    2022-01-05 [1] CRAN (R 4.1.2)\n readr          * 2.1.3    2022-10-01 [1] CRAN (R 4.2.1)\n readxl           1.4.1    2022-08-17 [1] CRAN (R 4.2.1)\n reprex           2.0.2    2022-08-17 [1] CRAN (R 4.2.1)\n reshape2         1.4.4    2020-04-09 [1] CRAN (R 4.0.0)\n rlang            1.0.6    2022-09-24 [1] CRAN (R 4.2.1)\n rmarkdown      * 2.18     2022-11-09 [1] CRAN (R 4.2.2)\n rprojroot        2.0.3    2022-04-02 [1] CRAN (R 4.2.2)\n rstan            2.21.7   2022-09-08 [1] CRAN (R 4.2.1)\n rstanarm       * 2.21.3   2022-04-09 [1] CRAN (R 4.2.0)\n rstantools       2.2.0    2022-04-08 [1] CRAN (R 4.2.0)\n rvest            1.0.3    2022-08-19 [1] CRAN (R 4.2.1)\n sass             0.4.2    2022-07-16 [1] CRAN (R 4.2.1)\n scales           1.2.1    2022-08-20 [1] CRAN (R 4.2.1)\n sessioninfo      1.2.2    2021-12-06 [1] CRAN (R 4.1.2)\n shiny            1.7.3    2022-10-25 [1] CRAN (R 4.2.1)\n shinyjs          2.1.0    2021-12-23 [1] CRAN (R 4.1.2)\n shinystan        2.6.0    2022-03-03 [1] CRAN (R 4.2.0)\n shinythemes      1.2.0    2021-01-25 [1] CRAN (R 4.0.4)\n StanHeaders      2.21.0-7 2020-12-17 [1] CRAN (R 4.0.3)\n stringi          1.7.8    2022-07-11 [1] CRAN (R 4.2.2)\n stringr        * 1.4.1    2022-08-20 [1] CRAN (R 4.2.1)\n survival         3.4-0    2022-08-09 [2] CRAN (R 4.2.2)\n threejs          0.3.3    2020-01-21 [1] CRAN (R 4.0.2)\n tibble         * 3.1.8    2022-07-22 [1] CRAN (R 4.2.1)\n tidyr          * 1.2.1    2022-09-08 [1] CRAN (R 4.2.1)\n tidyselect       1.2.0    2022-10-10 [1] CRAN (R 4.2.1)\n tidyverse      * 1.3.2    2022-07-18 [1] CRAN (R 4.2.1)\n timechange       0.1.1    2022-11-04 [1] CRAN (R 4.2.2)\n tzdb             0.3.0    2022-03-28 [1] CRAN (R 4.2.0)\n utf8             1.2.2    2021-07-24 [1] CRAN (R 4.1.0)\n vctrs            0.5.2    2023-01-23 [1] CRAN (R 4.2.2)\n vipor            0.4.5    2017-03-22 [1] CRAN (R 4.0.0)\n withr            2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun             0.34     2022-10-18 [1] CRAN (R 4.2.1)\n xml2             1.3.3    2021-11-30 [1] CRAN (R 4.1.2)\n xtable           1.8-4    2019-04-21 [1] CRAN (R 4.0.0)\n xts              0.12.2   2022-10-16 [1] CRAN (R 4.2.1)\n yaml             2.3.6    2022-10-18 [1] CRAN (R 4.2.1)\n zoo              1.8-11   2022-09-17 [1] CRAN (R 4.2.1)\n\n [1] /home/michael/.local/lib/R/library\n [2] /usr/lib/R/library\n\n────────────────────────────────────────────────────────────────────────\n\n\n\n\n",
    "preview": "posts/2023-02-08-effect-of-noise-on-egd/main_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2023-03-03T20:14:20+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-28-egd-theory-notes/",
    "title": "EGD theory notes",
    "description": "Captured notes on exponential growth detection.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2022-11-28",
    "categories": [
      "EGD",
      "theory"
    ],
    "contents": "\n\nContents\nStandard error of growth rate under IID multiplicative noise\n2022-03-28\n2022-08-06\n2022-11-28\n\n\nStandard error of growth rate under IID multiplicative noise\n2022-03-28\nUnder IID multiplicative noise, the exponential growth (EG) model is equivalent to the standard linear regression model applied to log abundance.\nThe standard error in the growth rate estimate is\n\\[\\begin{align}\n  se(\\hat r) = \\frac{\\sigma(\\varepsilon)}{\\sigma(t) \\sqrt{n}},\n\\end{align}\\]\nwhere \\(\\sigma(\\varepsilon)\\) is the standard deviation of the residual log measurement, \\(t\\) is the sampling times, and \\(n\\) is the number of samples.\nFor daily samples from \\(t=1\\) to \\(t=T\\), we have that \\(n = T\\) and that \\(\\sigma(t) = \\sqrt{(T^2-1)/12}\\).\nIn this case, the standard error is\n\\[\\begin{align}\n  se(\\hat r)\n    &= \\frac{\\sqrt{12} \\; \\sigma(\\varepsilon)}{\\sqrt{T (T^2 -2)}}\n  \\\\&\\approx \\frac{\\sqrt{12} \\; \\sigma(\\varepsilon)}{T^{3/2}} \\quad \\text{for $T \\gg 1$}.\n\\end{align}\\]\nThis formula tells us that our uncertainty decreases with more days of sampling as \\(T^{3/2}\\), with a factor \\(T\\) coming from the increased temporal spread of sampling days and a factor \\(T^{1/2}\\) coming from the increased number of samples.\nIf we included technical replicates, we could increase precision without requiring more days; however, an equal increase in precision requires processing and sequencing more samples from the current range of days than adding additional samples from additional days to the range.\n2022-08-06\nThis calculation just tells us about the standard error; it might be interesting to extend it to consider our ability to detect positive growth.\nincreasing the number of samples on a given day can only reduce the fraction of variance that is not day-by-day, e.g. due to sample processing and sequencing.\nshould think on a graphical / analogy / schematic representation of this for the team\n2022-11-28\nConsider a mixture model of lognormal + Poisson noise on the counts.\nI expect that the above applies in the regime where counts are >> 1 with very high probability, and the pure Poisson theory to apply when the counts are below some threshold defined by the (geometric) standard dev of the lognormal noise.\nHowever, I suspect we will often be in an intermediate regime for EGD when we are first able to detect an emerging pathogen, where the multiplicative noise is sufficient that it is not uncommon for the counts to be \\(\\lesssim 1\\) and \\(\\gg 1\\) on adjacent days.\nEven if that is so, perhaps the results for the two regimes still give us useful bounds.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-03-03T20:14:20+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-05-developing-a-qpcr-data-analysis-workflow/",
    "title": "Developing a qPCR data analysis workflow",
    "description": "In-progress qPCR data analysis workflow in R, using data from the 2022-Q2 Sprint spike-in experiment.",
    "author": [
      {
        "name": "Mike",
        "url": {}
      }
    ],
    "date": "2022-11-05",
    "categories": [
      "qPCR",
      "R"
    ],
    "contents": "\nSetup\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fs)\n# library(generics)\n\nlibrary(cowplot)\ntheme_set(theme_cowplot())\nlibrary(patchwork)\n\n\nThe data is from a folder in the NAO Drive currently called ‘Spike-in experiments’, which I’ve downloaded locally.\n\n\ndata_path <- here( '_data/nao/qpcr', '2022-06-29-spike-in-experiment/results')\ndir_ls(data_path) %>% path_file\n\n [1] \"!README.docx\"                          \n [2] \"2022-06-29-trip01.eds\"                 \n [3] \"2022-06-29-trip01.txt\"                 \n [4] \"2022-06-29-trip01.xlsx\"                \n [5] \"2022-06-29-trip01_2.eds\"               \n [6] \"2022-06-29-trip02.eds\"                 \n [7] \"2022-06-29-trip02.xlsx\"                \n [8] \"2022-06-30-trip02_009_seq_barcode.xlsx\"\n [9] \"2022-06-30_trip03.xlsx\"                \n[10] \"Results.ipynb\"                         \n[11] \"plate_layout_trip1.txt\"                \n[12] \"plate_layout_trip2.txt\"                \n[13] \"results.md5\"                           \n[14] \"trip1-template.edt\"                    \n\nTODO: try loading the data directly from Google Drive\nLoad qPCR data\nThis file assumes that the data is within a folder ‘data/’ within the root project folder.\nFile info:\n- Excel files have the raw and processed florescence measurements (Rn and Delta Rn), as well as the softwares autothreshold stuff in another sheet.\n- Also some sample metadata is here; however, we might want want to take that from the .txt file, since that is (I believe) closer to the original supplied table.\n\nTODO: Talk to Ari about whether we have/can save the files used to set up the qPCR experiment\nFirst, we can read in the relevant sections of the Excel file and clean up thedata a bit,\nFirst, load in the sample metadata.\nNOTE: Throughout, I’m using janitor::clean_names() to standardize the format of the column names\n\n\n\nsam <- path(data_path, '2022-06-29-trip01.txt') %>%\n  read_tsv(skip = 43) %>%\n  janitor::clean_names() %>%\n  mutate(\n    row = str_sub(well_position, 1, 1) %>% as.ordered,\n    column = str_sub(well_position, 2) %>% as.integer %>% as.ordered,\n  ) %>%\n  # relocate(row, column, .after = well_position) %>%\n  glimpse\n\nRows: 96\nColumns: 15\n$ well           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ well_position  <chr> \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8…\n$ sample_name    <chr> \"Blank\", \"NTC\", \"Trip1_010_Neg_Ctrl\", \"Trip1_…\n$ sample_color   <chr> \"RGB(0,139,69)\", \"RGB(142,56,142)\", \"RGB(139,…\n$ biogroup_name  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ biogroup_color <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ target_name    <chr> \"Blank\", \"Trip1_010_0\", \"Trip1_010_Neg_Ctrl\",…\n$ target_color   <chr> \"RGB(0,139,69)\", \"RGB(142,142,56)\", \"RGB(238,…\n$ task           <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"…\n$ reporter       <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FA…\n$ quencher       <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"…\n$ quantity       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ comments       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ row            <ord> A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, …\n$ column         <ord> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, …\n\nTODO: pre-specify the column types\nNOTE: In this experiment, the target name was incorrectly set to be identical to the sample name; instead, each non-blank sample should have the target corresponding to the primer/probe pair (here, 009 or 010). The import chunk therefore replaces the target name accordingly. The blank is ste to NA since no primers/master mix is added.\nTODO: confirm this with Anjali and Ari; and in future, make sure target corresponds to the relevant primer/probe set for that well.\nTODO: Suggest that we use more descriptive target names than ‘010’\n\nTODO: Suggest we find some way of directly adding the dilution and perhaps the concentration to the sample data, rather than having to parse from the sample name\n\nThere is additional sample data hidden in the sample names, which we’ll need to fix in future runs (it should be in its own columns in a table).\nBut for now we can parse it from the sample names.\nI’ll overwrite the faulty target names.\n\n\nsam <- sam %>%\n  separate(sample_name, \n    into = c('ww_triplicate', 'target_name', 'dilution_name'),\n    sep = '_', extra = 'merge',\n    remove = FALSE\n  )\n\n\nNOTE: The target name of the NTC is now NA, which is incorrect, but I’m not going to worry about that now.\nThe proper fix is for the target name to be fixed in the source data.\nNOTE: We need to use the actual starting concentrations for the standard curve. Need to talk to Anjali about how these should be supplied.\nHere I will assume the following.\n\n\n# concentration in copies per microliter\nconc_low <- 0.1\ndilution_step <- 20\nnum_samples <- 7\nconc_max <- conc_low * dilution_step^(num_samples - 1)\n\ndilution_df <- tibble(\n  dilution_power = 0:6,\n  dilution_factor = dilution_step^dilution_power,\n  conc = conc_max / dilution_factor,\n  dilution_name = str_c('D', '0', dilution_power + 1)\n)\n\n\nwhich we must add to the sample data,\n\n\nsam <- sam %>%\n  left_join(dilution_df, by = 'dilution_name')\n\n\nNext, load the amplification data — the relative florescence values (Rn and Delta Rn) — and join the sample metadata.\n\n\namp <- path(data_path, '2022-06-29-trip01.xlsx') %>%\n  readxl::read_excel(\n    sheet = 'Amplification Data',\n    skip = 40,\n    col_types = c('numeric', 'text', 'numeric', 'text', 'numeric', 'numeric')\n  ) %>%\n  janitor::clean_names() %>%\n  glimpse\n\nRows: 3,840\nColumns: 6\n$ well          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ well_position <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ cycle         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ target_name   <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"…\n$ rn            <dbl> 5.125966, 5.172779, 5.221941, 5.282046, 5.3525…\n$ delta_rn      <dbl> -0.228727385, -0.190693140, -0.150309995, -0.0…\n\nNote, this table has the original, incorrect target name.\nI’ll drop that, and join the sample metadata table which has the corrected target names.\n\n\namp <- amp %>%\n  select(-target_name) %>%\n  left_join(sam, by = c('well', 'well_position')) %>%\n  glimpse\n\nRows: 3,840\nColumns: 23\n$ well            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ well_position   <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A…\n$ cycle           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ rn              <dbl> 5.125966, 5.172779, 5.221941, 5.282046, 5.35…\n$ delta_rn        <dbl> -0.228727385, -0.190693140, -0.150309995, -0…\n$ sample_name     <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\",…\n$ ww_triplicate   <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\",…\n$ target_name     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ dilution_name   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sample_color    <chr> \"RGB(0,139,69)\", \"RGB(0,139,69)\", \"RGB(0,139…\n$ biogroup_name   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ biogroup_color  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ target_color    <chr> \"RGB(0,139,69)\", \"RGB(0,139,69)\", \"RGB(0,139…\n$ task            <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", …\n$ reporter        <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"F…\n$ quencher        <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", …\n$ quantity        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ comments        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ row             <ord> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,…\n$ column          <ord> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ dilution_power  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ dilution_factor <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conc            <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\nTODO: Read in the baseline start/stop points, so can view and check where in the amplifying/not portion\n\nExplore sample metadata\n\n\nsam %>%\n  count(target_name)\n\n# A tibble: 3 × 2\n  target_name     n\n  <chr>       <int>\n1 009            24\n2 010            24\n3 <NA>           48\n\nPlot the plate layout\nTODO: use code from vivo vitro to do this nicely; need to first parse the row and column from the well position\nTODO: see if can flow the sample names so that they print nicer\nQC checks\nCheck the blanks\nCheck the NTCs and Neg controls (though this is target-specific)\n\nWe’ll want to do the analysis separately for each target.\nMaybe there’s value in first looking at everything\nTODO: Set a fixed color scheme for the targets, and use this in all plots.\n\n\ndelta_rn_min <- 1e-3\np1 <- amp %>%\n  filter(!is.na(target_name)) %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  scale_color_brewer(type = 'qual') +\n  geom_point() +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nTODO: Consider if a better way to deal with non-positive values in the log-scale plot. See what the software does. (I think it might just not show these points.)\nAnalysis of a single target\nWill ultimately do this for all targets; perhaps show in tabs?\nFor now, I’ll use target 009.\n\n\namp_cur <- amp %>%\n  filter(target_name == '009')\n\n\n\n\ndelta_rn_min <- 1e-3\np1 <- amp_cur %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  scale_color_brewer(type = 'qual') +\n  geom_point() +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nPick threshold\nFor now, will pick the threshold manually.\n\n\nthreshold <- 3e-1\np1 / p2 &\n  geom_hline(yintercept = threshold)\n\n\n\nTODO: implement chosen auto-threshold algorithm.\nCompute Cq values\nTODO Ask Anjali if wants to use Ct or Cq as the name\nTODO Separate out into distinct files the code where I’m testing for myself, and demonstrating for others\n\nI’ll define a custom function estimate_cq to estimate the Cq value for a trajectotry crossing a given quantification threshold.\nSee the appendix below for more info.\n\n\nestimate_cq <- function(.data, threshold) {\n  .data <- .data %>%\n    arrange(cycle) %>%\n    mutate(\n      # NOTE: log transformation, important for interpolation\n      across(delta_rn, log),\n      below = delta_rn < threshold,\n      above = delta_rn > threshold,\n    )\n  before <- .data %>%\n    filter(below) %>%\n    slice_tail(n = 1)\n  after <- .data %>%\n    filter(above) %>%\n    slice_head(n = 1)\n  # And find the intersection of the line passing between these two points, and the threshold.\n  slope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\n  delta_rn_diff <- threshold - before$delta_rn\n  cycle_diff <- delta_rn_diff / slope\n  ct <- before$cycle + cycle_diff\n  ct\n}\n\n\nNow use this function to compute the Cq values for each trajectory\n\n\nsample_vars <-  sam %>% colnames\ncqs <- amp_cur %>%\n  with_groups(all_of(sample_vars), nest) %>%\n  mutate(\n    cq = map_dbl(data, estimate_cq, threshold = threshold)\n  ) %>%\n  select(-data) %>%\n  glimpse\n\nRows: 24\nColumns: 21\n$ well            <dbl> 51, 52, 53, 54, 55, 56, 57, 58, 63, 64, 65, …\n$ well_position   <chr> \"E3\", \"E4\", \"E5\", \"E6\", \"E7\", \"E8\", \"E9\", \"E…\n$ sample_name     <chr> \"Trip1_009_Neg_Ctrl\", \"Trip1_009_D07\", \"Trip…\n$ ww_triplicate   <chr> \"Trip1\", \"Trip1\", \"Trip1\", \"Trip1\", \"Trip1\",…\n$ target_name     <chr> \"009\", \"009\", \"009\", \"009\", \"009\", \"009\", \"0…\n$ dilution_name   <chr> \"Neg_Ctrl\", \"D07\", \"D06\", \"D05\", \"D04\", \"D03…\n$ sample_color    <chr> \"RGB(238,44,44)\", \"RGB(51,161,201)\", \"RGB(23…\n$ biogroup_name   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ biogroup_color  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ target_color    <chr> \"RGB(51,161,201)\", \"RGB(238,18,137)\", \"RGB(1…\n$ task            <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", …\n$ reporter        <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"F…\n$ quencher        <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", …\n$ quantity        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ comments        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ row             <ord> E, E, E, E, E, E, E, E, F, F, F, F, F, F, F,…\n$ column          <ord> 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 7, 8, 9…\n$ dilution_power  <int> NA, 6, 5, 4, 3, 2, 1, 0, NA, 6, 5, 4, 3, 2, …\n$ dilution_factor <dbl> NA, 6.4e+07, 3.2e+06, 1.6e+05, 8.0e+03, 4.0e…\n$ conc            <dbl> NA, 1.0e-01, 2.0e+00, 4.0e+01, 8.0e+02, 1.6e…\n$ cq              <dbl> 33.15247, 32.86074, 33.07862, 32.43584, 30.9…\n\nEstimate and plot standard curve\nQuestions:\n\n\ncqs <- cqs %>%\n  mutate(\n    conc_log10 = log10(conc) \n  )\n\n\n\n\nfit <- lm(cq ~ conc_log10, data = cqs)\nfit %>% summary\n\n\nCall:\nlm(formula = cq ~ conc_log10, data = cqs)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.006 -2.612 -0.405  1.888  3.032 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  33.8227     0.7298   46.35  < 2e-16 ***\nconc_log10   -2.0442     0.1872  -10.92 1.25e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.232 on 19 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.8626,    Adjusted R-squared:  0.8553 \nF-statistic: 119.3 on 1 and 19 DF,  p-value: 1.251e-09\n\n\n\ncqs %>%\n  ggplot(aes(conc_log10, cq)) +\n  geom_point() +\n  geom_abline(\n    intercept = coef(fit)[1],\n    slope = coef(fit)[2]\n  )\n\n\n\nNote, better to plot in a way that shows the uncertainty.\nCan use stat smooth, but then not connected to the fit we did.\nCould be better to use some of the ggdist et al tools.\n\n\ncqs %>%\n  ggplot(aes(conc_log10, cq)) +\n  geom_point() +\n  stat_smooth(method = 'lm')\n\n\n\n\n\nFALSE\n\n[1] FALSE\n\nEstimate the efficiency from the standard curve\nWe can estimate the efficiency from the slope of the standard curve using the standard formula,\n\n\nx <- fit %>% broom::tidy()\nslope <- coef(fit)['conc_log10']\nefficiency_estimate <- 10^(-1/slope) - 1\n\n\nIn this case, the estimate is unreasonably large because the standard curve isn’t good.\n90% confidence interval:\n\n\nslope_ci <- confint(fit, parm = 'conc_log10', level = 0.9)\nefficiency_ci <- 10^(-1/slope_ci) - 1\nefficiency_ci\n\n                5 %     95 %\nconc_log10 1.644336 2.812536\n\nQuite a large range!\nBayesian version with rstanarm\n\n\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\n\nlibrary(ggdist)\n\n\n\n\nstan_fit <- stan_glm(\n  cq ~ conc_log10, \n  data = cqs,\n)\nstan_fit %>% summary\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      cq ~ conc_log10\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 21\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 33.8    0.8 32.8  33.8  34.8 \nconc_log10  -2.0    0.2 -2.3  -2.0  -1.8 \nsigma        2.4    0.4  1.9   2.3   3.0 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 27.9    0.8 26.9  27.9  28.9 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3052 \nconc_log10    0.0  1.0  3186 \nsigma         0.0  1.0  2274 \nmean_PPD      0.0  1.0  2947 \nlog-posterior 0.0  1.0  1453 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nstan_fit %>% plot\n\n\n\ntry to get the posterior samples, so that we can then get the posterior of the efficiency estimate.\nTODO: google a better way to do this\n\n\nslope_post <- rstan::extract(stan_fit$stanfit)$beta\nefficiency_post <- 10^(-1/slope_post) - 1\nefficiency_post %>% qplot\n\n\n\nnote, we could use a stronger prior since we have a lot of relevant domain info.\nDemo how to use the standard curve for calibration\nAppendix\nFunction for finding ct in a well\nStandard method is to compute the Ct for each well independentally.\nNeed to interpolate between points in the trajectory, and find when the trajectory crosses the threshold.\nI wonder what the software does; simplest method is perhaps linear interpolation of log Delta Rn.\nTODO: google linear interpolation in R. This is essentially what geom_line is doing. It would be handy if I could just get access to that output. Can also google how to create a piecewise linear function.\nFirst try\nSince I don’t have wifi, I will need to hack it myself.\nFor each well, find the cycles immediately before and after the theshold crossing\nfind the intersection between the line segment between those two points and the horizontal line at the threshold.\n\n\n\nFALSE\n\n[1] FALSE\n\n\n\n# Suppose we have the trajectory for a particular well\nx <- amp_cur %>%\n  filter(well == 52) %>%\n  select(cycle, delta_rn) %>%\n  arrange(cycle)\n# We can get the crossing point as follows\nx <- x %>%\n  mutate(\n    below = delta_rn < threshold,\n    above = delta_rn > threshold,\n  )\nbefore <- x %>%\n  filter(below) %>%\n  slice_tail(n = 1)\nafter <- x %>%\n  filter(above) %>%\n  slice_head(n = 1)\n\n\nAnd find the intersection.\nline passing between these two points, and the threshold.\n\n\nslope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\ndelta_rn_diff <- threshold - before$delta_rn\ncycle_diff <- delta_rn_diff / slope\nct <- before$cycle + cycle_diff\n\n\nwe can put this all together in a function,\n\n\nestimate_cq <- function(.data, threshold) {\n  .data <- .data %>%\n    arrange(cycle) %>%\n    mutate(\n      # NOTE: log transformation, important for interpolation\n      across(delta_rn, log),\n      below = delta_rn < threshold,\n      above = delta_rn > threshold,\n    )\n  before <- .data %>%\n    filter(below) %>%\n    slice_tail(n = 1)\n  after <- .data %>%\n    filter(above) %>%\n    slice_head(n = 1)\n  # And find the intersection of the line passing between these two points, and the threshold.\n  slope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\n  delta_rn_diff <- threshold - before$delta_rn\n  cycle_diff <- delta_rn_diff / slope\n  ct <- before$cycle + cycle_diff\n  ct\n}\n\n\nways we could improve\ncheck that there is only one crossing\ncheck that the crossing is not in the noise region\n\nTry 2, with wifi\n\n\n# Suppose we have the trajectory for a particular well\nx <- amp_cur %>%\n  filter(well == 52) %>%\n  select(cycle, delta_rn) %>%\n  arrange(cycle)\n# We can get the crossing point as follows\nx <- x %>%\n  mutate(\n    below = delta_rn < threshold,\n    above = delta_rn > threshold,\n  )\nbefore <- x %>%\n  filter(below) %>%\n  slice_tail(n = 1)\nafter <- x %>%\n  filter(above) %>%\n  slice_head(n = 1)\n\n\nWe can use approxfun() to define the interpolating function.\n\n\nf <- approxfun(x$cycle, x$delta_rn %>% log, rule = 1)\na <- seq(from = -5, to = 45, by = 0.1)\nqplot(a, f(a))\n\n\n\nand then the intersection with e.g. a root-finding function, uniroot(). however, this approach is a bit funny because of the non-monotonicity in the noise region, and the fact that once we know the cycle interval we can calculate the intersection of the interpolation manually.\nEfficiency estimate\nDerivation:\nThe slope of the standard curve tells us how many extra cycles correspond to a 10X decrease in starting concentration.\nWith perfect efficiency of \\(E=1\\), this would equal \\(\\log(10) / \\log(2)\\).\nMore generally, the number of extra cycles is \\(A = \\log(10) / \\log(1 + E)\\), for any log base; taking the log base to be 10 gives \\(A = 1 / \\log(1 + E)\\).\nThe slope of the standard curve corresponds to \\(-A\\).\nTherefore, we can estimate \\(E\\) by\n\\[\\begin{align}\n  \\hat E = 10^{1 / A} - 1\n\\end{align}\\]\nMethods for setting the threshold\nhttps://www.researchgate.net/post/How-can-I-set-the-threshold-in-a-Real-Time-PCR-result has some discussion.\nOne suggestion is to find candidate points based on the maximum of the second derivative of the amplification curves\nMethods for finding the baseline region\nNot needed right now since we’re using the software’s determination of this.\nNext steps\nmake an r package that can house helper functions\ndevelop an autothreshold method\nadd the ability to check the baseline calculation\nsave output\n\n\n\n\n",
    "preview": "posts/2022-11-05-developing-a-qpcr-data-analysis-workflow/developing-a-qpcr-data-analysis-workflow_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2023-03-03T20:14:20+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-27-czid-r-analysis-demo/",
    "title": "CZID-to-R data import and analysis demo",
    "description": "Demonstration of how to import taxonomic profiles from CZID into R and do a few basic analyses.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2022-10-31",
    "categories": [
      "CZ ID",
      "hjelmso2019meta",
      "R"
    ],
    "contents": "\n\nContents\nR setup\nData import\nFrom a BIOM file (Hjelmsø et al. (2019) data)\nCreate a phyloseq object\n\n\nBasic data checks and stats\nTaxonomy\n\nAnalysis\nRecreating Figure 4 from Rothman et al. (2021) using the Hjelmso data\nBray-Curtis NMDS ordination (Panel A)\nAlpha diversity (Panel B)\nRelative abundances (Proportions) (Panel C)\nPut the panels together\n\n\n\nR setup\nStart by loading some useful R packages,\n\n\n# set of data analysis and viz tools\nlibrary(tidyverse)\n\n# file system helpers\nlibrary(fs)\n\n# specifying locations within a project\nlibrary(here)\n\n# microbiome analysis helpers\nlibrary(biomformat)\nlibrary(speedyseq)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n\nHere I’ll work with the BIOM file generated from the Hjelmsø et al. (2019) taxonomic profiles.\n\n\nhjelmso_data_path <- here(\"_data/hjelmso2019meta/czid\")\ndir_ls(hjelmso_data_path) %>% path_file\n\n[1] \"2022-11-07_combined_microbiome_file_nt_r.biom\"      \n[2] \"2022-11-07_combined_microbiome_file_nt_r_fixed.biom\"\n\nData import\nFrom a BIOM file (Hjelmsø et al. (2019) data)\nThe BIOM format (https://biom-format.org/, McDonald et al. (2012)) is a file format for including the abundance matrix, taxonomy, and sample metadata all in one file.\nBIOM export from CZID is supported but listed as being in Beta.\nIf we try reading in the file as directly exported from CZID, we get an error\n\n\nbiom <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  read_biom()\n\nError in validObject(.Object): invalid class \"biom\" object: type field has unsupported value\n\n\n\n\n\nThis error arises because the ‘type’ of the data object defined in the JSON-formatted contents of the .biom file isn’t valid as per the biom format v1.0 specs, see https://biom-format.org/documentation/format_versions/biom-1.0.html.\nWe can see this by opening up the file and looking for the type argument towards the beginning; or looking at the top items in the list after reading in the file with a JSON parser.\n\n\nbiom_json <- path(hjelmso_data_path, \n  '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  jsonlite::read_json()\nbiom_json %>% head(9)\n\n$id\n[1] \"None\"\n\n$format\n[1] \"Biological Observation Matrix 1.0.0\"\n\n$format_url\n[1] \"http://biom-format.org\"\n\n$matrix_type\n[1] \"sparse\"\n\n$generated_by\n[1] \"BIOM-Format 2.1.12\"\n\n$date\n[1] \"2022-11-07T20:37:42.385642\"\n\n$type\n[1] \"Table\"\n\n$matrix_element_type\n[1] \"float\"\n\n$shape\n$shape[[1]]\n[1] 25495\n\n$shape[[2]]\n[1] 85\n\nWe can fix the file by changing the type from ‘Table’ to something valid.\nIt doesn’t actually matter what we use:\n\nWhile type is a required entry in BIOM tables, the BIOM format itself does not change for different data types (e.g., OTU Table, function table, metabolite table). This information is included to allow tools that use BIOM files to determine the data type, if desired. (Caption for Additional file 5 in McDonald et al. (2012))\n\nLet’s use ‘Taxon table’.\nThe following code chunk should do the trick but is very slow, apparently because the jsonlite package is slow to work with large lists/files.\n\n\nbiom_json$type <- 'Taxon table'\njsonlite::write_json(\n  biom_json,\n  path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom')\n)\n\n\nSo I’ll instead simply replace the offending text.\n\n\nbiom_text <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  read_file\nstopifnot( identical(biom_text %>% str_count('\"Table\"'), 1L) )\nbiom_text_fixed <- biom_text %>%\n  str_replace('\"Table\"', '\"Taxon table\"')\nwrite_file(\n  biom_text_fixed,\n  path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom')\n)\n\n\nThe above chunk reads in the BIOM file’s contents as a single string, checks that ‘“Table”’ appears only once (in the field where it is set as the type), then replaces it with ‘“Taxon table”’), then writes the string as a new BIOM file.\nWe should now be able to load the corrected BIOM file with the biomformat package,\n\n\nbiom <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom') %>%\n  read_biom() %>%\n  print\n\nbiom object. \ntype: Taxon table \nmatrix_type: sparse \n25495 rows and 85 columns \n\nNote: For most objects in R, the print() and glimpse() methods silently return the object as well as printing information about it.\nAdding a print or glimpse call at the end of a variable-assignment pipe chain is a succinct way to save an object and show some info about it.\nCreate a phyloseq object\nThe abundance (count) matrix, sample metadata table, and taxonomy table can be extracted with three corresponding functions functions from the biomformat package.\nWe’ll tackle these one at a time.\nFirst, the abundance matrix.\n\n\nabun <- biom %>% biom_data()\nabun %>% class\n\n[1] \"dgCMatrix\"\nattr(,\"package\")\n[1] \"Matrix\"\n\nabun %>% dim\n\n[1] 25495    85\n\nThe abundance matrix is stored as a sparse matrix from the Matrix package.\nThat is fine for now, though phyloseq will want a standard (dense) matrix.\nNext, the sample metadata.\n\n\nsam <- biom %>% sample_metadata()\nsam %>% class\n\n[1] \"data.frame\"\n\nsam %>% head\n\n                      sample_type nucleotide_type collection_date\nERR3026532:288969 Airplane sewage             RNA         2013-01\nERR3026500:288937 Airplane sewage             RNA         2013-01\nERR3026576:289013 Airplane sewage             RNA         2013-01\nERR3026559:288996 Airplane sewage             RNA         2013-01\nERR3026571:289008 Airplane sewage             RNA         2013-01\nERR3026512:288949 Airplane sewage             RNA         2013-01\n                  water_control collection_location isolate\nERR3026532:288969            No            Pakistan      No\nERR3026500:288937            No               China      No\nERR3026576:289013            No             Denmark      No\nERR3026559:288996            No              Canada      No\nERR3026571:289008            No             Denmark      No\nERR3026512:288949            No               Japan      No\n                   Study Sample Name\nERR3026532:288969      Islamabad_2_c\nERR3026500:288937        Beijing_2_e\nERR3026576:289013    Library_blank_a\nERR3026559:288996          Toronto_e\nERR3026571:289008 Negative_control_a\nERR3026512:288949          Tokyo_1_d\n\nsam %>% glimpse\n\nRows: 85\nColumns: 7\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ `Study Sample Name` <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n\nThe sample metadata is a standard data frame with rownames giving sample ids, and the taxonomy information is stored as a list.\nNotice how all the variable names are in snake case except for one.\nThis is apparently because the CZID BIOM exports its own standard variables as snake case (though shows them otherwise in the online interface), but leaves custom variables unchanged.\nIt is convinient to standardize all variable names to snake case; an easy way to do this is with the function janitor::clean_names().\n\n\nsam <- sam %>%\n  janitor::clean_names() %>%\n  glimpse\n\nRows: 85\nColumns: 7\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ study_sample_name   <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n\nNext, the taxonomy table, or what the biomformat package calls the ‘observation metadata’.\n\n\ntax <- biom %>% observation_metadata()\ntax %>% class\n\n[1] \"list\"\n\ntax %>% head(2)\n\n$`Bacteria;;Proteobacteria;Alphaproteobacteria;Rhizobiales;Xanthobacteraceae;Azorhizobium;Azorhizobium caulinodans`\n                 taxonomy1                  taxonomy2 \n                \"Bacteria\"                         \"\" \n                 taxonomy3                  taxonomy4 \n          \"Proteobacteria\"      \"Alphaproteobacteria\" \n                 taxonomy5                  taxonomy6 \n             \"Rhizobiales\"        \"Xanthobacteraceae\" \n                 taxonomy7                  taxonomy8 \n            \"Azorhizobium\" \"Azorhizobium caulinodans\" \n\n$`Bacteria;;Proteobacteria;Gammaproteobacteria;Enterobacterales;Erwiniaceae;Buchnera;Buchnera aphidicola`\n            taxonomy1             taxonomy2             taxonomy3 \n           \"Bacteria\"                    \"\"      \"Proteobacteria\" \n            taxonomy4             taxonomy5             taxonomy6 \n\"Gammaproteobacteria\"    \"Enterobacterales\"         \"Erwiniaceae\" \n            taxonomy7             taxonomy8 \n           \"Buchnera\" \"Buchnera aphidicola\" \n\nWe can see that here we have a list, with one element per taxon.\nThe documentation for biomformat::observation_metadata indicates that this function may return a ‘data.frame’ rather than a list, if it is able to, but does not say under what conditions that will be the case.\nUltimately we want a data frame (or tibble).\nThe following code chunk checks which we have, and if we have a list, tries to turn it into a data frame by spreading out the taxonomy vector of each list element into a table.\n\n\ntax_tmp <- biom %>% observation_metadata()\nif (is.data.frame(tax_tmp)) {\n  tax <- tax_tmp %>% as_tibble(rownames = '.otu')\n} else {\n  tax <- tax_tmp %>% \n    enframe(name = 'feature_id') %>% \n    unnest_wider(value)\n}\nrm(tax_tmp)\ntax %>% glimpse\n\nRows: 25,495\nColumns: 9\n$ feature_id <chr> \"Bacteria;;Proteobacteria;Alphaproteobacteria;Rhi…\n$ taxonomy1  <chr> \"Bacteria\", \"Bacteria\", \"Bacteria\", \"Bacteria\", \"…\n$ taxonomy2  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ taxonomy3  <chr> \"Proteobacteria\", \"Proteobacteria\", \"Actinobacter…\n$ taxonomy4  <chr> \"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Ac…\n$ taxonomy5  <chr> \"Rhizobiales\", \"Enterobacterales\", \"Micrococcales…\n$ taxonomy6  <chr> \"Xanthobacteraceae\", \"Erwiniaceae\", \"Cellulomonad…\n$ taxonomy7  <chr> \"Azorhizobium\", \"Buchnera\", \"Cellulomonas\", \"Dict…\n$ taxonomy8  <chr> \"Azorhizobium caulinodans\", \"Buchnera aphidicola\"…\n\nTo facilitate downstream analysis, it is helpful to so some cleanup:\nReplace the taxonomic ranks with the standard NCBI rank names (see an example NCBI taxonomic record)\nIn cases where the rank is missing/unassigned, replace the empty string with NA\n\n\n\nrnks <- c('superkingdom', 'kingdom', 'phylum', 'class', 'order', 'family',\n  'genus', 'species')\ncolnames(tax)[2:9] <- rnks\n# use NA for missing ranks\ntax <- tax %>%\n  mutate(\n    across(everything(), ~ifelse(. == \"\", NA_character_, .))\n  )\ntax %>% glimpse\n\nRows: 25,495\nColumns: 9\n$ feature_id   <chr> \"Bacteria;;Proteobacteria;Alphaproteobacteria;R…\n$ superkingdom <chr> \"Bacteria\", \"Bacteria\", \"Bacteria\", \"Bacteria\",…\n$ kingdom      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ phylum       <chr> \"Proteobacteria\", \"Proteobacteria\", \"Actinobact…\n$ class        <chr> \"Alphaproteobacteria\", \"Gammaproteobacteria\", \"…\n$ order        <chr> \"Rhizobiales\", \"Enterobacterales\", \"Micrococcal…\n$ family       <chr> \"Xanthobacteraceae\", \"Erwiniaceae\", \"Cellulomon…\n$ genus        <chr> \"Azorhizobium\", \"Buchnera\", \"Cellulomonas\", \"Di…\n$ species      <chr> \"Azorhizobium caulinodans\", \"Buchnera aphidicol…\n\nNow let’s import all three tables into a single phyloseq object.\nThis involves converting each individual table into the corresponding class from the phyloseq package, and then combiningg these into one phyloseq-class object.\n\n\nps <- phyloseq(\n  otu_table(abun %>% as.matrix, taxa_are_rows = TRUE),\n  sample_data(sam),\n  tax_table(tax)\n)\n\n\nNote that we had to first coerce the abundance matrix to a standard dense matrix; we also needed to tell phyloseq that taxa corresponded to rows in the matrix.\nBasic data checks and stats\nTODO: explain below\n\n\nps <- ps %>%\n  mutate_sample_data(., \n    sample_sum = sample_sums(.)\n  )\nsam <- ps %>% sample_data %>% as_tibble\ntax <- ps %>% tax_table %>% as_tibble\n\n\n\n\nps %>% t\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 25495 taxa and 85 samples ]:\nsample_data() Sample Data:        [ 85 samples by 8 sample variables ]:\ntax_table()   Taxonomy Table:     [ 25495 taxa by 8 taxonomic ranks ]:\ntaxa are columns\n\n\n\nps %>% sample_names %>% head\n\n[1] \"ERR3026532:288969\" \"ERR3026500:288937\" \"ERR3026576:289013\"\n[4] \"ERR3026559:288996\" \"ERR3026571:289008\" \"ERR3026512:288949\"\n\nps %>% sample_data %>% glimpse\n\nRows: 85\nColumns: 8\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ study_sample_name   <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n$ sample_sum          <dbl> 83328, 188401, 313204, 234338, 767843, 5…\n\n\n\nsam %>%\n  ggplot(aes(sample_sum, fill = collection_location)) +\n  scale_x_log10() +\n  geom_histogram()\n\n\n\n\n\ntaxon_stats <- ps %>%\n  as_tibble %>%\n  mutate(across(superkingdom, fct_explicit_na)) %>%\n  with_groups(c(.otu, superkingdom), summarize, \n    prev_1 = sum(.abundance >= 1),\n    prev_10 = sum(.abundance >= 10),\n    total = sum(.abundance),\n    proportion = mean(.abundance / sample_sum)\n  )\n\n\n\n\ntaxon_stats %>%\n  pivot_longer(-c(.otu, superkingdom)) %>%\n  ggplot(aes(value, fill = superkingdom)) +\n  facet_wrap(~name, scales = 'free') +\n  scale_x_log10() +\n  scale_fill_brewer(type = 'qual') +\n  geom_histogram() \n\n\n\nTaxonomy\nNCBI taxonomy has recently received changes in some prokaryotic phylum names.\nLet’s check to see which version of phylum names are being used here, by seeing whether a Bacteroides species’ phylum is listed as ‘Bacteroidetes’ (old name) or ‘Bacteroidota’ (new name).\n\n\ntax %>%\n  filter(genus == 'Bacteroides') %>%\n  slice(1)%>%\n  glimpse\n\nRows: 1\nColumns: 9\n$ .otu         <chr> \"Bacteria;;Bacteroidetes;Bacteroidia;Bacteroida…\n$ superkingdom <chr> \"Bacteria\"\n$ kingdom      <chr> NA\n$ phylum       <chr> \"Bacteroidetes\"\n$ class        <chr> \"Bacteroidia\"\n$ order        <chr> \"Bacteroidales\"\n$ family       <chr> \"Bacteroidaceae\"\n$ genus        <chr> \"Bacteroides\"\n$ species      <chr> \"Bacteroides fragilis\"\n\nIf we look at this taxon in NCBI taxonomy, we can see that NCBI has adopted the new phylum name ‘Bacteroidota’; however, here we see the old phylum name.\nThis suggests that CZID is currently using an older version of NCBI prior to the name change.\nsee\n- https://ncbiinsights.ncbi.nlm.nih.gov/2021/12/10/ncbi-taxonomy-prokaryote-phyla-added/\n- https://www.the-scientist.com/news-opinion/newly-renamed-prokaryote-phyla-cause-uproar-69578\n\nCheck classification percentages\n\n\ntax %>%\n  pivot_longer(-.otu, names_to = 'rank') %>%\n  with_groups(rank, summarize,\n    features_classified = sum(!is.na(value)),\n    features_total = n()\n  ) %>%\n  mutate(\n    frac_classified = features_classified / features_total,\n    rank = factor(rank, rank_names(ps))\n  ) %>%\n  arrange(rank)\n\n# A tibble: 8 × 4\n  rank         features_classified features_total frac_classified\n  <fct>                      <int>          <int>           <dbl>\n1 superkingdom               25381          25495           0.996\n2 kingdom                     4995          25495           0.196\n3 phylum                     22667          25495           0.889\n4 class                      22119          25495           0.868\n5 order                      22199          25495           0.871\n6 family                     21744          25495           0.853\n7 genus                      20649          25495           0.810\n8 species                    25493          25495           1.00 \n\nThis analysis points to some notable features of the data.\nFor example, not every taxonomic feature has a superkingdom.\nLet’s take a look at some of those ‘species’ that don’t,\n\n\nset.seed(42)\ntax %>%\n  filter(is.na(superkingdom)) %>%\n  select(superkingdom, kingdom, genus, species) %>%\n  slice_sample(n=20)\n\n# A tibble: 20 × 4\n   superkingdom kingdom genus species                                 \n   <chr>        <chr>   <chr> <chr>                                   \n 1 <NA>         <NA>    <NA>  Cloning vector pPKm-230                 \n 2 <NA>         <NA>    <NA>  Cloning vector pMT450                   \n 3 <NA>         <NA>    <NA>  Vector pAAV-hSyn1-FLEX-GAP43-GCaMP6s-P2…\n 4 <NA>         <NA>    <NA>  uncultured microorganism                \n 5 <NA>         <NA>    <NA>  Cloning vector shRNA EYFP-P2A Puro      \n 6 <NA>         <NA>    <NA>  Cloning vector pMT449                   \n 7 <NA>         <NA>    <NA>  IncQ plasmid pIE1120                    \n 8 <NA>         <NA>    <NA>  Cloning vector pHal7-FAPG462VRFP        \n 9 <NA>         <NA>    <NA>  Shuttle vector pG106                    \n10 <NA>         <NA>    <NA>  uncultured gut microbe of Zootermopsis …\n11 <NA>         <NA>    <NA>  Vector EP-Pol                           \n12 <NA>         <NA>    <NA>  Cloning vector pRGPDuo4                 \n13 <NA>         <NA>    <NA>  Cloning vector IA5_YQR_DIMER            \n14 <NA>         <NA>    <NA>  Plasmid pMCBF1                          \n15 <NA>         <NA>    <NA>  Plasmid pM3                             \n16 <NA>         <NA>    <NA>  uncultured marine organism              \n17 <NA>         <NA>    <NA>  Transposon Tn4551                       \n18 <NA>         <NA>    <NA>  Sphinx1.76-related DNA                  \n19 <NA>         <NA>    <NA>  Cloning vector pMT451                   \n20 <NA>         <NA>    <NA>  uncultured marine microorganism         \n\nCan see that CZID report reference sequences that are in NT but don’t corresopnd to known organisms.\nWhat about ‘species’ without intermediate ranks?\n\n\ntax %>%\n  filter(!is.na(superkingdom), is.na(family)) %>%\n  select(superkingdom, kingdom, phylum, family, genus, species) %>%\n  slice_sample(n=20)\n\n# A tibble: 20 × 6\n   superkingdom kingdom phylum          family genus           species\n   <chr>        <chr>   <chr>           <chr>  <chr>           <chr>  \n 1 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n 2 Bacteria     <NA>    <NA>            <NA>   <NA>            swine …\n 3 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n 4 Bacteria     <NA>    <NA>            <NA>   <NA>            butyra…\n 5 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n 6 Bacteria     <NA>    Chloroflexi     <NA>   Dehalogenimonas Dehalo…\n 7 Bacteria     <NA>    Proteobacteria  <NA>   <NA>            alpha …\n 8 Eukaryota    <NA>    Bacillariophyta <NA>   <NA>            uncult…\n 9 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n10 Bacteria     <NA>    Actinobacteria  <NA>   <NA>            actino…\n11 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n12 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n13 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n14 Bacteria     <NA>    Proteobacteria  <NA>   <NA>            arseni…\n15 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n16 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n17 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n18 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n19 Archaea      <NA>    <NA>            <NA>   <NA>            uncult…\n20 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n\nCan also see cases where a species does not have an intermediate rank defined, such as family.\nHaving NA for intermediate ranks could cause issues, and we might consider replacing these with a new string such as ‘Enterobacterales_unclassified’.\nHow do features break down by superkingdom?\n\n\ntax %>% \n  count(superkingdom) %>%\n  mutate(fraction = n / sum(n))\n\n# A tibble: 5 × 3\n  superkingdom     n fraction\n  <chr>        <int>    <dbl>\n1 Archaea        391  0.0153 \n2 Bacteria     16583  0.650  \n3 Eukaryota     5655  0.222  \n4 Viruses       2752  0.108  \n5 <NA>           114  0.00447\n\nAnalysis\nRecreating Figure 4 from Rothman et al. (2021) using the Hjelmso data\nFigure 4 from Rothman et al. (2021) combines three common types of plots in microbiome analysis: An ordination plot to visualize the similarities and differences between samples, a plot showing the distribution of an alpha diversity metric (Shannon index) across samples, and the proportions (relative abundance) of particular species across samples (faceted by species).\nHere I’ll show how to (mostly) recreate this plot using the Hjelmso data.\nFirst, we’ll filter out some samples and taxa, which is a typical first step to any analysis.\nThere is a lot more to say about how you might do said filtering; but here I’ll\nRemove samples with very low read counts, since the low read counts can be a sign of experimental issues with those samples and can skew interpretation of some analyses\nSubset to just viruses (the Rothman analysis only considers viruses)\nRemove species not appearing in at least 2 samples and 10 reads, which will speed up calculations and likely make our results more meaningful since these identifications can easily be spurious.\nAggregate to the genus level\n\n\n\nps_plot <- ps %>%\n  filter_sample_data(sample_sum > 1e5) %>%\n  filter_tax_table(superkingdom == 'Viruses') %>%\n  filter_taxa2(~ sum(. > 0) > 2 & sum(.) >= 10) %>%\n  tax_glom('genus', NArm = TRUE) %>%\n  print\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 110 taxa and 84 samples ]:\nsample_data() Sample Data:        [ 84 samples by 8 sample variables ]:\ntax_table()   Taxonomy Table:     [ 110 taxa by 8 taxonomic ranks ]:\ntaxa are rows\n\nNow there are only 110 species being considered, compared to 25495 in the entire CZID output.\nNote that taxa with a missing genus name have been filtered out; this is the phyloseq default but it can have big effects so need to be aware and consider whether this is desired for a given analysis.\nNote, the names of the taxonomic features after aggregation are set to the name of the most abundant feature within the genus; they are not automatically set to the genus name.\nThat is easy to do manually, provided that you have unique genus names (which is not always the case).\nIt can be useful to check if the genus names are unique for plotting by genus later on,\n\n\nps_plot %>% tax_table %>% as_tibble %>% pull(genus) %>% anyDuplicated\n\n[1] 0\n\nThere are no duplicates, so we can uniquely refer to taxa by genus name.\nBray-Curtis NMDS ordination (Panel A)\nThere are many ways to do this; here I’ll use the ordinate() and plot_ordination() helper function from phyloseq to create the NMDS plot using the Bray-Curtis community dissimilarity metric.\nNote, that it is important to manually normalize the abundances to have the same total in each sample (e.g. by normalizing to proportions, as done here), otherwise the different total counts across samples will affect the results.\n\n\nnmds <- ps_plot %>%\n  transform_sample_counts(~ . / sum(.)) %>%\n  ordinate(method = \"NMDS\", distance = \"bray\", trymax = 50)\n\nRun 0 stress 0.1002992 \nRun 1 stress 0.1173969 \nRun 2 stress 0.1480709 \nRun 3 stress 0.1173968 \nRun 4 stress 0.1002992 \n... Procrustes: rmse 3.717243e-06  max resid 1.691263e-05 \n... Similar to previous best\nRun 5 stress 0.1424676 \nRun 6 stress 0.130377 \nRun 7 stress 0.1002992 \n... Procrustes: rmse 2.661887e-06  max resid 1.175533e-05 \n... Similar to previous best\nRun 8 stress 0.1002992 \n... Procrustes: rmse 1.104143e-05  max resid 6.17019e-05 \n... Similar to previous best\nRun 9 stress 0.1173976 \nRun 10 stress 0.1173975 \nRun 11 stress 0.1292318 \nRun 12 stress 0.1294527 \nRun 13 stress 0.1002992 \n... New best solution\n... Procrustes: rmse 5.355197e-06  max resid 1.53618e-05 \n... Similar to previous best\nRun 14 stress 0.1629023 \nRun 15 stress 0.117396 \nRun 16 stress 0.1417549 \nRun 17 stress 0.1417084 \nRun 18 stress 0.1002992 \n... Procrustes: rmse 1.162339e-05  max resid 7.001561e-05 \n... Similar to previous best\nRun 19 stress 0.1002992 \n... Procrustes: rmse 7.305224e-06  max resid 4.166893e-05 \n... Similar to previous best\nRun 20 stress 0.1292315 \n*** Best solution repeated 3 times\n\np_ord <- plot_ordination(ps_plot, nmds, \n  color = \"collection_location\", type = \"samples\"\n) +\n  labs(color = 'Country')\np_ord \n\n\n\nAlpha diversity (Panel B)\nWe can compute Shannon alpha diversity index for each sample in a variety of ways:\nphyloseq::estimate_richness()\nvegan::diversity()\nPerforming the calculation ourselves from the definition\n\n\n\nshannon_index <- otu_table(ps_plot) %>% \n  orient_taxa(as = 'cols') %>%\n  vegan::diversity()\nshannon_index %>% head\n\nERR3026500:288937 ERR3026576:289013 ERR3026559:288996 \n        1.3500820         0.6443310         1.3777211 \nERR3026571:289008 ERR3026512:288949 ERR3026526:288963 \n        0.9189514         2.0592347         1.6738584 \n\nNote that we needed to reorient the abundance matrix (i.e. OTU table) to have taxa corresponding to columns, as this is what functions in the vegan package expect.\nWe can tell that we used the correct orientation because the resulting diversity values are in a named vector where the names correspond to the sample names.\nIf we had passed the matrix in the incorrect orientation, then the vector names would be the taxa names.\nLet’s add the Shannon index to a copy of the sample data,\n\n\nsam_plot <- ps_plot %>% sample_data %>% as_tibble %>%\n  add_column(shannon_index = shannon_index)\n\n\nthen create the plot,\n\n\np_div <- sam_plot %>%\n  ggplot(aes(y = shannon_index, x = collection_location, \n      color = collection_location)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_quasirandom() +\n  expand_limits(y = 0) +\n  # scale_color_manual(values = colors_countries) +\n  labs(x = 'Country', y = 'Shannon index') +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    legend.position = 'none'\n  )\np_div\n\n\n\nNote, I’m plotting the data points over the box plots, since it is useful to see the scatter when we’re working with a relatively small number of points like this.\nI therefore turned off the plotting of outliers in the boxplot layer.\nNote, I suggest using the exponential of the Shannon index and plotting on a log scale), so that the numbers shown on the axis are in terms of effective number of species.\nRelative abundances (Proportions) (Panel C)\nIn the actual Rothman figure, the abundances for a set of viruses are shown; the particular viruses were picked based on an analysis to determine viruses that vary across treatment plant, using the ANCOM R package.\nI may do that in a future version of this script, but for now I’ll just pick the 10 most abundant viruses by average proportion.\nFirst, get a data frame for plotting, with the proportions of all taxa alongside the original read counts,\n\n\nx <- ps_plot %>%\n  as_tibble %>%\n  with_groups(.sample, mutate,\n    proportion = .abundance / sum(.abundance))\n\n\nNext, get the top 10 viruses by median proportion. We can do this various ways, e.g.\n\n\ntop_viruses1 <- ps_plot %>% \n  transform_sample_counts(~ . / sum(.)) %>%\n  orient_taxa(as = 'rows') %>%\n  otu_table %>%\n  apply(1, median) %>%\n  sort(decreasing = TRUE) %>%\n  head(10) %>%\n  names\n\n\nor\n\n\ntop_viruses2 <- x %>%\n  with_groups(.otu, summarize, across(proportion, median)) %>%\n  slice_max(proportion, n = 10) %>%\n  print %>%\n  pull(.otu)\n\n# A tibble: 10 × 2\n   .otu                                                        propo…¹\n   <chr>                                                         <dbl>\n 1 Viruses;Orthornavirae;Kitrinoviricota;Alsuviricetes;Martel… 0.282  \n 2 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.0821 \n 3 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Siphoviri… 0.0456 \n 4 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Siphoviri… 0.0127 \n 5 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Herellevi… 0.00844\n 6 Viruses;Orthornavirae;Pisuviricota;Duplopiviricetes;Durnav… 0.00821\n 7 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.00500\n 8 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Podovirid… 0.00478\n 9 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.00353\n10 Viruses;Orthornavirae;Pisuviricota;Pisoniviricetes;Picorna… 0.00323\n# … with abbreviated variable name ¹​proportion\n\nidentical(top_viruses1, top_viruses2)\n\n[1] TRUE\n\n\n\np_prop <- x %>%\n  filter(.otu %in% top_viruses1) %>%\n  mutate(\n    across(genus, fct_reorder, proportion, .fun = median, .desc = TRUE),\n  ) %>%\n  ggplot(aes(x = collection_location, y = proportion,\n      color = collection_location)) +\n  facet_wrap(~genus, nrow = 2, scales = 'free_y') +\n  # scale_y_log10() +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(y = 'Proportion', x = 'Country') +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    legend.position = 'none'\n  )\np_prop\n\n\n\nNote, I’ve ordered the facets as decreasing in median proportion.\nPut the panels together\nPutting multiple panels together is often very easy with the patchwork package loaded,\n\n\n(p_ord + p_div) / p_prop +\n  plot_annotation(tag_levels = 'A')\n\n\n\nThis plot could definitely benefit from some extra fiddling, to adjust the spacing and colors etc.\n\n\n\nHjelmsø, Mathis Hjort, Sarah Mollerup, Randi Holm Jensen, Carlotta Pietroni, Oksana Lukjancenko, Anna Charlotte Schultz, Frank M. Aarestrup, and Anders Johannes Hansen. 2019. “Metagenomic Analysis of Viruses in Toilet Waste from Long Distance Flights New Procedure for Global Infectious Disease Surveillance.” PLOS ONE 14 (1): e0210368. https://doi.org/10.1371/journal.pone.0210368.\n\n\nMcDonald, Daniel, Jose C Clemente, Justin Kuczynski, Jai Ram Rideout, Jesse Stombaugh, Doug Wendel, Andreas Wilke, et al. 2012. “The Biological Observation Matrix (BIOM) Format or: How I Learned to Stop Worrying and Love the Ome-Ome.” GigaScience 1 (1): 2047-217X-1-7. https://doi.org/10.1186/2047-217X-1-7.\n\n\nRothman, Jason A., Theresa B. Loveless, Joseph Kapcia, Eric D. Adams, Joshua A. Steele, Amity G. Zimmer-Faust, Kylie Langlois, et al. 2021. “RNA Viromics of Southern California Wastewater and Detection of SARS-CoV-2 Single-Nucleotide Variants.” Applied and Environmental Microbiology 87 (23): e01448–21. https://doi.org/10.1128/AEM.01448-21.\n\n\n\n\n",
    "preview": "posts/2022-10-27-czid-r-analysis-demo/czid-r-analysis-demo_files/figure-html5/unnamed-chunk-19-1.svg",
    "last_modified": "2023-03-03T20:14:20+00:00",
    "input_file": {}
  }
]
