[
  {
    "path": "posts/2023-02-08-effect-of-noise-on-egd/",
    "title": "Examining the effect of noise on Exponential Growth Detection",
    "description": {},
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "EGD",
      "R",
      "theory"
    ],
    "contents": "\n\nContents\nBackground\nAnalysis\nNext steps\nDiscussion\nSession info\n\n\n\n# set of data analysis and viz tools\nlibrary(tidyverse)\nlibrary(furrr)\nplan(multisession, workers = 3)\n# file system helpers\nlibrary(fs)\n# specifying locations within a project\nlibrary(here)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\nlibrary(ggdist)\ntheme_set(theme_cowplot())\n\n# stats\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\nlibrary(broom.mixed)\n\n\nBackground\nWe’ve done some examination of the ability to detect exponential increase when there is no noise beyond the Poisson error process associated with sequencing.\nBut we know there is additional noise due to processes such as shedding, sample collection, and sample processing.\nQuestions this analysis considers:\nHow does increasing the amount of noise in the relative abundance of pathogen in the sequencing data reduce our power to estimate its growth rate or infer that it is exponentially increasing?\nHow much additional sequencing effort is required to make up for a given increase in noise/dispersion? this is a naive question — we know from statistical principles that you can’t really make up for more noise upstream of sequencing by doing more sequencing; you need to make new measurements that have independent noise. But I consider it here for illustration purposes.\nAnalysis\nAssume a Gamma-Poisson (also known as Negative Binomial) model for both the real data and the statistical inference model.\nI will start with a simple model.\nFor now, I will assume that the fraction of the population that is infected grows exponentially without any noise.\n\\[\\begin{align}\n  i(t) = i(0) \\exp(r t),\n\\end{align}\\]\nand that number of reads of the pathogen in the sample from time \\(t\\), \\(M(t)\\), has expectation\n\\[\\begin{align}\n  E[M(t)] &= i(t) \\cdot s B \\mathcal M\n        \\\\&\\equiv i(s) \\cdot a,\n\\end{align}\\]\nwhere\n\\(s\\) is the rate of pathogen shedding in infected, relative to background microbiome\n\\(B\\) is the measurement efficiency (bias), relative to background microbiome\n\\(\\mathcal M\\) is the total number of sequencing reads\nThis formula for the mean read count of the pathogen is derived elsewhere.\nFor simplicity I’m treating all of \\(s\\), \\(B\\), and \\(\\mathcal M\\) as fixed parameters.\nSince all of these parameters are constants that multiply together, it is convenient for later calculations to define \\(a \\equiv sB\\mathcal M\\).\nTODO\nExplain nuance regarding noise etc\nConsider using more nuanced model for expected pathogen reads based on integral of \\(i(t)\\).\nI assume a Negative Binomial model for the number of reads of the pathogen in the sample from time \\(t\\), using the ‘alternative parameterization’ used described in the Stan docs; however, I’ll follow Rstanarm and use \\(\\theta\\) in place of \\(\\phi\\) for the reciprocal dispersion parameter.\nIf \\(E[M] = \\mu\\), then the variance of \\(M\\) is\n\\[\\begin{align}\n  \\text{Var}[M]\n    &= \\mu + \\frac{\\mu^2}{\\theta}\n  \\\\&= \\mu \\left(1 + \\frac{\\mu}{\\theta} \\right),\n\\end{align}\\]\nand the coefficient of variation is\n\\[\\begin{align}\n  \\text{CV}[M]\n    &= \\frac{\\sqrt{\\text{Var}[M]}}{E[M]}\n  \\\\&= \\sqrt{\\frac{1}{\\mu} + \\frac{1}{\\theta}}.\n\\end{align}\\]\n(speculative) Intuitively, the coefficient of variation is the measure of noise relevant for our power to infer exponential growth rate.\nThe CV is Poisson-like when \\(\\mu \\ll \\theta\\) and Gamma-like when \\(\\mu \\gg \\theta\\).\nIn the Poisson regime, we can reduce the CV by increasing sequencing effort and hence increasing \\(\\mu\\);\nhowever, once \\(\\mu \\gg \\theta\\), increasing the sequencing effort will have a negligible benefit.\nInstead, we need to collect additional samples and/or re-measure existing samples so as to effectively average out the extra-Poisson noise.\nI will simulate using stats::rnbinom with the parameterization matching\nThe size argument in rnbinom is the (reciprocal) dispersion parameter \\(\\phi\\) in stan docs, but which is called \\(\\theta\\) in rstanarm::neg_binomial_2.\n\n\n# Total time of monitoring, in days\ntotal_time <- 20\nsampling_days <- seq(0, total_time - 1, by = 1)\n# Initial fraction of population infected\ni_0 <- 1e-4\n# Growth rate of infections to correspond to 4 doublings\ndoubling_time <- 5\nr <- log(2) / doubling_time\n# Multiplier a s.t. expected number of reads spans 1 over the range\na <- 0.4 / i_0\n# a * i_0 * exp(r * sampling_days)\n\n# reciprocal-dispersion parameter; smaller values = lower variance\ntheta_sim <- 5e-1\n\nset.seed(42)\nsim <- tibble(t = sampling_days) %>%\n  mutate(\n    i_t = i_0 * exp(r * t),\n    M_t_expected = a * i_t,\n    # lambda_t = rgamma(n(), shape = 1, rate = 1 / M_t_expected),\n    # M_t = rpois(n(), lambda_t)\n    M_t = rnbinom(n(), size = theta_sim, mu = M_t_expected)\n  )\n\n\n\n\nsim %>%\n  ggplot(aes(t)) +\n    geom_line(aes(y = M_t_expected)) +\n    geom_point(aes(y = M_t))\n\n\n\nNow we want to repeat the simulations many times for a range of values of \\(\\theta\\) and \\(a\\).\n\n\nsimulate_monitoring <- function(theta, a) {\n  tibble(t = sampling_days) %>%\n    mutate(\n      i_t = i_0 * exp(r * t),\n      M_t_expected = a * i_t,\n      M_t = rnbinom(n(), size = theta, mu = M_t_expected)\n    )\n}\n\nset.seed(42)\n\nsims <- crossing(\n  theta = c(3e-2, 1e-1, 3e-1, 1e0),\n  # a = 4000 * c(1, 3, 10, 30),\n  a = 4000 * c(1, 10, 100),\n  rep = 1:40\n) %>%\n  mutate(\n    data = map2(theta, a, simulate_monitoring),\n    M_total = map_dbl(data, ~ sum(.x$M_t))\n  )\n\n\n\n\nsims %>%\n  unnest(data) %>%\n  ggplot(aes(t, M_t)) +\n  facet_grid(a ~ theta, scales = 'free_y') +\n  geom_line(aes(group = rep))\n\n\n\nCHECK: Do any simulations have no observations?\n\n\nsims %>%\n  count(theta, M_total == 0)\n\n# A tibble: 5 × 3\n  theta `M_total == 0`     n\n  <dbl> <lgl>          <int>\n1  0.03 FALSE            113\n2  0.03 TRUE               7\n3  0.1  FALSE            120\n4  0.3  FALSE            120\n5  1    FALSE            120\n\nYes! We need to handle these separately, since they will cause errors when we try to fit the GLM.\n\n\nFALSE\n\n[1] FALSE\n\nLet’s try fitting on a subset, for now using the ‘optimizing’ algorithm to speed things up.\nWe must restrict ourselves to cases where the total count is at least 1, to be able to fit.\nWe might consider restricting ourselves to a higher count than this.\nTODO: disable centering the predictors; otherwise, need to change our intercept prior to be based on the midpoint of the simulation.\n\n\nsims_fit <- sims %>%\n  # filter(rep <= 2, M_total > 0) %>%\n  filter(M_total > 0) %>%\n  mutate(\n    prior_intercept_mean = log(a * i_0),\n    # Note: For testing with 'optimizing', we can fit in parallel\n    fit = map(data, ~stan_glm(\n        M_t ~ t,, \n        data = .x,\n        family = neg_binomial_2,\n        prior = normal(0, 0.5, autoscale = FALSE),\n        # prior_intercept = normal(prior_intercept_mean, 2.5, autoscale = FALSE),\n        prior_aux = exponential(2, autoscale = FALSE),\n        algorithm = 'optimizing',\n    ))\n  )\n\n\nTODO\nLook at a single model and its fit against the ‘real’ data.\nConsider the prior on theta, and explicitly code it in. For the purposes of this study, I can make the prior accurately reflect the actual range of theta values that I’m using. In fact, it would even make sense to set the prior tightly around the correct value of theta, which should also speed up the inference.\nreconsider the prior on the intercept; could set to a range like with theta though that doesn’t make too much sense if we think of the fact that we’re modeling an increase in sequencing effort and so we have extra info that the mean should increase accordingly\n\n\nfit <- sims_fit %>% pull(fit) %>% pluck(1)\nfit\n\nstan_glm\n family:       neg_binomial_2 [log]\n formula:      M_t ~ t\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept)  2.9    2.0  \nt           -0.4    0.3  \n\nAuxiliary parameter(s):\n                      Median MAD_SD\nreciprocal_dispersion 0.1    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nfit %>% tidy(conf.int = TRUE)\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    2.86      1.96    -0.536    6.58  \n2 t             -0.429     0.272   -0.914    0.0584\n\nfit %>% prior_summary\n\nPriors for model '.' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n ~ normal(location = 0, scale = 0.5)\n\nAuxiliary (reciprocal_dispersion)\n ~ exponential(rate = 2)\n------\nSee help('prior_summary.stanreg') for more details\n\nOur goal is to assess how dispersion impacts our ability to infer the exponential trend.\nOne way we can do that is plot credible intervals for the growth rate \\(r\\), for all simulations, group by theta, against the actual growth rate.\nAn easy way to get CIs is with broom.mixed::tidy(),\n\n\nfit %>% tidy(conf.int = TRUE, conf.level = 0.9)\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    2.86      1.96    -0.536    6.58  \n2 t             -0.429     0.272   -0.914    0.0584\n\nBesides correctly inferring the growth rate, we are also interested in the our posterior that the sequence is increasing (possibly above a certain rate of increase; here I’ll just consider a rate above 0).\nWe can do this for a single fit like\n\n\n# post <- fit %>% as.matrix %>% as_tibble %>% janitor::clean_names() %>%\n  # glimpse\nfit %>% as.matrix %>% {mean(.[, 't'] > 0)}\n\n[1] 0.075\n\n\n\nx <- sims_fit %>%\n  mutate(\n    prob_increasing = map_dbl(fit, ~ .x %>% as.matrix %>% {mean(.[, 't'] > 0)}),\n    fit = map(fit, tidy, conf.int = TRUE, conf.level = 0.9),\n  ) %>%\n  unnest(fit) %>%\n  filter(term == 't')\n\n\n\n\nx %>%\n  ggplot(aes(y = rep, x = estimate)) +\n  facet_grid(a ~ theta) +\n  geom_pointinterval(\n    aes(xmin = conf.low, xmax = conf.high),\n    fatten_point = 1\n  ) +\n  geom_vline(xintercept = 0, color = 'grey') +\n  geom_vline(xintercept = r, linetype = 2, color = 'darkred')\n\n\n\nNote that for lambda=0.03, some intervals are missing; these are cases where the dispersion was so high that we never saw the pathogen.\nIn those cases, our estimate of the growth rate is simply the prior; perhaps we can show that?\nFrom this graph, it looks like a 10X increase in dispersion requires more than a 10X increase in sequencing effort to achieve the same power.\nLet’s check how calibrated these CIs are, by comparing the proportion of fits that contain the true value of \\(r\\) against the expected 90%.\nNote, the case where \\(\\theta = 0.03\\) is currently not adjusted for the missing data.\n\n\nx %>%\n  mutate(\n    true_value_in_ci = r >= conf.low & r <= conf.high\n  ) %>%\n  summarize(.by = c(theta, a),\n    proportion = mean(true_value_in_ci)\n  )\n\n# A tibble: 12 × 3\n   theta      a proportion\n   <dbl>  <dbl>      <dbl>\n 1  0.03   4000      0.853\n 2  0.03  40000      0.744\n 3  0.03 400000      0.8  \n 4  0.1    4000      0.775\n 5  0.1   40000      0.825\n 6  0.1  400000      0.725\n 7  0.3    4000      0.85 \n 8  0.3   40000      0.85 \n 9  0.3  400000      0.875\n10  1      4000      0.85 \n11  1     40000      0.925\n12  1    400000      0.95 \n\nThese seem to be fairly calibrated; would need to do a binomial test to look for evidence of deviation.\nNow let’s look at the power to detect exponential growth, by considering the posterior probability that \\(r>0\\).\n\n\nx %>%\n  ggplot(aes(y = as.factor(theta), x = prob_increasing)) +\n  facet_wrap(~a, labeller = label_both) +\n  labs(x = 'Pr(r > 0)', y = 'reciprocal dispersion, theta') +\n  geom_dots(binwidth = 0.01)\n\n\n  # stat_dotsinterval()\n\n\nTODO: in revising, do the above calculation at the same time as the inteval extraction. Could also do the interval in the same manner, from the posterior.\n\n\nx %>%\n  ggplot(aes(y = as.factor(a), x = prob_increasing)) +\n  facet_wrap(~theta, labeller = label_both) +\n  labs(x = 'Pr(r > 0)', y = 'reciprocal dispersion, theta') +\n  geom_dots(binwidth = 0.01)\n\n\n  # stat_dotsinterval()\n\n\nHow often do we infer that \\(r>0\\) is more likely than not? Or with 80% certainty?\nNote: We have not filled in the missing rows where there were no observations; in these cases, we cannot infer increase.\n\n\nx %>%\n  summarize(.by = c(theta, a),\n    prob_0.5 = mean(prob_increasing > 0.5),\n    prob_0.6 = mean(prob_increasing > 0.6),\n    prob_0.8 = mean(prob_increasing > 0.8),\n  )\n\n# A tibble: 12 × 5\n   theta      a prob_0.5 prob_0.6 prob_0.8\n   <dbl>  <dbl>    <dbl>    <dbl>    <dbl>\n 1  0.03   4000    0.618    0.588    0.324\n 2  0.03  40000    0.513    0.513    0.282\n 3  0.03 400000    0.55     0.525    0.325\n 4  0.1    4000    0.775    0.75     0.525\n 5  0.1   40000    0.7      0.65     0.475\n 6  0.1  400000    0.8      0.675    0.575\n 7  0.3    4000    0.95     0.9      0.85 \n 8  0.3   40000    0.9      0.9      0.825\n 9  0.3  400000    0.95     0.95     0.825\n10  1      4000    0.975    0.975    0.95 \n11  1     40000    1        1        1    \n12  1    400000    1        1        1    \n\nNext steps\nwrite out the model, and compute the coefficient of variation under it.\nwrite some background\nmake the model more concrete, perhaps by framing as having a random relative abundance and Poisson sampling.\nstart with a lower value of a, s.t. can see that sequencing depth matters until we’re seeing a large enough expected count\ninvestigate the errors during fitting\nDiscussion\nIncreasing the sequencing depth cannot make up for an increase in overdispersion.\nThis makes sense — once we’re in a regime where the expected read count is above 0 and there is lots of overdispersion relative to Poisson, sequencing more doesn’t help much; it just helps us get a more precise measurement of the latent noisy (relative) abundance, and what we need is to reduce noise in that latent abundance.\nTo do this, we need to measure more samples with independent relative abundances.\nWe can reduce the noise from sample processing by processing the same sample repeatedly; however, for other noise sources we’d need to collect new samples from different sources or from more days.\nNote that because I use the correct model to fit the data, increasing the dispersion does not make the fit overconfident; the credible intervals are still tending to cover the true value of r the expected 90% of the time.\nIn contrast, if I fit using Poisson regression instead, I expect the fit to be overconfident, that is that the credible intervals will be too small and we’ll be missing the true r more than 90% of the time, for the cases there theta is significantly less than 1.\nSession info\n\nClick for session info\n\n\nsessioninfo::session_info()\n\n─ Session info ─────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       Arch Linux\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-02-09\n pandoc   3.0 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ─────────────────────────────────────────────────────────────\n package        * version  date (UTC) lib source\n assertthat       0.2.1    2019-03-21 [1] CRAN (R 4.0.0)\n backports        1.4.1    2021-12-13 [1] CRAN (R 4.1.2)\n base64enc        0.1-3    2015-07-28 [1] CRAN (R 4.0.0)\n bayesplot        1.9.0    2022-03-10 [1] CRAN (R 4.2.0)\n beeswarm         0.4.0    2021-06-01 [1] CRAN (R 4.1.0)\n boot             1.3-28   2021-05-03 [2] CRAN (R 4.2.2)\n broom            1.0.1    2022-08-29 [1] CRAN (R 4.2.1)\n broom.mixed    * 0.2.9.4  2022-04-17 [1] CRAN (R 4.2.0)\n bslib            0.4.1    2022-11-02 [1] CRAN (R 4.2.2)\n cachem           1.0.6    2021-08-19 [1] CRAN (R 4.1.1)\n callr            3.7.3    2022-11-02 [1] CRAN (R 4.2.1)\n cellranger       1.1.0    2016-07-27 [1] CRAN (R 4.0.0)\n cli              3.4.1    2022-09-23 [1] CRAN (R 4.2.1)\n codetools        0.2-18   2020-11-04 [2] CRAN (R 4.2.2)\n colorspace       2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n colourpicker     1.2.0    2022-10-28 [1] CRAN (R 4.2.1)\n cowplot        * 1.1.1    2021-08-27 [1] Github (wilkelab/cowplot@555c9ae)\n crayon           1.5.2    2022-09-29 [1] CRAN (R 4.2.1)\n crosstalk        1.2.0    2021-11-04 [1] CRAN (R 4.1.2)\n DBI              1.1.3    2022-06-18 [1] CRAN (R 4.2.1)\n dbplyr           2.2.1    2022-06-27 [1] CRAN (R 4.2.1)\n digest           0.6.30   2022-10-18 [1] CRAN (R 4.2.1)\n distill          1.5.2    2022-11-10 [1] Github (rstudio/distill@9c1a1a2)\n distributional   0.3.1    2022-09-02 [1] CRAN (R 4.2.1)\n downlit          0.4.2    2022-07-05 [1] CRAN (R 4.2.1)\n dplyr          * 1.1.0    2023-01-29 [1] CRAN (R 4.2.2)\n DT               0.26     2022-10-19 [1] CRAN (R 4.2.1)\n dygraphs         1.1.1.6  2018-07-11 [1] CRAN (R 4.0.2)\n ellipsis         0.3.2    2021-04-29 [1] CRAN (R 4.1.0)\n evaluate         0.18     2022-11-07 [1] CRAN (R 4.2.2)\n fansi            1.0.3    2022-03-24 [1] CRAN (R 4.2.1)\n farver           2.1.1    2022-07-06 [1] CRAN (R 4.2.1)\n fastmap          1.1.0    2021-01-25 [1] CRAN (R 4.0.4)\n forcats        * 0.5.2    2022-08-19 [1] CRAN (R 4.2.1)\n fs             * 1.5.2    2021-12-08 [1] CRAN (R 4.1.2)\n furrr          * 0.3.1    2022-08-15 [1] CRAN (R 4.2.1)\n future         * 1.28.0   2022-09-02 [1] CRAN (R 4.2.1)\n gargle           1.2.1    2022-09-08 [1] CRAN (R 4.2.1)\n generics         0.1.3    2022-07-05 [1] CRAN (R 4.2.1)\n ggbeeswarm     * 0.6.0    2017-08-07 [1] CRAN (R 4.0.0)\n ggdist         * 3.2.0    2022-07-19 [1] CRAN (R 4.2.1)\n ggplot2        * 3.3.6    2022-05-03 [1] CRAN (R 4.2.0)\n ggridges         0.5.4    2022-09-26 [1] CRAN (R 4.2.1)\n globals          0.16.1   2022-08-28 [1] CRAN (R 4.2.1)\n glue             1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n googledrive      2.0.0    2021-07-08 [1] CRAN (R 4.1.0)\n googlesheets4    1.0.1    2022-08-13 [1] CRAN (R 4.2.1)\n gridExtra        2.3      2017-09-09 [1] CRAN (R 4.0.2)\n gtable           0.3.1    2022-09-01 [1] CRAN (R 4.2.1)\n gtools           3.9.3    2022-07-11 [1] CRAN (R 4.2.1)\n haven            2.5.1    2022-08-22 [1] CRAN (R 4.2.1)\n here           * 1.0.1    2020-12-13 [1] CRAN (R 4.0.5)\n highr            0.9      2021-04-16 [1] CRAN (R 4.1.0)\n hms              1.1.2    2022-08-19 [1] CRAN (R 4.2.1)\n htmltools        0.5.3    2022-07-18 [1] CRAN (R 4.2.1)\n htmlwidgets      1.5.4    2021-09-08 [1] CRAN (R 4.1.1)\n httpuv           1.6.6    2022-09-08 [1] CRAN (R 4.2.1)\n httr             1.4.4    2022-08-17 [1] CRAN (R 4.2.1)\n igraph           1.3.5    2022-09-22 [1] CRAN (R 4.2.1)\n inline           0.3.19   2021-05-31 [1] CRAN (R 4.1.0)\n jquerylib        0.1.4    2021-04-26 [1] CRAN (R 4.1.0)\n jsonlite         1.8.3    2022-10-21 [1] CRAN (R 4.2.1)\n knitr            1.40     2022-08-24 [1] CRAN (R 4.2.1)\n labeling         0.4.2    2020-10-20 [1] CRAN (R 4.0.3)\n later            1.3.0    2021-08-18 [1] CRAN (R 4.1.1)\n lattice          0.20-45  2021-09-22 [2] CRAN (R 4.2.2)\n lifecycle        1.0.3    2022-10-07 [1] CRAN (R 4.2.1)\n listenv          0.8.0    2019-12-05 [1] CRAN (R 4.0.0)\n lme4             1.1-31   2022-11-01 [1] CRAN (R 4.2.1)\n loo              2.5.1    2022-03-24 [1] CRAN (R 4.2.0)\n lubridate        1.9.0    2022-11-06 [1] CRAN (R 4.2.2)\n magrittr         2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n markdown         1.3      2022-10-29 [1] CRAN (R 4.2.1)\n MASS             7.3-58.1 2022-08-03 [1] CRAN (R 4.2.1)\n Matrix           1.5-1    2022-09-13 [1] CRAN (R 4.2.1)\n matrixStats      0.62.0   2022-04-19 [1] CRAN (R 4.2.0)\n memoise          2.0.1    2021-11-26 [1] CRAN (R 4.1.2)\n mime             0.12     2021-09-28 [1] CRAN (R 4.1.1)\n miniUI           0.1.1.1  2018-05-18 [1] CRAN (R 4.0.2)\n minqa            1.2.5    2022-10-19 [1] CRAN (R 4.2.1)\n modelr           0.1.9    2022-08-19 [1] CRAN (R 4.2.1)\n munsell          0.5.0    2018-06-12 [1] CRAN (R 4.0.0)\n nlme             3.1-160  2022-10-10 [2] CRAN (R 4.2.2)\n nloptr           2.0.3    2022-05-26 [1] CRAN (R 4.2.0)\n nvimcom        * 0.9-142  2022-12-22 [1] local\n parallelly       1.32.1   2022-07-21 [1] CRAN (R 4.2.1)\n patchwork      * 1.1.2    2022-08-19 [1] CRAN (R 4.2.1)\n pillar           1.8.1    2022-08-19 [1] CRAN (R 4.2.1)\n pkgbuild         1.3.1    2021-12-20 [1] CRAN (R 4.1.2)\n pkgconfig        2.0.3    2019-09-22 [1] CRAN (R 4.0.0)\n plyr             1.8.7    2022-03-24 [1] CRAN (R 4.2.0)\n prettyunits      1.1.1    2020-01-24 [1] CRAN (R 4.0.0)\n processx         3.8.0    2022-10-26 [1] CRAN (R 4.2.1)\n promises         1.2.0.1  2021-02-11 [1] CRAN (R 4.0.4)\n ps               1.7.2    2022-10-26 [1] CRAN (R 4.2.1)\n purrr          * 0.3.5    2022-10-06 [1] CRAN (R 4.2.1)\n R6               2.5.1    2021-08-19 [1] CRAN (R 4.1.1)\n Rcpp           * 1.0.9    2022-07-08 [1] CRAN (R 4.2.1)\n RcppParallel     5.1.5    2022-01-05 [1] CRAN (R 4.1.2)\n readr          * 2.1.3    2022-10-01 [1] CRAN (R 4.2.1)\n readxl           1.4.1    2022-08-17 [1] CRAN (R 4.2.1)\n reprex           2.0.2    2022-08-17 [1] CRAN (R 4.2.1)\n reshape2         1.4.4    2020-04-09 [1] CRAN (R 4.0.0)\n rlang            1.0.6    2022-09-24 [1] CRAN (R 4.2.1)\n rmarkdown      * 2.18     2022-11-09 [1] CRAN (R 4.2.2)\n rprojroot        2.0.3    2022-04-02 [1] CRAN (R 4.2.2)\n rstan            2.21.7   2022-09-08 [1] CRAN (R 4.2.1)\n rstanarm       * 2.21.3   2022-04-09 [1] CRAN (R 4.2.0)\n rstantools       2.2.0    2022-04-08 [1] CRAN (R 4.2.0)\n rvest            1.0.3    2022-08-19 [1] CRAN (R 4.2.1)\n sass             0.4.2    2022-07-16 [1] CRAN (R 4.2.1)\n scales           1.2.1    2022-08-20 [1] CRAN (R 4.2.1)\n sessioninfo      1.2.2    2021-12-06 [1] CRAN (R 4.1.2)\n shiny            1.7.3    2022-10-25 [1] CRAN (R 4.2.1)\n shinyjs          2.1.0    2021-12-23 [1] CRAN (R 4.1.2)\n shinystan        2.6.0    2022-03-03 [1] CRAN (R 4.2.0)\n shinythemes      1.2.0    2021-01-25 [1] CRAN (R 4.0.4)\n StanHeaders      2.21.0-7 2020-12-17 [1] CRAN (R 4.0.3)\n stringi          1.7.8    2022-07-11 [1] CRAN (R 4.2.2)\n stringr        * 1.4.1    2022-08-20 [1] CRAN (R 4.2.1)\n survival         3.4-0    2022-08-09 [2] CRAN (R 4.2.2)\n threejs          0.3.3    2020-01-21 [1] CRAN (R 4.0.2)\n tibble         * 3.1.8    2022-07-22 [1] CRAN (R 4.2.1)\n tidyr          * 1.2.1    2022-09-08 [1] CRAN (R 4.2.1)\n tidyselect       1.2.0    2022-10-10 [1] CRAN (R 4.2.1)\n tidyverse      * 1.3.2    2022-07-18 [1] CRAN (R 4.2.1)\n timechange       0.1.1    2022-11-04 [1] CRAN (R 4.2.2)\n tzdb             0.3.0    2022-03-28 [1] CRAN (R 4.2.0)\n utf8             1.2.2    2021-07-24 [1] CRAN (R 4.1.0)\n vctrs            0.5.2    2023-01-23 [1] CRAN (R 4.2.2)\n vipor            0.4.5    2017-03-22 [1] CRAN (R 4.0.0)\n withr            2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun             0.34     2022-10-18 [1] CRAN (R 4.2.1)\n xml2             1.3.3    2021-11-30 [1] CRAN (R 4.1.2)\n xtable           1.8-4    2019-04-21 [1] CRAN (R 4.0.0)\n xts              0.12.2   2022-10-16 [1] CRAN (R 4.2.1)\n yaml             2.3.6    2022-10-18 [1] CRAN (R 4.2.1)\n zoo              1.8-11   2022-09-17 [1] CRAN (R 4.2.1)\n\n [1] /home/michael/.local/lib/R/library\n [2] /usr/lib/R/library\n\n────────────────────────────────────────────────────────────────────────\n\n\n\n\n",
    "preview": "posts/2023-02-08-effect-of-noise-on-egd/main_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2023-02-13T15:42:57+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-28-egd-theory-notes/",
    "title": "EGD theory notes",
    "description": "Captured notes on exponential growth detection.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2022-11-28",
    "categories": [
      "EGD",
      "theory"
    ],
    "contents": "\n\nContents\nStandard error of growth rate under IID multiplicative noise\n2022-03-28\n2022-08-06\n2022-11-28\n\n\nStandard error of growth rate under IID multiplicative noise\n2022-03-28\nUnder IID multiplicative noise, the exponential growth (EG) model is equivalent to the standard linear regression model applied to log abundance.\nThe standard error in the growth rate estimate is\n\\[\\begin{align}\n  se(\\hat r) = \\frac{\\sigma(\\varepsilon)}{\\sigma(t) \\sqrt{n}},\n\\end{align}\\]\nwhere \\(\\sigma(\\varepsilon)\\) is the standard deviation of the residual log measurement, \\(t\\) is the sampling times, and \\(n\\) is the number of samples.\nFor daily samples from \\(t=1\\) to \\(t=T\\), we have that \\(n = T\\) and that \\(\\sigma(t) = \\sqrt{(T^2-1)/12}\\).\nIn this case, the standard error is\n\\[\\begin{align}\n  se(\\hat r)\n    &= \\frac{\\sqrt{12} \\; \\sigma(\\varepsilon)}{\\sqrt{T (T^2 -2)}}\n  \\\\&\\approx \\frac{\\sqrt{12} \\; \\sigma(\\varepsilon)}{T^{3/2}} \\quad \\text{for $T \\gg 1$}.\n\\end{align}\\]\nThis formula tells us that our uncertainty decreases with more days of sampling as \\(T^{3/2}\\), with a factor \\(T\\) coming from the increased temporal spread of sampling days and a factor \\(T^{1/2}\\) coming from the increased number of samples.\nIf we included technical replicates, we could increase precision without requiring more days; however, an equal increase in precision requires processing and sequencing more samples from the current range of days than adding additional samples from additional days to the range.\n2022-08-06\nThis calculation just tells us about the standard error; it might be interesting to extend it to consider our ability to detect positive growth.\nincreasing the number of samples on a given day can only reduce the fraction of variance that is not day-by-day, e.g. due to sample processing and sequencing.\nshould think on a graphical / analogy / schematic representation of this for the team\n2022-11-28\nConsider a mixture model of lognormal + Poisson noise on the counts.\nI expect that the above applies in the regime where counts are >> 1 with very high probability, and the pure Poisson theory to apply when the counts are below some threshold defined by the (geometric) standard dev of the lognormal noise.\nHowever, I suspect we will often be in an intermediate regime for EGD when we are first able to detect an emerging pathogen, where the multiplicative noise is sufficient that it is not uncommon for the counts to be \\(\\lesssim 1\\) and \\(\\gg 1\\) on adjacent days.\nEven if that is so, perhaps the results for the two regimes still give us useful bounds.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-02-13T15:42:57+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-05-developing-a-qpcr-data-analysis-workflow/",
    "title": "Developing a qPCR data analysis workflow",
    "description": "In-progress qPCR data analysis workflow in R, using data from the 2022-Q2 Sprint spike-in experiment.",
    "author": [
      {
        "name": "Mike",
        "url": {}
      }
    ],
    "date": "2022-11-05",
    "categories": [
      "qPCR",
      "R"
    ],
    "contents": "\nSetup\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fs)\n# library(generics)\n\nlibrary(cowplot)\ntheme_set(theme_cowplot())\nlibrary(patchwork)\n\n\nThe data is from a folder in the NAO Drive currently called ‘Spike-in experiments’, which I’ve downloaded locally.\n\n\ndata_path <- here( '_data/nao/qpcr', '2022-06-29-spike-in-experiment/results')\ndir_ls(data_path) %>% path_file\n\n [1] \"!README.docx\"                          \n [2] \"2022-06-29-trip01.eds\"                 \n [3] \"2022-06-29-trip01.txt\"                 \n [4] \"2022-06-29-trip01.xlsx\"                \n [5] \"2022-06-29-trip01_2.eds\"               \n [6] \"2022-06-29-trip02.eds\"                 \n [7] \"2022-06-29-trip02.xlsx\"                \n [8] \"2022-06-30-trip02_009_seq_barcode.xlsx\"\n [9] \"2022-06-30_trip03.xlsx\"                \n[10] \"Results.ipynb\"                         \n[11] \"plate_layout_trip1.txt\"                \n[12] \"plate_layout_trip2.txt\"                \n[13] \"results.md5\"                           \n[14] \"trip1-template.edt\"                    \n\nTODO: try loading the data directly from Google Drive\nLoad qPCR data\nThis file assumes that the data is within a folder ‘data/’ within the root project folder.\nFile info:\n- Excel files have the raw and processed florescence measurements (Rn and Delta Rn), as well as the softwares autothreshold stuff in another sheet.\n- Also some sample metadata is here; however, we might want want to take that from the .txt file, since that is (I believe) closer to the original supplied table.\n\nTODO: Talk to Ari about whether we have/can save the files used to set up the qPCR experiment\nFirst, we can read in the relevant sections of the Excel file and clean up thedata a bit,\nFirst, load in the sample metadata.\nNOTE: Throughout, I’m using janitor::clean_names() to standardize the format of the column names\n\n\n\nsam <- path(data_path, '2022-06-29-trip01.txt') %>%\n  read_tsv(skip = 43) %>%\n  janitor::clean_names() %>%\n  mutate(\n    row = str_sub(well_position, 1, 1) %>% as.ordered,\n    column = str_sub(well_position, 2) %>% as.integer %>% as.ordered,\n  ) %>%\n  # relocate(row, column, .after = well_position) %>%\n  glimpse\n\nRows: 96\nColumns: 15\n$ well           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ well_position  <chr> \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8…\n$ sample_name    <chr> \"Blank\", \"NTC\", \"Trip1_010_Neg_Ctrl\", \"Trip1_…\n$ sample_color   <chr> \"RGB(0,139,69)\", \"RGB(142,56,142)\", \"RGB(139,…\n$ biogroup_name  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ biogroup_color <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ target_name    <chr> \"Blank\", \"Trip1_010_0\", \"Trip1_010_Neg_Ctrl\",…\n$ target_color   <chr> \"RGB(0,139,69)\", \"RGB(142,142,56)\", \"RGB(238,…\n$ task           <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"…\n$ reporter       <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FA…\n$ quencher       <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"…\n$ quantity       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ comments       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ row            <ord> A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, …\n$ column         <ord> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, …\n\nTODO: pre-specify the column types\nNOTE: In this experiment, the target name was incorrectly set to be identical to the sample name; instead, each non-blank sample should have the target corresponding to the primer/probe pair (here, 009 or 010). The import chunk therefore replaces the target name accordingly. The blank is ste to NA since no primers/master mix is added.\nTODO: confirm this with Anjali and Ari; and in future, make sure target corresponds to the relevant primer/probe set for that well.\nTODO: Suggest that we use more descriptive target names than ‘010’\n\nTODO: Suggest we find some way of directly adding the dilution and perhaps the concentration to the sample data, rather than having to parse from the sample name\n\nThere is additional sample data hidden in the sample names, which we’ll need to fix in future runs (it should be in its own columns in a table).\nBut for now we can parse it from the sample names.\nI’ll overwrite the faulty target names.\n\n\nsam <- sam %>%\n  separate(sample_name, \n    into = c('ww_triplicate', 'target_name', 'dilution_name'),\n    sep = '_', extra = 'merge',\n    remove = FALSE\n  )\n\n\nNOTE: The target name of the NTC is now NA, which is incorrect, but I’m not going to worry about that now.\nThe proper fix is for the target name to be fixed in the source data.\nNOTE: We need to use the actual starting concentrations for the standard curve. Need to talk to Anjali about how these should be supplied.\nHere I will assume the following.\n\n\n# concentration in copies per microliter\nconc_low <- 0.1\ndilution_step <- 20\nnum_samples <- 7\nconc_max <- conc_low * dilution_step^(num_samples - 1)\n\ndilution_df <- tibble(\n  dilution_power = 0:6,\n  dilution_factor = dilution_step^dilution_power,\n  conc = conc_max / dilution_factor,\n  dilution_name = str_c('D', '0', dilution_power + 1)\n)\n\n\nwhich we must add to the sample data,\n\n\nsam <- sam %>%\n  left_join(dilution_df, by = 'dilution_name')\n\n\nNext, load the amplification data — the relative florescence values (Rn and Delta Rn) — and join the sample metadata.\n\n\namp <- path(data_path, '2022-06-29-trip01.xlsx') %>%\n  readxl::read_excel(\n    sheet = 'Amplification Data',\n    skip = 40,\n    col_types = c('numeric', 'text', 'numeric', 'text', 'numeric', 'numeric')\n  ) %>%\n  janitor::clean_names() %>%\n  glimpse\n\nRows: 3,840\nColumns: 6\n$ well          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ well_position <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ cycle         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ target_name   <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"…\n$ rn            <dbl> 5.125966, 5.172779, 5.221941, 5.282046, 5.3525…\n$ delta_rn      <dbl> -0.228727385, -0.190693140, -0.150309995, -0.0…\n\nNote, this table has the original, incorrect target name.\nI’ll drop that, and join the sample metadata table which has the corrected target names.\n\n\namp <- amp %>%\n  select(-target_name) %>%\n  left_join(sam, by = c('well', 'well_position')) %>%\n  glimpse\n\nRows: 3,840\nColumns: 23\n$ well            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ well_position   <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A…\n$ cycle           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ rn              <dbl> 5.125966, 5.172779, 5.221941, 5.282046, 5.35…\n$ delta_rn        <dbl> -0.228727385, -0.190693140, -0.150309995, -0…\n$ sample_name     <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\",…\n$ ww_triplicate   <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\",…\n$ target_name     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ dilution_name   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sample_color    <chr> \"RGB(0,139,69)\", \"RGB(0,139,69)\", \"RGB(0,139…\n$ biogroup_name   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ biogroup_color  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ target_color    <chr> \"RGB(0,139,69)\", \"RGB(0,139,69)\", \"RGB(0,139…\n$ task            <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", …\n$ reporter        <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"F…\n$ quencher        <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", …\n$ quantity        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ comments        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ row             <ord> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,…\n$ column          <ord> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ dilution_power  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ dilution_factor <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conc            <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\nTODO: Read in the baseline start/stop points, so can view and check where in the amplifying/not portion\n\nExplore sample metadata\n\n\nsam %>%\n  count(target_name)\n\n# A tibble: 3 × 2\n  target_name     n\n  <chr>       <int>\n1 009            24\n2 010            24\n3 <NA>           48\n\nPlot the plate layout\nTODO: use code from vivo vitro to do this nicely; need to first parse the row and column from the well position\nTODO: see if can flow the sample names so that they print nicer\nQC checks\nCheck the blanks\nCheck the NTCs and Neg controls (though this is target-specific)\n\nWe’ll want to do the analysis separately for each target.\nMaybe there’s value in first looking at everything\nTODO: Set a fixed color scheme for the targets, and use this in all plots.\n\n\ndelta_rn_min <- 1e-3\np1 <- amp %>%\n  filter(!is.na(target_name)) %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  scale_color_brewer(type = 'qual') +\n  geom_point() +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nTODO: Consider if a better way to deal with non-positive values in the log-scale plot. See what the software does. (I think it might just not show these points.)\nAnalysis of a single target\nWill ultimately do this for all targets; perhaps show in tabs?\nFor now, I’ll use target 009.\n\n\namp_cur <- amp %>%\n  filter(target_name == '009')\n\n\n\n\ndelta_rn_min <- 1e-3\np1 <- amp_cur %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  scale_color_brewer(type = 'qual') +\n  geom_point() +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nPick threshold\nFor now, will pick the threshold manually.\n\n\nthreshold <- 3e-1\np1 / p2 &\n  geom_hline(yintercept = threshold)\n\n\n\nTODO: implement chosen auto-threshold algorithm.\nCompute Cq values\nTODO Ask Anjali if wants to use Ct or Cq as the name\nTODO Separate out into distinct files the code where I’m testing for myself, and demonstrating for others\n\nI’ll define a custom function estimate_cq to estimate the Cq value for a trajectotry crossing a given quantification threshold.\nSee the appendix below for more info.\n\n\nestimate_cq <- function(.data, threshold) {\n  .data <- .data %>%\n    arrange(cycle) %>%\n    mutate(\n      # NOTE: log transformation, important for interpolation\n      across(delta_rn, log),\n      below = delta_rn < threshold,\n      above = delta_rn > threshold,\n    )\n  before <- .data %>%\n    filter(below) %>%\n    slice_tail(n = 1)\n  after <- .data %>%\n    filter(above) %>%\n    slice_head(n = 1)\n  # And find the intersection of the line passing between these two points, and the threshold.\n  slope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\n  delta_rn_diff <- threshold - before$delta_rn\n  cycle_diff <- delta_rn_diff / slope\n  ct <- before$cycle + cycle_diff\n  ct\n}\n\n\nNow use this function to compute the Cq values for each trajectory\n\n\nsample_vars <-  sam %>% colnames\ncqs <- amp_cur %>%\n  with_groups(all_of(sample_vars), nest) %>%\n  mutate(\n    cq = map_dbl(data, estimate_cq, threshold = threshold)\n  ) %>%\n  select(-data) %>%\n  glimpse\n\nRows: 24\nColumns: 21\n$ well            <dbl> 51, 52, 53, 54, 55, 56, 57, 58, 63, 64, 65, …\n$ well_position   <chr> \"E3\", \"E4\", \"E5\", \"E6\", \"E7\", \"E8\", \"E9\", \"E…\n$ sample_name     <chr> \"Trip1_009_Neg_Ctrl\", \"Trip1_009_D07\", \"Trip…\n$ ww_triplicate   <chr> \"Trip1\", \"Trip1\", \"Trip1\", \"Trip1\", \"Trip1\",…\n$ target_name     <chr> \"009\", \"009\", \"009\", \"009\", \"009\", \"009\", \"0…\n$ dilution_name   <chr> \"Neg_Ctrl\", \"D07\", \"D06\", \"D05\", \"D04\", \"D03…\n$ sample_color    <chr> \"RGB(238,44,44)\", \"RGB(51,161,201)\", \"RGB(23…\n$ biogroup_name   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ biogroup_color  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ target_color    <chr> \"RGB(51,161,201)\", \"RGB(238,18,137)\", \"RGB(1…\n$ task            <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", …\n$ reporter        <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"F…\n$ quencher        <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", …\n$ quantity        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ comments        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ row             <ord> E, E, E, E, E, E, E, E, F, F, F, F, F, F, F,…\n$ column          <ord> 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 7, 8, 9…\n$ dilution_power  <int> NA, 6, 5, 4, 3, 2, 1, 0, NA, 6, 5, 4, 3, 2, …\n$ dilution_factor <dbl> NA, 6.4e+07, 3.2e+06, 1.6e+05, 8.0e+03, 4.0e…\n$ conc            <dbl> NA, 1.0e-01, 2.0e+00, 4.0e+01, 8.0e+02, 1.6e…\n$ cq              <dbl> 33.15247, 32.86074, 33.07862, 32.43584, 30.9…\n\nEstimate and plot standard curve\nQuestions:\n\n\ncqs <- cqs %>%\n  mutate(\n    conc_log10 = log10(conc) \n  )\n\n\n\n\nfit <- lm(cq ~ conc_log10, data = cqs)\nfit %>% summary\n\n\nCall:\nlm(formula = cq ~ conc_log10, data = cqs)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.006 -2.612 -0.405  1.888  3.032 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  33.8227     0.7298   46.35  < 2e-16 ***\nconc_log10   -2.0442     0.1872  -10.92 1.25e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.232 on 19 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.8626,    Adjusted R-squared:  0.8553 \nF-statistic: 119.3 on 1 and 19 DF,  p-value: 1.251e-09\n\n\n\ncqs %>%\n  ggplot(aes(conc_log10, cq)) +\n  geom_point() +\n  geom_abline(\n    intercept = coef(fit)[1],\n    slope = coef(fit)[2]\n  )\n\n\n\nNote, better to plot in a way that shows the uncertainty.\nCan use stat smooth, but then not connected to the fit we did.\nCould be better to use some of the ggdist et al tools.\n\n\ncqs %>%\n  ggplot(aes(conc_log10, cq)) +\n  geom_point() +\n  stat_smooth(method = 'lm')\n\n\n\n\n\nFALSE\n\n[1] FALSE\n\nEstimate the efficiency from the standard curve\nWe can estimate the efficiency from the slope of the standard curve using the standard formula,\n\n\nx <- fit %>% broom::tidy()\nslope <- coef(fit)['conc_log10']\nefficiency_estimate <- 10^(-1/slope) - 1\n\n\nIn this case, the estimate is unreasonably large because the standard curve isn’t good.\n90% confidence interval:\n\n\nslope_ci <- confint(fit, parm = 'conc_log10', level = 0.9)\nefficiency_ci <- 10^(-1/slope_ci) - 1\nefficiency_ci\n\n                5 %     95 %\nconc_log10 1.644336 2.812536\n\nQuite a large range!\nBayesian version with rstanarm\n\n\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\n\nlibrary(ggdist)\n\n\n\n\nstan_fit <- stan_glm(\n  cq ~ conc_log10, \n  data = cqs,\n)\nstan_fit %>% summary\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      cq ~ conc_log10\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 21\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 33.8    0.8 32.8  33.8  34.8 \nconc_log10  -2.0    0.2 -2.3  -2.0  -1.8 \nsigma        2.4    0.4  1.9   2.3   3.0 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 27.9    0.8 26.9  27.9  28.9 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3052 \nconc_log10    0.0  1.0  3186 \nsigma         0.0  1.0  2274 \nmean_PPD      0.0  1.0  2947 \nlog-posterior 0.0  1.0  1453 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nstan_fit %>% plot\n\n\n\ntry to get the posterior samples, so that we can then get the posterior of the efficiency estimate.\nTODO: google a better way to do this\n\n\nslope_post <- rstan::extract(stan_fit$stanfit)$beta\nefficiency_post <- 10^(-1/slope_post) - 1\nefficiency_post %>% qplot\n\n\n\nnote, we could use a stronger prior since we have a lot of relevant domain info.\nDemo how to use the standard curve for calibration\nAppendix\nFunction for finding ct in a well\nStandard method is to compute the Ct for each well independentally.\nNeed to interpolate between points in the trajectory, and find when the trajectory crosses the threshold.\nI wonder what the software does; simplest method is perhaps linear interpolation of log Delta Rn.\nTODO: google linear interpolation in R. This is essentially what geom_line is doing. It would be handy if I could just get access to that output. Can also google how to create a piecewise linear function.\nFirst try\nSince I don’t have wifi, I will need to hack it myself.\nFor each well, find the cycles immediately before and after the theshold crossing\nfind the intersection between the line segment between those two points and the horizontal line at the threshold.\n\n\n\nFALSE\n\n[1] FALSE\n\n\n\n# Suppose we have the trajectory for a particular well\nx <- amp_cur %>%\n  filter(well == 52) %>%\n  select(cycle, delta_rn) %>%\n  arrange(cycle)\n# We can get the crossing point as follows\nx <- x %>%\n  mutate(\n    below = delta_rn < threshold,\n    above = delta_rn > threshold,\n  )\nbefore <- x %>%\n  filter(below) %>%\n  slice_tail(n = 1)\nafter <- x %>%\n  filter(above) %>%\n  slice_head(n = 1)\n\n\nAnd find the intersection.\nline passing between these two points, and the threshold.\n\n\nslope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\ndelta_rn_diff <- threshold - before$delta_rn\ncycle_diff <- delta_rn_diff / slope\nct <- before$cycle + cycle_diff\n\n\nwe can put this all together in a function,\n\n\nestimate_cq <- function(.data, threshold) {\n  .data <- .data %>%\n    arrange(cycle) %>%\n    mutate(\n      # NOTE: log transformation, important for interpolation\n      across(delta_rn, log),\n      below = delta_rn < threshold,\n      above = delta_rn > threshold,\n    )\n  before <- .data %>%\n    filter(below) %>%\n    slice_tail(n = 1)\n  after <- .data %>%\n    filter(above) %>%\n    slice_head(n = 1)\n  # And find the intersection of the line passing between these two points, and the threshold.\n  slope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\n  delta_rn_diff <- threshold - before$delta_rn\n  cycle_diff <- delta_rn_diff / slope\n  ct <- before$cycle + cycle_diff\n  ct\n}\n\n\nways we could improve\ncheck that there is only one crossing\ncheck that the crossing is not in the noise region\n\nTry 2, with wifi\n\n\n# Suppose we have the trajectory for a particular well\nx <- amp_cur %>%\n  filter(well == 52) %>%\n  select(cycle, delta_rn) %>%\n  arrange(cycle)\n# We can get the crossing point as follows\nx <- x %>%\n  mutate(\n    below = delta_rn < threshold,\n    above = delta_rn > threshold,\n  )\nbefore <- x %>%\n  filter(below) %>%\n  slice_tail(n = 1)\nafter <- x %>%\n  filter(above) %>%\n  slice_head(n = 1)\n\n\nWe can use approxfun() to define the interpolating function.\n\n\nf <- approxfun(x$cycle, x$delta_rn %>% log, rule = 1)\na <- seq(from = -5, to = 45, by = 0.1)\nqplot(a, f(a))\n\n\n\nand then the intersection with e.g. a root-finding function, uniroot(). however, this approach is a bit funny because of the non-monotonicity in the noise region, and the fact that once we know the cycle interval we can calculate the intersection of the interpolation manually.\nEfficiency estimate\nDerivation:\nThe slope of the standard curve tells us how many extra cycles correspond to a 10X decrease in starting concentration.\nWith perfect efficiency of \\(E=1\\), this would equal \\(\\log(10) / \\log(2)\\).\nMore generally, the number of extra cycles is \\(A = \\log(10) / \\log(1 + E)\\), for any log base; taking the log base to be 10 gives \\(A = 1 / \\log(1 + E)\\).\nThe slope of the standard curve corresponds to \\(-A\\).\nTherefore, we can estimate \\(E\\) by\n\\[\\begin{align}\n  \\hat E = 10^{1 / A} - 1\n\\end{align}\\]\nMethods for setting the threshold\nhttps://www.researchgate.net/post/How-can-I-set-the-threshold-in-a-Real-Time-PCR-result has some discussion.\nOne suggestion is to find candidate points based on the maximum of the second derivative of the amplification curves\nMethods for finding the baseline region\nNot needed right now since we’re using the software’s determination of this.\nNext steps\nmake an r package that can house helper functions\ndevelop an autothreshold method\nadd the ability to check the baseline calculation\nsave output\n\n\n\n\n",
    "preview": "posts/2022-11-05-developing-a-qpcr-data-analysis-workflow/developing-a-qpcr-data-analysis-workflow_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2023-02-13T15:42:57+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-27-czid-r-analysis-demo/",
    "title": "CZID-to-R data import and analysis demo",
    "description": "Demonstration of how to import taxonomic profiles from CZID into R and do a few basic analyses.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2022-10-31",
    "categories": [
      "CZ ID",
      "hjelmso2019meta",
      "R"
    ],
    "contents": "\n\nContents\nR setup\nData import\nFrom a BIOM file (Hjelmsø et al. (2019) data)\nCreate a phyloseq object\n\n\nBasic data checks and stats\nTaxonomy\n\nAnalysis\nRecreating Figure 4 from Rothman et al. (2021) using the Hjelmso data\nBray-Curtis NMDS ordination (Panel A)\nAlpha diversity (Panel B)\nRelative abundances (Proportions) (Panel C)\nPut the panels together\n\n\n\nR setup\nStart by loading some useful R packages,\n\n\n# set of data analysis and viz tools\nlibrary(tidyverse)\n\n# file system helpers\nlibrary(fs)\n\n# specifying locations within a project\nlibrary(here)\n\n# microbiome analysis helpers\nlibrary(biomformat)\nlibrary(speedyseq)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n\nHere I’ll work with the BIOM file generated from the Hjelmsø et al. (2019) taxonomic profiles.\n\n\nhjelmso_data_path <- here(\"_data/hjelmso2019meta/czid\")\ndir_ls(hjelmso_data_path) %>% path_file\n\n[1] \"2022-11-07_combined_microbiome_file_nt_r.biom\"      \n[2] \"2022-11-07_combined_microbiome_file_nt_r_fixed.biom\"\n\nData import\nFrom a BIOM file (Hjelmsø et al. (2019) data)\nThe BIOM format (https://biom-format.org/, McDonald et al. (2012)) is a file format for including the abundance matrix, taxonomy, and sample metadata all in one file.\nBIOM export from CZID is supported but listed as being in Beta.\nIf we try reading in the file as directly exported from CZID, we get an error\n\n\nbiom <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  read_biom()\n\nError in validObject(.Object): invalid class \"biom\" object: type field has unsupported value\n\n\n\n\n\nThis error arises because the ‘type’ of the data object defined in the JSON-formatted contents of the .biom file isn’t valid as per the biom format v1.0 specs, see https://biom-format.org/documentation/format_versions/biom-1.0.html.\nWe can see this by opening up the file and looking for the type argument towards the beginning; or looking at the top items in the list after reading in the file with a JSON parser.\n\n\nbiom_json <- path(hjelmso_data_path, \n  '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  jsonlite::read_json()\nbiom_json %>% head(9)\n\n$id\n[1] \"None\"\n\n$format\n[1] \"Biological Observation Matrix 1.0.0\"\n\n$format_url\n[1] \"http://biom-format.org\"\n\n$matrix_type\n[1] \"sparse\"\n\n$generated_by\n[1] \"BIOM-Format 2.1.12\"\n\n$date\n[1] \"2022-11-07T20:37:42.385642\"\n\n$type\n[1] \"Table\"\n\n$matrix_element_type\n[1] \"float\"\n\n$shape\n$shape[[1]]\n[1] 25495\n\n$shape[[2]]\n[1] 85\n\nWe can fix the file by changing the type from ‘Table’ to something valid.\nIt doesn’t actually matter what we use:\n\nWhile type is a required entry in BIOM tables, the BIOM format itself does not change for different data types (e.g., OTU Table, function table, metabolite table). This information is included to allow tools that use BIOM files to determine the data type, if desired. (Caption for Additional file 5 in McDonald et al. (2012))\n\nLet’s use ‘Taxon table’.\nThe following code chunk should do the trick but is very slow, apparently because the jsonlite package is slow to work with large lists/files.\n\n\nbiom_json$type <- 'Taxon table'\njsonlite::write_json(\n  biom_json,\n  path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom')\n)\n\n\nSo I’ll instead simply replace the offending text.\n\n\nbiom_text <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  read_file\nstopifnot( identical(biom_text %>% str_count('\"Table\"'), 1L) )\nbiom_text_fixed <- biom_text %>%\n  str_replace('\"Table\"', '\"Taxon table\"')\nwrite_file(\n  biom_text_fixed,\n  path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom')\n)\n\n\nThe above chunk reads in the BIOM file’s contents as a single string, checks that ‘“Table”’ appears only once (in the field where it is set as the type), then replaces it with ‘“Taxon table”’), then writes the string as a new BIOM file.\nWe should now be able to load the corrected BIOM file with the biomformat package,\n\n\nbiom <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom') %>%\n  read_biom() %>%\n  print\n\nbiom object. \ntype: Taxon table \nmatrix_type: sparse \n25495 rows and 85 columns \n\nNote: For most objects in R, the print() and glimpse() methods silently return the object as well as printing information about it.\nAdding a print or glimpse call at the end of a variable-assignment pipe chain is a succinct way to save an object and show some info about it.\nCreate a phyloseq object\nThe abundance (count) matrix, sample metadata table, and taxonomy table can be extracted with three corresponding functions functions from the biomformat package.\nWe’ll tackle these one at a time.\nFirst, the abundance matrix.\n\n\nabun <- biom %>% biom_data()\nabun %>% class\n\n[1] \"dgCMatrix\"\nattr(,\"package\")\n[1] \"Matrix\"\n\nabun %>% dim\n\n[1] 25495    85\n\nThe abundance matrix is stored as a sparse matrix from the Matrix package.\nThat is fine for now, though phyloseq will want a standard (dense) matrix.\nNext, the sample metadata.\n\n\nsam <- biom %>% sample_metadata()\nsam %>% class\n\n[1] \"data.frame\"\n\nsam %>% head\n\n                      sample_type nucleotide_type collection_date\nERR3026532:288969 Airplane sewage             RNA         2013-01\nERR3026500:288937 Airplane sewage             RNA         2013-01\nERR3026576:289013 Airplane sewage             RNA         2013-01\nERR3026559:288996 Airplane sewage             RNA         2013-01\nERR3026571:289008 Airplane sewage             RNA         2013-01\nERR3026512:288949 Airplane sewage             RNA         2013-01\n                  water_control collection_location isolate\nERR3026532:288969            No            Pakistan      No\nERR3026500:288937            No               China      No\nERR3026576:289013            No             Denmark      No\nERR3026559:288996            No              Canada      No\nERR3026571:289008            No             Denmark      No\nERR3026512:288949            No               Japan      No\n                   Study Sample Name\nERR3026532:288969      Islamabad_2_c\nERR3026500:288937        Beijing_2_e\nERR3026576:289013    Library_blank_a\nERR3026559:288996          Toronto_e\nERR3026571:289008 Negative_control_a\nERR3026512:288949          Tokyo_1_d\n\nsam %>% glimpse\n\nRows: 85\nColumns: 7\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ `Study Sample Name` <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n\nThe sample metadata is a standard data frame with rownames giving sample ids, and the taxonomy information is stored as a list.\nNotice how all the variable names are in snake case except for one.\nThis is apparently because the CZID BIOM exports its own standard variables as snake case (though shows them otherwise in the online interface), but leaves custom variables unchanged.\nIt is convinient to standardize all variable names to snake case; an easy way to do this is with the function janitor::clean_names().\n\n\nsam <- sam %>%\n  janitor::clean_names() %>%\n  glimpse\n\nRows: 85\nColumns: 7\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ study_sample_name   <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n\nNext, the taxonomy table, or what the biomformat package calls the ‘observation metadata’.\n\n\ntax <- biom %>% observation_metadata()\ntax %>% class\n\n[1] \"list\"\n\ntax %>% head(2)\n\n$`Bacteria;;Proteobacteria;Alphaproteobacteria;Rhizobiales;Xanthobacteraceae;Azorhizobium;Azorhizobium caulinodans`\n                 taxonomy1                  taxonomy2 \n                \"Bacteria\"                         \"\" \n                 taxonomy3                  taxonomy4 \n          \"Proteobacteria\"      \"Alphaproteobacteria\" \n                 taxonomy5                  taxonomy6 \n             \"Rhizobiales\"        \"Xanthobacteraceae\" \n                 taxonomy7                  taxonomy8 \n            \"Azorhizobium\" \"Azorhizobium caulinodans\" \n\n$`Bacteria;;Proteobacteria;Gammaproteobacteria;Enterobacterales;Erwiniaceae;Buchnera;Buchnera aphidicola`\n            taxonomy1             taxonomy2             taxonomy3 \n           \"Bacteria\"                    \"\"      \"Proteobacteria\" \n            taxonomy4             taxonomy5             taxonomy6 \n\"Gammaproteobacteria\"    \"Enterobacterales\"         \"Erwiniaceae\" \n            taxonomy7             taxonomy8 \n           \"Buchnera\" \"Buchnera aphidicola\" \n\nWe can see that here we have a list, with one element per taxon.\nThe documentation for biomformat::observation_metadata indicates that this function may return a ‘data.frame’ rather than a list, if it is able to, but does not say under what conditions that will be the case.\nUltimately we want a data frame (or tibble).\nThe following code chunk checks which we have, and if we have a list, tries to turn it into a data frame by spreading out the taxonomy vector of each list element into a table.\n\n\ntax_tmp <- biom %>% observation_metadata()\nif (is.data.frame(tax_tmp)) {\n  tax <- tax_tmp %>% as_tibble(rownames = '.otu')\n} else {\n  tax <- tax_tmp %>% \n    enframe(name = 'feature_id') %>% \n    unnest_wider(value)\n}\nrm(tax_tmp)\ntax %>% glimpse\n\nRows: 25,495\nColumns: 9\n$ feature_id <chr> \"Bacteria;;Proteobacteria;Alphaproteobacteria;Rhi…\n$ taxonomy1  <chr> \"Bacteria\", \"Bacteria\", \"Bacteria\", \"Bacteria\", \"…\n$ taxonomy2  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ taxonomy3  <chr> \"Proteobacteria\", \"Proteobacteria\", \"Actinobacter…\n$ taxonomy4  <chr> \"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Ac…\n$ taxonomy5  <chr> \"Rhizobiales\", \"Enterobacterales\", \"Micrococcales…\n$ taxonomy6  <chr> \"Xanthobacteraceae\", \"Erwiniaceae\", \"Cellulomonad…\n$ taxonomy7  <chr> \"Azorhizobium\", \"Buchnera\", \"Cellulomonas\", \"Dict…\n$ taxonomy8  <chr> \"Azorhizobium caulinodans\", \"Buchnera aphidicola\"…\n\nTo facilitate downstream analysis, it is helpful to so some cleanup:\nReplace the taxonomic ranks with the standard NCBI rank names (see an example NCBI taxonomic record)\nIn cases where the rank is missing/unassigned, replace the empty string with NA\n\n\n\nrnks <- c('superkingdom', 'kingdom', 'phylum', 'class', 'order', 'family',\n  'genus', 'species')\ncolnames(tax)[2:9] <- rnks\n# use NA for missing ranks\ntax <- tax %>%\n  mutate(\n    across(everything(), ~ifelse(. == \"\", NA_character_, .))\n  )\ntax %>% glimpse\n\nRows: 25,495\nColumns: 9\n$ feature_id   <chr> \"Bacteria;;Proteobacteria;Alphaproteobacteria;R…\n$ superkingdom <chr> \"Bacteria\", \"Bacteria\", \"Bacteria\", \"Bacteria\",…\n$ kingdom      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ phylum       <chr> \"Proteobacteria\", \"Proteobacteria\", \"Actinobact…\n$ class        <chr> \"Alphaproteobacteria\", \"Gammaproteobacteria\", \"…\n$ order        <chr> \"Rhizobiales\", \"Enterobacterales\", \"Micrococcal…\n$ family       <chr> \"Xanthobacteraceae\", \"Erwiniaceae\", \"Cellulomon…\n$ genus        <chr> \"Azorhizobium\", \"Buchnera\", \"Cellulomonas\", \"Di…\n$ species      <chr> \"Azorhizobium caulinodans\", \"Buchnera aphidicol…\n\nNow let’s import all three tables into a single phyloseq object.\nThis involves converting each individual table into the corresponding class from the phyloseq package, and then combiningg these into one phyloseq-class object.\n\n\nps <- phyloseq(\n  otu_table(abun %>% as.matrix, taxa_are_rows = TRUE),\n  sample_data(sam),\n  tax_table(tax)\n)\n\n\nNote that we had to first coerce the abundance matrix to a standard dense matrix; we also needed to tell phyloseq that taxa corresponded to rows in the matrix.\nBasic data checks and stats\nTODO: explain below\n\n\nps <- ps %>%\n  mutate_sample_data(., \n    sample_sum = sample_sums(.)\n  )\nsam <- ps %>% sample_data %>% as_tibble\ntax <- ps %>% tax_table %>% as_tibble\n\n\n\n\nps %>% t\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 25495 taxa and 85 samples ]:\nsample_data() Sample Data:        [ 85 samples by 8 sample variables ]:\ntax_table()   Taxonomy Table:     [ 25495 taxa by 8 taxonomic ranks ]:\ntaxa are columns\n\n\n\nps %>% sample_names %>% head\n\n[1] \"ERR3026532:288969\" \"ERR3026500:288937\" \"ERR3026576:289013\"\n[4] \"ERR3026559:288996\" \"ERR3026571:289008\" \"ERR3026512:288949\"\n\nps %>% sample_data %>% glimpse\n\nRows: 85\nColumns: 8\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ study_sample_name   <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n$ sample_sum          <dbl> 83328, 188401, 313204, 234338, 767843, 5…\n\n\n\nsam %>%\n  ggplot(aes(sample_sum, fill = collection_location)) +\n  scale_x_log10() +\n  geom_histogram()\n\n\n\n\n\ntaxon_stats <- ps %>%\n  as_tibble %>%\n  mutate(across(superkingdom, fct_explicit_na)) %>%\n  with_groups(c(.otu, superkingdom), summarize, \n    prev_1 = sum(.abundance >= 1),\n    prev_10 = sum(.abundance >= 10),\n    total = sum(.abundance),\n    proportion = mean(.abundance / sample_sum)\n  )\n\n\n\n\ntaxon_stats %>%\n  pivot_longer(-c(.otu, superkingdom)) %>%\n  ggplot(aes(value, fill = superkingdom)) +\n  facet_wrap(~name, scales = 'free') +\n  scale_x_log10() +\n  scale_fill_brewer(type = 'qual') +\n  geom_histogram() \n\n\n\nTaxonomy\nNCBI taxonomy has recently received changes in some prokaryotic phylum names.\nLet’s check to see which version of phylum names are being used here, by seeing whether a Bacteroides species’ phylum is listed as ‘Bacteroidetes’ (old name) or ‘Bacteroidota’ (new name).\n\n\ntax %>%\n  filter(genus == 'Bacteroides') %>%\n  slice(1)%>%\n  glimpse\n\nRows: 1\nColumns: 9\n$ .otu         <chr> \"Bacteria;;Bacteroidetes;Bacteroidia;Bacteroida…\n$ superkingdom <chr> \"Bacteria\"\n$ kingdom      <chr> NA\n$ phylum       <chr> \"Bacteroidetes\"\n$ class        <chr> \"Bacteroidia\"\n$ order        <chr> \"Bacteroidales\"\n$ family       <chr> \"Bacteroidaceae\"\n$ genus        <chr> \"Bacteroides\"\n$ species      <chr> \"Bacteroides fragilis\"\n\nIf we look at this taxon in NCBI taxonomy, we can see that NCBI has adopted the new phylum name ‘Bacteroidota’; however, here we see the old phylum name.\nThis suggests that CZID is currently using an older version of NCBI prior to the name change.\nsee\n- https://ncbiinsights.ncbi.nlm.nih.gov/2021/12/10/ncbi-taxonomy-prokaryote-phyla-added/\n- https://www.the-scientist.com/news-opinion/newly-renamed-prokaryote-phyla-cause-uproar-69578\n\nCheck classification percentages\n\n\ntax %>%\n  pivot_longer(-.otu, names_to = 'rank') %>%\n  with_groups(rank, summarize,\n    features_classified = sum(!is.na(value)),\n    features_total = n()\n  ) %>%\n  mutate(\n    frac_classified = features_classified / features_total,\n    rank = factor(rank, rank_names(ps))\n  ) %>%\n  arrange(rank)\n\n# A tibble: 8 × 4\n  rank         features_classified features_total frac_classified\n  <fct>                      <int>          <int>           <dbl>\n1 superkingdom               25381          25495           0.996\n2 kingdom                     4995          25495           0.196\n3 phylum                     22667          25495           0.889\n4 class                      22119          25495           0.868\n5 order                      22199          25495           0.871\n6 family                     21744          25495           0.853\n7 genus                      20649          25495           0.810\n8 species                    25493          25495           1.00 \n\nThis analysis points to some notable features of the data.\nFor example, not every taxonomic feature has a superkingdom.\nLet’s take a look at some of those ‘species’ that don’t,\n\n\nset.seed(42)\ntax %>%\n  filter(is.na(superkingdom)) %>%\n  select(superkingdom, kingdom, genus, species) %>%\n  slice_sample(n=20)\n\n# A tibble: 20 × 4\n   superkingdom kingdom genus species                                 \n   <chr>        <chr>   <chr> <chr>                                   \n 1 <NA>         <NA>    <NA>  Cloning vector pPKm-230                 \n 2 <NA>         <NA>    <NA>  Cloning vector pMT450                   \n 3 <NA>         <NA>    <NA>  Vector pAAV-hSyn1-FLEX-GAP43-GCaMP6s-P2…\n 4 <NA>         <NA>    <NA>  uncultured microorganism                \n 5 <NA>         <NA>    <NA>  Cloning vector shRNA EYFP-P2A Puro      \n 6 <NA>         <NA>    <NA>  Cloning vector pMT449                   \n 7 <NA>         <NA>    <NA>  IncQ plasmid pIE1120                    \n 8 <NA>         <NA>    <NA>  Cloning vector pHal7-FAPG462VRFP        \n 9 <NA>         <NA>    <NA>  Shuttle vector pG106                    \n10 <NA>         <NA>    <NA>  uncultured gut microbe of Zootermopsis …\n11 <NA>         <NA>    <NA>  Vector EP-Pol                           \n12 <NA>         <NA>    <NA>  Cloning vector pRGPDuo4                 \n13 <NA>         <NA>    <NA>  Cloning vector IA5_YQR_DIMER            \n14 <NA>         <NA>    <NA>  Plasmid pMCBF1                          \n15 <NA>         <NA>    <NA>  Plasmid pM3                             \n16 <NA>         <NA>    <NA>  uncultured marine organism              \n17 <NA>         <NA>    <NA>  Transposon Tn4551                       \n18 <NA>         <NA>    <NA>  Sphinx1.76-related DNA                  \n19 <NA>         <NA>    <NA>  Cloning vector pMT451                   \n20 <NA>         <NA>    <NA>  uncultured marine microorganism         \n\nCan see that CZID report reference sequences that are in NT but don’t corresopnd to known organisms.\nWhat about ‘species’ without intermediate ranks?\n\n\ntax %>%\n  filter(!is.na(superkingdom), is.na(family)) %>%\n  select(superkingdom, kingdom, phylum, family, genus, species) %>%\n  slice_sample(n=20)\n\n# A tibble: 20 × 6\n   superkingdom kingdom phylum          family genus           species\n   <chr>        <chr>   <chr>           <chr>  <chr>           <chr>  \n 1 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n 2 Bacteria     <NA>    <NA>            <NA>   <NA>            swine …\n 3 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n 4 Bacteria     <NA>    <NA>            <NA>   <NA>            butyra…\n 5 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n 6 Bacteria     <NA>    Chloroflexi     <NA>   Dehalogenimonas Dehalo…\n 7 Bacteria     <NA>    Proteobacteria  <NA>   <NA>            alpha …\n 8 Eukaryota    <NA>    Bacillariophyta <NA>   <NA>            uncult…\n 9 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n10 Bacteria     <NA>    Actinobacteria  <NA>   <NA>            actino…\n11 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n12 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n13 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n14 Bacteria     <NA>    Proteobacteria  <NA>   <NA>            arseni…\n15 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n16 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n17 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n18 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n19 Archaea      <NA>    <NA>            <NA>   <NA>            uncult…\n20 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n\nCan also see cases where a species does not have an intermediate rank defined, such as family.\nHaving NA for intermediate ranks could cause issues, and we might consider replacing these with a new string such as ‘Enterobacterales_unclassified’.\nHow do features break down by superkingdom?\n\n\ntax %>% \n  count(superkingdom) %>%\n  mutate(fraction = n / sum(n))\n\n# A tibble: 5 × 3\n  superkingdom     n fraction\n  <chr>        <int>    <dbl>\n1 Archaea        391  0.0153 \n2 Bacteria     16583  0.650  \n3 Eukaryota     5655  0.222  \n4 Viruses       2752  0.108  \n5 <NA>           114  0.00447\n\nAnalysis\nRecreating Figure 4 from Rothman et al. (2021) using the Hjelmso data\nFigure 4 from Rothman et al. (2021) combines three common types of plots in microbiome analysis: An ordination plot to visualize the similarities and differences between samples, a plot showing the distribution of an alpha diversity metric (Shannon index) across samples, and the proportions (relative abundance) of particular species across samples (faceted by species).\nHere I’ll show how to (mostly) recreate this plot using the Hjelmso data.\nFirst, we’ll filter out some samples and taxa, which is a typical first step to any analysis.\nThere is a lot more to say about how you might do said filtering; but here I’ll\nRemove samples with very low read counts, since the low read counts can be a sign of experimental issues with those samples and can skew interpretation of some analyses\nSubset to just viruses (the Rothman analysis only considers viruses)\nRemove species not appearing in at least 2 samples and 10 reads, which will speed up calculations and likely make our results more meaningful since these identifications can easily be spurious.\nAggregate to the genus level\n\n\n\nps_plot <- ps %>%\n  filter_sample_data(sample_sum > 1e5) %>%\n  filter_tax_table(superkingdom == 'Viruses') %>%\n  filter_taxa2(~ sum(. > 0) > 2 & sum(.) >= 10) %>%\n  tax_glom('genus', NArm = TRUE) %>%\n  print\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 110 taxa and 84 samples ]:\nsample_data() Sample Data:        [ 84 samples by 8 sample variables ]:\ntax_table()   Taxonomy Table:     [ 110 taxa by 8 taxonomic ranks ]:\ntaxa are rows\n\nNow there are only 110 species being considered, compared to 25495 in the entire CZID output.\nNote that taxa with a missing genus name have been filtered out; this is the phyloseq default but it can have big effects so need to be aware and consider whether this is desired for a given analysis.\nNote, the names of the taxonomic features after aggregation are set to the name of the most abundant feature within the genus; they are not automatically set to the genus name.\nThat is easy to do manually, provided that you have unique genus names (which is not always the case).\nIt can be useful to check if the genus names are unique for plotting by genus later on,\n\n\nps_plot %>% tax_table %>% as_tibble %>% pull(genus) %>% anyDuplicated\n\n[1] 0\n\nThere are no duplicates, so we can uniquely refer to taxa by genus name.\nBray-Curtis NMDS ordination (Panel A)\nThere are many ways to do this; here I’ll use the ordinate() and plot_ordination() helper function from phyloseq to create the NMDS plot using the Bray-Curtis community dissimilarity metric.\nNote, that it is important to manually normalize the abundances to have the same total in each sample (e.g. by normalizing to proportions, as done here), otherwise the different total counts across samples will affect the results.\n\n\nnmds <- ps_plot %>%\n  transform_sample_counts(~ . / sum(.)) %>%\n  ordinate(method = \"NMDS\", distance = \"bray\", trymax = 50)\n\nRun 0 stress 0.1002992 \nRun 1 stress 0.1173969 \nRun 2 stress 0.1480709 \nRun 3 stress 0.1173968 \nRun 4 stress 0.1002992 \n... Procrustes: rmse 3.717243e-06  max resid 1.691263e-05 \n... Similar to previous best\nRun 5 stress 0.1424676 \nRun 6 stress 0.130377 \nRun 7 stress 0.1002992 \n... Procrustes: rmse 2.661887e-06  max resid 1.175533e-05 \n... Similar to previous best\nRun 8 stress 0.1002992 \n... Procrustes: rmse 1.104143e-05  max resid 6.17019e-05 \n... Similar to previous best\nRun 9 stress 0.1173976 \nRun 10 stress 0.1173975 \nRun 11 stress 0.1292318 \nRun 12 stress 0.1294527 \nRun 13 stress 0.1002992 \n... New best solution\n... Procrustes: rmse 5.355197e-06  max resid 1.53618e-05 \n... Similar to previous best\nRun 14 stress 0.1629023 \nRun 15 stress 0.117396 \nRun 16 stress 0.1417549 \nRun 17 stress 0.1417084 \nRun 18 stress 0.1002992 \n... Procrustes: rmse 1.162339e-05  max resid 7.001561e-05 \n... Similar to previous best\nRun 19 stress 0.1002992 \n... Procrustes: rmse 7.305224e-06  max resid 4.166893e-05 \n... Similar to previous best\nRun 20 stress 0.1292315 \n*** Best solution repeated 3 times\n\np_ord <- plot_ordination(ps_plot, nmds, \n  color = \"collection_location\", type = \"samples\"\n) +\n  labs(color = 'Country')\np_ord \n\n\n\nAlpha diversity (Panel B)\nWe can compute Shannon alpha diversity index for each sample in a variety of ways:\nphyloseq::estimate_richness()\nvegan::diversity()\nPerforming the calculation ourselves from the definition\n\n\n\nshannon_index <- otu_table(ps_plot) %>% \n  orient_taxa(as = 'cols') %>%\n  vegan::diversity()\nshannon_index %>% head\n\nERR3026500:288937 ERR3026576:289013 ERR3026559:288996 \n        1.3500820         0.6443310         1.3777211 \nERR3026571:289008 ERR3026512:288949 ERR3026526:288963 \n        0.9189514         2.0592347         1.6738584 \n\nNote that we needed to reorient the abundance matrix (i.e. OTU table) to have taxa corresponding to columns, as this is what functions in the vegan package expect.\nWe can tell that we used the correct orientation because the resulting diversity values are in a named vector where the names correspond to the sample names.\nIf we had passed the matrix in the incorrect orientation, then the vector names would be the taxa names.\nLet’s add the Shannon index to a copy of the sample data,\n\n\nsam_plot <- ps_plot %>% sample_data %>% as_tibble %>%\n  add_column(shannon_index = shannon_index)\n\n\nthen create the plot,\n\n\np_div <- sam_plot %>%\n  ggplot(aes(y = shannon_index, x = collection_location, \n      color = collection_location)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_quasirandom() +\n  expand_limits(y = 0) +\n  # scale_color_manual(values = colors_countries) +\n  labs(x = 'Country', y = 'Shannon index') +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    legend.position = 'none'\n  )\np_div\n\n\n\nNote, I’m plotting the data points over the box plots, since it is useful to see the scatter when we’re working with a relatively small number of points like this.\nI therefore turned off the plotting of outliers in the boxplot layer.\nNote, I suggest using the exponential of the Shannon index and plotting on a log scale), so that the numbers shown on the axis are in terms of effective number of species.\nRelative abundances (Proportions) (Panel C)\nIn the actual Rothman figure, the abundances for a set of viruses are shown; the particular viruses were picked based on an analysis to determine viruses that vary across treatment plant, using the ANCOM R package.\nI may do that in a future version of this script, but for now I’ll just pick the 10 most abundant viruses by average proportion.\nFirst, get a data frame for plotting, with the proportions of all taxa alongside the original read counts,\n\n\nx <- ps_plot %>%\n  as_tibble %>%\n  with_groups(.sample, mutate,\n    proportion = .abundance / sum(.abundance))\n\n\nNext, get the top 10 viruses by median proportion. We can do this various ways, e.g.\n\n\ntop_viruses1 <- ps_plot %>% \n  transform_sample_counts(~ . / sum(.)) %>%\n  orient_taxa(as = 'rows') %>%\n  otu_table %>%\n  apply(1, median) %>%\n  sort(decreasing = TRUE) %>%\n  head(10) %>%\n  names\n\n\nor\n\n\ntop_viruses2 <- x %>%\n  with_groups(.otu, summarize, across(proportion, median)) %>%\n  slice_max(proportion, n = 10) %>%\n  print %>%\n  pull(.otu)\n\n# A tibble: 10 × 2\n   .otu                                                        propo…¹\n   <chr>                                                         <dbl>\n 1 Viruses;Orthornavirae;Kitrinoviricota;Alsuviricetes;Martel… 0.282  \n 2 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.0821 \n 3 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Siphoviri… 0.0456 \n 4 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Siphoviri… 0.0127 \n 5 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Herellevi… 0.00844\n 6 Viruses;Orthornavirae;Pisuviricota;Duplopiviricetes;Durnav… 0.00821\n 7 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.00500\n 8 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Podovirid… 0.00478\n 9 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.00353\n10 Viruses;Orthornavirae;Pisuviricota;Pisoniviricetes;Picorna… 0.00323\n# … with abbreviated variable name ¹​proportion\n\nidentical(top_viruses1, top_viruses2)\n\n[1] TRUE\n\n\n\np_prop <- x %>%\n  filter(.otu %in% top_viruses1) %>%\n  mutate(\n    across(genus, fct_reorder, proportion, .fun = median, .desc = TRUE),\n  ) %>%\n  ggplot(aes(x = collection_location, y = proportion,\n      color = collection_location)) +\n  facet_wrap(~genus, nrow = 2, scales = 'free_y') +\n  # scale_y_log10() +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(y = 'Proportion', x = 'Country') +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    legend.position = 'none'\n  )\np_prop\n\n\n\nNote, I’ve ordered the facets as decreasing in median proportion.\nPut the panels together\nPutting multiple panels together is often very easy with the patchwork package loaded,\n\n\n(p_ord + p_div) / p_prop +\n  plot_annotation(tag_levels = 'A')\n\n\n\nThis plot could definitely benefit from some extra fiddling, to adjust the spacing and colors etc.\n\n\n\nHjelmsø, Mathis Hjort, Sarah Mollerup, Randi Holm Jensen, Carlotta Pietroni, Oksana Lukjancenko, Anna Charlotte Schultz, Frank M. Aarestrup, and Anders Johannes Hansen. 2019. “Metagenomic Analysis of Viruses in Toilet Waste from Long Distance Flights New Procedure for Global Infectious Disease Surveillance.” PLOS ONE 14 (1): e0210368. https://doi.org/10.1371/journal.pone.0210368.\n\n\nMcDonald, Daniel, Jose C Clemente, Justin Kuczynski, Jai Ram Rideout, Jesse Stombaugh, Doug Wendel, Andreas Wilke, et al. 2012. “The Biological Observation Matrix (BIOM) Format or: How I Learned to Stop Worrying and Love the Ome-Ome.” GigaScience 1 (1): 2047-217X-1-7. https://doi.org/10.1186/2047-217X-1-7.\n\n\nRothman, Jason A., Theresa B. Loveless, Joseph Kapcia, Eric D. Adams, Joshua A. Steele, Amity G. Zimmer-Faust, Kylie Langlois, et al. 2021. “RNA Viromics of Southern California Wastewater and Detection of SARS-CoV-2 Single-Nucleotide Variants.” Applied and Environmental Microbiology 87 (23): e01448–21. https://doi.org/10.1128/AEM.01448-21.\n\n\n\n\n",
    "preview": "posts/2022-10-27-czid-r-analysis-demo/czid-r-analysis-demo_files/figure-html5/unnamed-chunk-19-1.svg",
    "last_modified": "2023-02-13T15:42:57+00:00",
    "input_file": {}
  }
]
