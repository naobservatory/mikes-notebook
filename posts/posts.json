[
  {
    "path": "posts/2023-06-17-amicon-mwcos-and-dissoc-treatments/",
    "title": "Evaluate viral MGS protocols: Dissociation treatments and Amicon MWCOs",
    "description": {},
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-06-24",
    "categories": [
      "R",
      "qPCR"
    ],
    "contents": "\n\nContents\nData import\nAnalysis\nInspect SARS2 amplification curves\nQubit measurements\nLinear modeling\n\nSession info\n\nThis experiment compares two different Amicon molecular weight cut-offs (MWCOs), 30 kDa and 100 kDa, and 4 dissociation treatments (None, NaCl, Tween-20, and a method using beef extract (BE).\nDrive folder\n\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(here)\n\nlibrary(knitr)\n\nlibrary(broom)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n# Okabe Ito color scheme with amber for yellow; see https://easystats.github.io/see/reference/scale_color_okabeito.html\ncolors_oi <- grDevices::palette.colors()  \ncolors_oi['yellow'] <- \"#F5C710\"\n\n# Custon qPCR helpers\nsource('_functions.R')\n\n\nData import\n\n\nfns <- '_data' %>%\n  dir_ls(recurse = TRUE, glob = '*_Results_*.csv')\nfns %>% path_file\n\n[1] \"2023-06-15_OSH-16S-Amicon_Results_20230616 113441.csv\"          \n[2] \"2023-06-15_OSH_CrA-amicon_Results_20230616 114121.csv\"          \n[3] \"2023-06-15_OSH-Phg-Amicon_Results_20230616 121105.csv\"          \n[4] \"2023-06-15_OSH-Cov2-Amicon_Raw_Results_20230616 105748.csv\"     \n[5] \"2023-06-15_OSH-Cov2-Amicon_Adjusted_Results_20230616 105816.csv\"\n\nI’ll just use the ‘Adjusted’ SARS2 results when I import,\n\n\nfns_filt <- fns %>% str_subset(negate = TRUE, 'Raw')\nfns_filt %>% path_file\n\n[1] \"2023-06-15_OSH-16S-Amicon_Results_20230616 113441.csv\"          \n[2] \"2023-06-15_OSH_CrA-amicon_Results_20230616 114121.csv\"          \n[3] \"2023-06-15_OSH-Phg-Amicon_Results_20230616 121105.csv\"          \n[4] \"2023-06-15_OSH-Cov2-Amicon_Adjusted_Results_20230616 105816.csv\"\n\nWe can use the following snippet to pull out the target name from the file path,\n\n\nbase <- fns %>% path_common\nbase_num <- base %>% path_split %>% .[[1]] %>% length\nfns %>% path_split %>% map(tail, -base_num) %>% map_chr(head, 1)\n\n[1] \"[2023-06-15] 16S\"        \"[2023-06-15] Crassphage\"\n[3] \"[2023-06-15] Phagemid\"   \"[2023-06-15] SARS-CoV-2\"\n[5] \"[2023-06-15] SARS-CoV-2\"\n\n\n\n# Pattern for extracting experimental metadata from sample names; \n# Experimental metadata stored in qpcr sample name as '<sewer_system>_<amicon_mwco>_<dissoc_treatment>'\npat <- '(N|S)_(30|100)_(None|Tween|BE|NaCl)'\n\nresults <- tibble(file = fns_filt) %>%\n  mutate(\n    dir_name = file %>% path_split %>% map(tail, -base_num) %>% map_chr(head, 1),\n    data = map(file, read_qpcr_results_csv)\n  ) %>%\n  # select(-file) %>%\n  separate(dir_name, into = c('dir_date', 'dir_target'), remove = FALSE, sep = ' ') %>%\n  mutate(\n    dir_date = dir_date %>% lubridate::ymd(),\n  ) %>%\n  unnest(data) %>%\n  # Extract experimental sample metadata from sample names\n  mutate(\n    match = str_match(sample, pat),\n  ) %>%\n  mutate(.keep = 'unused',\n    sample_experiment = match[,1],\n    sewer_system = match[,2],\n    amicon_mwco = match[,3] %>% fct_relevel('30'),\n    dissoc_treatment = match[,4] %>% fct_relevel('None'),\n  ) %>%\n  mutate(\n    cq = ifelse(is.na(cq), 40, cq)\n  ) %>%\n  glimpse\n\nRows: 264\nColumns: 32\n$ file                  <chr> \"_data/[2023-06-15] 16S/2023-06-15_OSH…\n$ dir_name              <chr> \"[2023-06-15] 16S\", \"[2023-06-15] 16S\"…\n$ dir_date              <date> 2023-06-15, 2023-06-15, 2023-06-15, 2…\n$ dir_target            <chr> \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16…\n$ well                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15,…\n$ well_position         <chr> \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A…\n$ row                   <ord> A, A, A, A, A, A, A, A, A, B, B, B, B,…\n$ column                <ord> 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4,…\n$ omit                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ sample                <chr> \"N_30_None\", \"N_30_None\", \"N_30_None\",…\n$ target                <chr> \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16…\n$ task                  <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKN…\n$ reporter              <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FA…\n$ quencher              <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-…\n$ amp_status            <chr> \"Amp\", \"Amp\", \"Amp\", \"Amp\", \"Amp\", \"No…\n$ amp_score             <dbl> 1.5151167, 1.5159783, 1.4994412, 1.228…\n$ curve_quality         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ result_quality_issues <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ cq                    <dbl> 21.95765, 21.82037, 21.87888, 31.92359…\n$ cq_confidence         <dbl> 0.9866581, 0.9902164, 0.9869926, 0.938…\n$ cq_mean               <dbl> 21.88563, 21.88563, 21.88563, 31.79154…\n$ cq_sd                 <dbl> 0.06889292, 0.06889292, 0.06889292, 0.…\n$ auto_threshold        <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ threshold             <dbl> 0.1680576, 0.1680576, 0.1680576, 0.168…\n$ auto_baseline         <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ baseline_start        <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ baseline_end          <int> 17, 16, 17, 25, 25, 39, 14, 24, 15, 16…\n$ cq_status             <chr> \"Determined\", \"Determined\", \"Determine…\n$ sample_experiment     <chr> \"N_30_None\", \"N_30_None\", \"N_30_None\",…\n$ sewer_system          <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", NA, NA, …\n$ amicon_mwco           <fct> 30, 30, 30, 30, 30, 30, NA, NA, NA, 30…\n$ dissoc_treatment      <fct> None, None, None, BE, BE, BE, NA, NA, …\n\nNote, in cases where the Cq value could not be determined, I set the value to 40.\n\n\nresults %>% count(dir_target)\n\n# A tibble: 4 × 2\n  dir_target     n\n  <chr>      <int>\n1 16S           66\n2 Crassphage    66\n3 Phagemid      69\n4 SARS-CoV-2    63\n\nresults %>% count(dir_target, file %>% path_file)\n\n# A tibble: 4 × 3\n  dir_target `file %>% path_file`                                    n\n  <chr>      <chr>                                               <int>\n1 16S        2023-06-15_OSH-16S-Amicon_Results_20230616 113441.…    66\n2 Crassphage 2023-06-15_OSH_CrA-amicon_Results_20230616 114121.…    66\n3 Phagemid   2023-06-15_OSH-Phg-Amicon_Results_20230616 121105.…    69\n4 SARS-CoV-2 2023-06-15_OSH-Cov2-Amicon_Adjusted_Results_202306…    63\n\n# results %>% count(target)\nresults %>% count(dir_target, target)\n\n# A tibble: 4 × 3\n  dir_target target       n\n  <chr>      <chr>    <int>\n1 16S        16S         66\n2 Crassphage CrA         66\n3 Phagemid   Target 1    69\n4 SARS-CoV-2 Target 1    63\n\n# results %>% count(sample) %>% print(n=Inf)\n\n\n\n\nresults %>% count(sample) %>% deframe\n\n          100.0          1000.0         10000.0        100000.0 \n              3               3               3               3 \n           10E3            10E4            10E5            10E6 \n              3               3               3               3 \n           10E7              75             750            7500 \n              3               3               3               3 \n          75000          750000 CPT ultrafilter        N_100_BE \n              3               3               3              12 \n     N_100_NaCl      N_100_None     N_100_Tween         N_30_BE \n             12              12              12              12 \n      N_30_NaCl       N_30_None      N_30_Tween        S_100_BE \n             12              12              12              12 \n     S_100_NaCl      S_100_None     S_100_Tween         S_30_BE \n             12              12              12              12 \n      S_30_NaCl       S_30_None      S_30_Tween           Std 1 \n             12              12              12               3 \n          Std 2           Std 3           Std 4           Std 5 \n              3               3               3               3 \n           <NA> \n             12 \n\nresults %>% count(sample_experiment) %>% deframe\n\n   N_100_BE  N_100_NaCl  N_100_None N_100_Tween     N_30_BE \n         12          12          12          12          12 \n  N_30_NaCl   N_30_None  N_30_Tween    S_100_BE  S_100_NaCl \n         12          12          12          12          12 \n S_100_None S_100_Tween     S_30_BE   S_30_NaCl   S_30_None \n         12          12          12          12          12 \n S_30_Tween        <NA> \n         12          72 \n\nTODO: Check status of samples with no name; are these blanks?\nLoad amplification data,\n\n\nfns_amp <- '_data' %>%\n  dir_ls(recurse = TRUE, glob = '*_Amplification Data_*.csv') %>%\n  str_subset(negate = TRUE, 'Raw')\n\namp <- tibble(file = fns_amp) %>%\n  mutate(\n    dir_name = file %>% path_split %>% map(tail, -base_num) %>% map_chr(head, 1),\n    data = map(file, read_qpcr_amplification_csv)\n  ) %>%\n  # select(-file) %>%\n  separate(dir_name, into = c('dir_date', 'dir_target'), remove = FALSE, sep = ' ') %>%\n  mutate(\n    dir_date = dir_date %>% lubridate::ymd(),\n  ) %>%\n  unnest(data) %>%\n  glimpse\n\nRows: 10,680\nColumns: 14\n$ file          <chr> \"_data/[2023-06-15] 16S/2023-06-15_OSH-16S-Ami…\n$ dir_name      <chr> \"[2023-06-15] 16S\", \"[2023-06-15] 16S\", \"[2023…\n$ dir_date      <date> 2023-06-15, 2023-06-15, 2023-06-15, 2023-06-1…\n$ dir_target    <chr> \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16S…\n$ well          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ well_position <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ row           <ord> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A…\n$ column        <ord> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ cycle_number  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ target        <chr> \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16S…\n$ rn            <dbl> 2.798063, 2.799998, 2.801923, 2.808614, 2.8092…\n$ d_rn          <dbl> -0.0024182955, -0.0022691886, -0.0021315257, 0…\n$ sample        <chr> \"N_30_None\", \"N_30_None\", \"N_30_None\", \"N_30_N…\n$ omit          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n\nresults_min <- results %>%\n  select(dir_name, well, task:dissoc_treatment)\namp <- amp %>% \n  left_join(results_min, by = c('dir_name', 'well'))\n\n\nTODO: find a better way to import the amplification data and results at the same time\nGet the baseline coordinates for plotting,\n\n\nbaselines <- results %>%\n  pivot_longer(\n    cols = c(baseline_start, baseline_end),\n    names_to = 'baseline_boundary',\n    values_to = 'cycle_number',\n    names_prefix = 'baseline_',\n  ) %>%\n  left_join(\n    amp %>% select(well_position, cycle_number, rn, d_rn), \n    by = c('well_position', 'cycle_number')\n  ) %>%\n  glimpse\n\nRows: 2,094\nColumns: 34\n$ file                  <chr> \"_data/[2023-06-15] 16S/2023-06-15_OSH…\n$ dir_name              <chr> \"[2023-06-15] 16S\", \"[2023-06-15] 16S\"…\n$ dir_date              <date> 2023-06-15, 2023-06-15, 2023-06-15, 2…\n$ dir_target            <chr> \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16…\n$ well                  <int> 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,…\n$ well_position         <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A…\n$ row                   <ord> A, A, A, A, A, A, A, A, A, A, A, A, A,…\n$ column                <ord> 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,…\n$ omit                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ sample                <chr> \"N_30_None\", \"N_30_None\", \"N_30_None\",…\n$ target                <chr> \"16S\", \"16S\", \"16S\", \"16S\", \"16S\", \"16…\n$ task                  <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKN…\n$ reporter              <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FA…\n$ quencher              <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-…\n$ amp_status            <chr> \"Amp\", \"Amp\", \"Amp\", \"Amp\", \"Amp\", \"Am…\n$ amp_score             <dbl> 1.515117, 1.515117, 1.515117, 1.515117…\n$ curve_quality         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ result_quality_issues <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ cq                    <dbl> 21.95765, 21.95765, 21.95765, 21.95765…\n$ cq_confidence         <dbl> 0.9866581, 0.9866581, 0.9866581, 0.986…\n$ cq_mean               <dbl> 21.88563, 21.88563, 21.88563, 21.88563…\n$ cq_sd                 <dbl> 0.06889292, 0.06889292, 0.06889292, 0.…\n$ auto_threshold        <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ threshold             <dbl> 0.1680576, 0.1680576, 0.1680576, 0.168…\n$ auto_baseline         <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ cq_status             <chr> \"Determined\", \"Determined\", \"Determine…\n$ sample_experiment     <chr> \"N_30_None\", \"N_30_None\", \"N_30_None\",…\n$ sewer_system          <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\"…\n$ amicon_mwco           <fct> 30, 30, 30, 30, 30, 30, 30, 30, 30, 30…\n$ dissoc_treatment      <fct> None, None, None, None, None, None, No…\n$ baseline_boundary     <chr> \"start\", \"start\", \"start\", \"start\", \"e…\n$ cycle_number          <int> 3, 3, 3, 3, 17, 17, 17, 17, 3, 3, 3, 3…\n$ rn                    <dbl> 2.8019226, 2.9759669, 5.9895606, 0.921…\n$ d_rn                  <dbl> -0.0021315257, 0.0002468450, -0.014340…\n\nAnalysis\nThree types of conditions:\nSewer system (N or S)\nAmicon filter pore size / molecular weight cutoff\nDissociation treatment\n\n\nresults %>%\n  count(sewer_system, amicon_mwco, dissoc_treatment)\n\n# A tibble: 17 × 4\n   sewer_system amicon_mwco dissoc_treatment     n\n   <chr>        <fct>       <fct>            <int>\n 1 N            30          None                12\n 2 N            30          BE                  12\n 3 N            30          NaCl                12\n 4 N            30          Tween               12\n 5 N            100         None                12\n 6 N            100         BE                  12\n 7 N            100         NaCl                12\n 8 N            100         Tween               12\n 9 S            30          None                12\n10 S            30          BE                  12\n11 S            30          NaCl                12\n12 S            30          Tween               12\n13 S            100         None                12\n14 S            100         BE                  12\n15 S            100         NaCl                12\n16 S            100         Tween               12\n17 <NA>         <NA>        <NA>                72\n\n\n\nresults_main <- results %>%\n  filter(!is.na(sample_experiment))\n\n\n\n\nresults_main %>%\n  ggplot(aes(x = cq, y = amicon_mwco, color = dissoc_treatment)) +\n  # theme_minimal_vgrid() +\n  theme(\n    legend.position = 'bottom',\n    panel.spacing = unit(1, 'char'),\n  ) +\n  labs(\n    y = 'Amicon MWCO (kDa)',\n    x = 'Cq value',\n    color = 'Dissoc. treatment',\n  ) +\n  facet_grid(sewer_system ~ dir_target, scales = 'free') +\n  scale_color_manual(values = colors_oi %>% unname) +\n  geom_vline(xintercept = 40, color = 'grey', size = 0.3) + \n  geom_quasirandom()\n\n\n\nThis figure splits out the qPCR results by target, Amicon MWCO and sewer system (N vs. S), with the points colors by dissociation treatment.\nThe three points of the same color for a given MWCO and facet panel are qPCR replicates.\nWe can see that the impact of the BE treatment is highly variable.\nBE tends to increase the Cq of 16S (decrease abundance) relative to the other treatments, but the effect is minor relative to None in the N-100 sample and large in the other three.\nBE decreases Crassphage and phagemid Cq values in the N-100 and S-100 samples (relative to other treatments), but increases the Cq in the N-30 and S-30 samples.\nThere is an extreme effect (increase in Cq) of BE on Crassphage in the S-30 sample, with two qPCR replicates failing to yield Cq values below 40.\nFor SARS2, we see BE appears to decrease abundance to a large extent across all four samples, to the extent that we fail to get Cq values for any replicates except one N-100 replicate.\nFrom these results, I suggest that we remove the BE treatment from further consideration, due to its high variance and strong negative effect on SARS2 measurement (along with the technical difficulty of applying it in the lab).\nInspect SARS2 amplification curves\nMany SARS2 BE replicates are shown in the results as didn’t amplify\nThe software will compute an average Cq value even if one of the three replicates is Undetermined.\nI should inspect the amplification curves to see what went wrong\n\n\n\ncondition_vars <- c('sewer_system', 'amicon_mwco', 'dissoc_treatment')\nresults %>%\n  filter(dir_target == 'SARS-CoV-2') %>%\n  summarize(.by = c(sample, condition_vars), num_na = sum(is.na(cq))) %>%\n  arrange(dissoc_treatment) %>%\n  print(n=Inf)\n\n# A tibble: 21 × 5\n   sample      sewer_system amicon_mwco dissoc_treatment num_na\n   <chr>       <chr>        <fct>       <fct>             <int>\n 1 N_30_None   N            30          None                  0\n 2 S_30_None   S            30          None                  0\n 3 N_100_None  N            100         None                  0\n 4 S_100_None  S            100         None                  0\n 5 N_30_BE     N            30          BE                    0\n 6 S_30_BE     S            30          BE                    0\n 7 N_100_BE    N            100         BE                    0\n 8 S_100_BE    S            100         BE                    0\n 9 N_30_NaCl   N            30          NaCl                  0\n10 S_30_NaCl   S            30          NaCl                  0\n11 N_100_NaCl  N            100         NaCl                  0\n12 S_100_NaCl  S            100         NaCl                  0\n13 N_30_Tween  N            30          Tween                 0\n14 S_30_Tween  S            30          Tween                 0\n15 N_100_Tween N            100         Tween                 0\n16 S_100_Tween S            100         Tween                 0\n17 100000.0    <NA>         <NA>        <NA>                  0\n18 10000.0     <NA>         <NA>        <NA>                  0\n19 1000.0      <NA>         <NA>        <NA>                  0\n20 100.0       <NA>         <NA>        <NA>                  0\n21 <NA>        <NA>         <NA>        <NA>                  0\n\n  # arrange(desc(num_na))\n\n\nHERE. Check amplification.\n\n\ndelta_rn_min <- 1e-3\nct_threshold <- results %>% filter(dir_target == 'SARS-CoV-2') %>% pull(threshold) %>% unique\nstopifnot(length(ct_threshold) == 1)\n\namp %>%\n  filter(\n    dir_target == 'SARS-CoV-2',\n    !is.na(sewer_system)\n  ) %>%\n  ggplot(aes(cycle_number, pmax(d_rn, delta_rn_min), color = dissoc_treatment)) +\n  facet_grid(sewer_system ~ amicon_mwco, scales = 'free') +\n  scale_color_manual(values = colors_oi %>% unname) +\n  scale_y_log10() +\n  geom_line(aes(group = well)) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  # scale_color_brewer(type = 'qual') +\n  # geom_point(data = baselines, aes(shape = baseline_boundary), size = 3) +\n  # scale_shape_manual(values = c(1, 4)) +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\n\n\n\n\n\ndelta_rn_min <- 1e-3\nct_threshold <- results %>% filter(dir_target == 'SARS-CoV-2') %>% pull(threshold) %>% unique\nstopifnot(length(ct_threshold) == 1)\n\namp %>%\n  filter(\n    dir_target == 'SARS-CoV-2',\n    !is.na(sewer_system)\n  ) %>%\n  ggplot(aes(cycle_number, pmax(rn, delta_rn_min), color = dissoc_treatment)) +\n  facet_grid(sewer_system ~ amicon_mwco, scales = 'free') +\n  scale_color_manual(values = colors_oi %>% unname) +\n  scale_y_log10() +\n  geom_line(aes(group = well)) +\n  # scale_color_brewer(type = 'qual') +\n  # geom_point(data = baselines, aes(shape = baseline_boundary), size = 3) +\n  # scale_shape_manual(values = c(1, 4)) +\n  labs(y = 'Rn', x = 'Cycle', color = 'Target')\n\n\n\nQubit measurements\n\n\nurl <- 'https://docs.google.com/spreadsheets/d/1OUp0DBVb3QOrELZ32-4C6Ia3QH1x_G95nCJFW2h8I4k'\nqubit <- googlesheets4::read_sheet(url, col_types = 'ccnccc') %>%\n  janitor::clean_names() %>%\n  rename(\n    sewer_system = system,\n    amicon_mwco = mwco,\n    dissoc_treatment = treatment,\n    qubit_rna_raw = qubit_hs_rna_ng_ul,\n    qubit_dna_raw = qubit_br_dna_ng_ul,\n  ) %>%\n  glimpse\n\nRows: 16\nColumns: 6\n$ sample           <chr> \"N_30_Tween\", \"N_30_None\", \"N_30_NaCl\", \"N_…\n$ sewer_system     <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"S\"…\n$ amicon_mwco      <dbl> 30, 30, 30, 30, 100, 100, 100, 100, 30, 30,…\n$ dissoc_treatment <chr> \"Tween\", \"None\", \"NaCl\", \"BE\", \"Tween\", \"No…\n$ qubit_rna_raw    <chr> \"0.52\", \"0.284\", \"0.24\", \"OOR (>10 ng/uL)\",…\n$ qubit_dna_raw    <chr> \"OOR (<4 ng/uL)\", \"OOR (<4 ng/uL)\", \"OOR (<…\n\nqubit_raw <- qubit\n\n\n\n\nqubit <- qubit_raw %>%\n  mutate(\n    across(matches('qubit_(rna|dna)_raw'), \n      ~ifelse(str_detect(.x, 'OOR'), NA, .x) %>% as.numeric,\n      .names = \"{str_replace(.col, '_raw', '')}\"\n    ),\n    qubit_rna_adj = case_when(\n      qubit_rna_raw == 'OOR (>10 ng/uL)' ~ 10,\n      qubit_rna_raw == 'OOR (<0.2 ng/uL)' ~ 0.2,\n      TRUE ~ qubit_rna\n    ),\n    qubit_dna_adj = case_when(\n      qubit_dna_raw == 'OOR (<4 ng/uL)' ~ 4,\n      TRUE ~ qubit_dna\n    ),\n    across(amicon_mwco, as.factor),\n    across(dissoc_treatment, ~fct_relevel(.x, 'None')),\n  ) %>%\n  glimpse\n\nRows: 16\nColumns: 10\n$ sample           <chr> \"N_30_Tween\", \"N_30_None\", \"N_30_NaCl\", \"N_…\n$ sewer_system     <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"S\"…\n$ amicon_mwco      <fct> 30, 30, 30, 30, 100, 100, 100, 100, 30, 30,…\n$ dissoc_treatment <fct> Tween, None, NaCl, BE, Tween, None, NaCl, B…\n$ qubit_rna_raw    <chr> \"0.52\", \"0.284\", \"0.24\", \"OOR (>10 ng/uL)\",…\n$ qubit_dna_raw    <chr> \"OOR (<4 ng/uL)\", \"OOR (<4 ng/uL)\", \"OOR (<…\n$ qubit_rna        <dbl> 0.520, 0.284, 0.240, NA, 3.530, 1.300, 0.51…\n$ qubit_dna        <dbl> NA, NA, NA, 146, NA, NA, NA, 333, NA, NA, N…\n$ qubit_rna_adj    <dbl> 0.520, 0.284, 0.240, 10.000, 3.530, 1.300, …\n$ qubit_dna_adj    <dbl> 4, 4, 4, 146, 4, 4, 4, 333, 4, 4, 4, 188, 4…\n\n\n\nqubit %>%\n  select(sewer_system:dissoc_treatment, qubit_rna_raw, qubit_dna_raw) %>%\n  knitr::kable()\n\nsewer_system\namicon_mwco\ndissoc_treatment\nqubit_rna_raw\nqubit_dna_raw\nN\n30\nTween\n0.52\nOOR (<4 ng/uL)\nN\n30\nNone\n0.284\nOOR (<4 ng/uL)\nN\n30\nNaCl\n0.24\nOOR (<4 ng/uL)\nN\n30\nBE\nOOR (>10 ng/uL)\n146\nN\n100\nTween\n3.53\nOOR (<4 ng/uL)\nN\n100\nNone\n1.3\nOOR (<4 ng/uL)\nN\n100\nNaCl\n0.511\nOOR (<4 ng/uL)\nN\n100\nBE\nOOR (>10 ng/uL)\n333\nS\n30\nTween\n1.97\nOOR (<4 ng/uL)\nS\n30\nNone\n2.73\nOOR (<4 ng/uL)\nS\n30\nNaCl\nOOR (<0.2 ng/uL)\nOOR (<4 ng/uL)\nS\n30\nBE\nOOR (>10 ng/uL)\n188\nS\n100\nTween\n5.5\nOOR (<4 ng/uL)\nS\n100\nNone\n1.2\nOOR (<4 ng/uL)\nS\n100\nNaCl\n0.351\nOOR (<4 ng/uL)\nS\n100\nBE\nOOR (>10 ng/uL)\n379\n\n\n\nqubit %>%\n  select(sample:dissoc_treatment, matches('qubit_(rna|dna)_adj')) %>%\n  pivot_longer(matches('qubit_(rna|dna)_adj')) %>%\n  ggplot(aes(x = value, y = amicon_mwco, color = dissoc_treatment)) +\n  # theme_minimal_vgrid() +\n  theme(\n    legend.position = 'bottom',\n    panel.spacing = unit(1, 'char'),\n  ) +\n  labs(\n    y = 'Amicon MWCO (kDa)',\n    x = 'Concentration (ng/uL)',\n    color = 'Dissoc. treatment',\n  ) +\n  facet_grid(sewer_system ~ name, scales = 'free') +\n  scale_color_manual(values = colors_oi %>% unname) +\n  # geom_vline(xintercept = 40, color = 'grey', size = 0.3) + \n  scale_x_log10() +\n  # expand_limits(x = 0.1) +\n  geom_point()\n\n\n\nWe see high (out of range) values for the BE-treated samples, which may be caused by bovine NA in the beef extract.\nWe also see that the DNA measurements can’t distinguish the other treatments, due to the relatively high lower limit of the BR assay that was used.\nCompared to our 2022 tests using DNA extractions, we have concentrated more wastewater than in our previous experiments (40 mL instead of 10 mL per sample).\nThose extractions used a different extraction kit.\nI would need to revisit those results to see if I’d expect to be seeing <4 ng/uL.\nLet’s look at the RNA measurements of the non-BE treatments, where the Qubit results are most informative.\nNote that I have set the measurement that is OOR (below lower limit) to the lower limit value of 0.2.\n\n\nqubit %>%\n  filter(dissoc_treatment != 'BE') %>%\n  ggplot(aes(x = qubit_rna_adj, y = amicon_mwco, color = dissoc_treatment)) +\n  # theme_minimal_vgrid() +\n  theme(\n    legend.position = 'bottom',\n    panel.spacing = unit(1, 'char'),\n  ) +\n  labs(\n    y = 'Amicon MWCO (kDa)',\n    x = 'Concentration (ng/uL)',\n    color = 'Dissoc. treatment',\n  ) +\n  facet_grid(sewer_system ~ ., scales = 'free') +\n  scale_x_log10() +\n  # expand_limits(x = 0.1) +\n  scale_color_manual(values = colors_oi %>% unname) +\n  geom_vline(xintercept = 0.2, color = 'grey', size = 0.3) + \n  geom_point()\n\n\n\nThis evidence suggests that the NaCl treatment tends to give lower yields than the other two treatments, and that the Tween treatment tends to give higher yields than the None treatment, though the evidence is not that strong (particularly for the advantage of Tween over None) due to limited sample size.\nLinear modeling\nThis was a warm-up exercise I did before looking at data carefully and making the above exploratory plots.\nCare should be taken in interpretting these results, including my quick attempts at interpreting them below.\nFit linear model to assess the effect of conditions on the Cq value of a single target.\n\n\nfits <- results_main  %>%\n  summarize(\n    .by = c(sample_experiment, dir_target, sewer_system, amicon_mwco, dissoc_treatment),\n    across(cq, mean)\n  ) %>%\n  nest(.by = dir_target) %>%\n  mutate(.keep = 'unused',\n    fit = map(data, ~lm(data = .x, cq ~ sewer_system + amicon_mwco + dissoc_treatment)),\n    # fit_tidy = map(fit, broom::tidy)\n  )\nfits_summary <- fits %>%\n  mutate(.keep = 'unused',\n    tidy = map(fit, broom::tidy)\n  ) %>%\n  unnest(tidy)\n\n\nImpact of dissoc treatment:\n\n\nfits_summary %>%\n  filter(str_detect(term, 'dissoc')) %>%\n  select(dir_target, term, estimate, std.error, p.value) \n\n# A tibble: 12 × 5\n   dir_target term                  estimate std.error    p.value\n   <chr>      <chr>                    <dbl>     <dbl>      <dbl>\n 1 16S        dissoc_treatmentBE       7.23      1.52  0.000787  \n 2 16S        dissoc_treatmentNaCl    -0.255     1.52  0.870     \n 3 16S        dissoc_treatmentTween   -1.25      1.52  0.429     \n 4 Crassphage dissoc_treatmentBE       2.93      3.24  0.386     \n 5 Crassphage dissoc_treatmentNaCl    -0.307     3.24  0.926     \n 6 Crassphage dissoc_treatmentTween   -1.44      3.24  0.666     \n 7 Phagemid   dissoc_treatmentBE       0.491     0.639 0.460     \n 8 Phagemid   dissoc_treatmentNaCl     0.698     0.639 0.300     \n 9 Phagemid   dissoc_treatmentTween   -0.192     0.639 0.770     \n10 SARS-CoV-2 dissoc_treatmentBE       5.73      0.611 0.00000285\n11 SARS-CoV-2 dissoc_treatmentNaCl    -0.119     0.611 0.850     \n12 SARS-CoV-2 dissoc_treatmentTween   -1.10      0.611 0.101     \n\nBE caused bacterial 16S to decrease by a substantial factor (increase in Cq) relative to the other dissociation treatments\nBE had an uncertain effect on the phage and phagemid; but smaller than the effect on bacteria. TODO: Look into why no est for SARS2.\nThus it appears that BE helps by reducing bacteria. I don’t think we expected this result, and I’d want to understand it better.\nImpact of Amicon filter type:\n\n\nfits_summary %>%\n  filter(str_detect(term, 'amicon')) %>%\n  select(dir_target, term, estimate, std.error, p.value) \n\n# A tibble: 4 × 5\n  dir_target term           estimate std.error p.value\n  <chr>      <chr>             <dbl>     <dbl>   <dbl>\n1 16S        amicon_mwco100 -0.400       1.08    0.718\n2 Crassphage amicon_mwco100 -1.79        2.29    0.453\n3 Phagemid   amicon_mwco100  0.00536     0.452   0.991\n4 SARS-CoV-2 amicon_mwco100  0.491       0.432   0.283\n\nNo clear effect (large standard errors relative to average effect).\nFor viruses except SASR2, data consistent with effects on the order of roughly 2-4-fold in either direction.\nFor SARS2, data consistent with a 1-4-fold decrease with a mean est of 2-fold decrease.\nNow, fit model to lo at cq differences of viruses relative to bacteria\nSession info\n\nClick for session info\n\n\nsessioninfo::session_info()\n\n─ Session info ─────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21)\n os       Arch Linux\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-06-24\n pandoc   3.0.1 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ─────────────────────────────────────────────────────────────\n package       * version date (UTC) lib source\n askpass         1.1     2019-01-13 [1] CRAN (R 4.0.0)\n backports       1.4.1   2021-12-13 [1] CRAN (R 4.1.2)\n beeswarm        0.4.0   2021-06-01 [1] CRAN (R 4.1.0)\n bit             4.0.5   2022-11-15 [1] CRAN (R 4.3.0)\n bit64           4.0.5   2020-08-30 [1] CRAN (R 4.0.2)\n bookdown        0.34    2023-05-09 [1] CRAN (R 4.3.0)\n broom         * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n bslib           0.5.0   2023-06-09 [1] CRAN (R 4.3.0)\n cachem          1.0.8   2023-05-01 [1] CRAN (R 4.3.0)\n cellranger      1.1.0   2016-07-27 [1] CRAN (R 4.0.0)\n cli             3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools       0.2-19  2023-02-01 [2] CRAN (R 4.3.0)\n colorspace      2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n cowplot       * 1.1.1   2021-08-27 [1] Github (wilkelab/cowplot@555c9ae)\n crayon          1.5.2   2022-09-29 [1] CRAN (R 4.2.1)\n curl            5.0.1   2023-06-07 [1] CRAN (R 4.3.0)\n digest          0.6.31  2022-12-11 [1] CRAN (R 4.3.0)\n distill         1.5.2   2022-11-10 [1] Github (rstudio/distill@9c1a1a2)\n downlit         0.4.2   2022-07-05 [1] CRAN (R 4.2.1)\n dplyr         * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n evaluate        0.21    2023-05-05 [1] CRAN (R 4.3.0)\n fansi           1.0.4   2023-01-22 [1] CRAN (R 4.3.0)\n farver          2.1.1   2022-07-06 [1] CRAN (R 4.2.1)\n fastmap         1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n fontawesome     0.5.1   2023-04-18 [1] CRAN (R 4.3.0)\n forcats       * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            * 1.6.2   2023-04-25 [1] CRAN (R 4.3.0)\n gargle          1.5.0   2023-06-10 [1] CRAN (R 4.3.0)\n generics        0.1.3   2022-07-05 [1] CRAN (R 4.2.1)\n ggbeeswarm    * 0.7.2   2023-04-29 [1] CRAN (R 4.3.0)\n ggplot2       * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue            1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n googledrive     2.1.1   2023-06-11 [1] CRAN (R 4.3.0)\n googlesheets4   1.1.1   2023-06-11 [1] CRAN (R 4.3.0)\n gtable          0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n here          * 1.0.1   2020-12-13 [1] CRAN (R 4.0.5)\n highr           0.10    2022-12-22 [1] CRAN (R 4.3.0)\n hms             1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools       0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n httr            1.4.6   2023-05-08 [1] CRAN (R 4.3.0)\n janitor         2.2.0   2023-02-02 [1] CRAN (R 4.3.0)\n jquerylib       0.1.4   2021-04-26 [1] CRAN (R 4.1.0)\n jsonlite        1.8.5   2023-06-05 [1] CRAN (R 4.3.0)\n knitr         * 1.43    2023-05-25 [1] CRAN (R 4.3.0)\n labeling        0.4.2   2020-10-20 [1] CRAN (R 4.0.3)\n lifecycle       1.0.3   2022-10-07 [1] CRAN (R 4.2.1)\n lubridate     * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr        2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n memoise         2.0.1   2021-11-26 [1] CRAN (R 4.1.2)\n munsell         0.5.0   2018-06-12 [1] CRAN (R 4.0.0)\n nvimcom       * 0.9-144 2023-05-01 [1] local\n openssl         2.0.6   2023-03-09 [1] CRAN (R 4.2.3)\n patchwork     * 1.1.2   2022-08-19 [1] CRAN (R 4.2.1)\n pillar          1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig       2.0.3   2019-09-22 [1] CRAN (R 4.0.0)\n purrr         * 1.0.1   2023-01-10 [1] CRAN (R 4.2.3)\n R6              2.5.1   2021-08-19 [1] CRAN (R 4.1.1)\n rappdirs        0.3.3   2021-01-31 [1] CRAN (R 4.0.4)\n readr         * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n rlang           1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     * 2.22    2023-06-01 [1] CRAN (R 4.3.0)\n rprojroot       2.0.3   2022-04-02 [1] CRAN (R 4.2.2)\n rstudioapi      0.14    2022-08-22 [1] CRAN (R 4.2.1)\n sass            0.4.6   2023-05-03 [1] CRAN (R 4.3.0)\n scales          1.2.1   2022-08-20 [1] CRAN (R 4.2.1)\n sessioninfo     1.2.2   2021-12-06 [1] CRAN (R 4.1.2)\n snakecase       0.11.0  2019-05-25 [1] CRAN (R 4.0.0)\n stringi         1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr       * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble        * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr         * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect      1.2.0   2022-10-10 [1] CRAN (R 4.2.1)\n tidyverse     * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n timechange      0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb            0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n utf8            1.2.3   2023-01-31 [1] CRAN (R 4.3.0)\n vctrs           0.6.2   2023-04-19 [1] CRAN (R 4.3.0)\n vipor           0.4.5   2017-03-22 [1] CRAN (R 4.0.0)\n vroom           1.6.3   2023-04-28 [1] CRAN (R 4.3.0)\n withr           2.5.0   2022-03-03 [1] CRAN (R 4.2.0)\n xfun            0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xml2            1.3.4   2023-04-27 [1] CRAN (R 4.3.0)\n yaml            2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] /home/michael/.local/lib/R/library\n [2] /usr/lib/R/library\n\n────────────────────────────────────────────────────────────────────────\n\n\n\n\n",
    "preview": "posts/2023-06-17-amicon-mwcos-and-dissoc-treatments/main_files/figure-html5/unnamed-chunk-13-1.svg",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-14-conceicaoneto2015modu-reanalysis/",
    "title": "Reanalysis of Conceição-Neto et al 2015 protocol testing",
    "description": {},
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-06-17",
    "categories": [
      "R",
      "MGS protocols"
    ],
    "contents": "\n\nContents\nData import\nPlots\n\nData from Conceição-Neto et al. (2015)\nR setup\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(here)\n# library(speedyseq)\n# library(furrr)\n# plan(multisession, workers = 3)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n# Okabe Ito color scheme with amber for yellow; see https://easystats.github.io/see/reference/scale_color_okabeito.html\ncolors_oi <- grDevices::palette.colors()  \ncolors_oi['yellow'] <- \"#F5C710\"\n\n# today <- format(Sys.time(), '%Y-%m-%d')\n\n\nData import\nImport the MGS read count data from Table S4 from the supplementary pdf file,\n\n\nShow code\n\n# library(tabulizer)\nsupplement_path <- '~/Zotero/storage/B867AJV4/Conceição-Neto et al. - 2015 - supplement.pdf'\nx <- tabulizer::extract_tables(supplement_path, pages = 4, method = 'lattice')\nx <- x[[1]]\nx[1,1] <- 'taxon'\n# x <- tabulizer::extract_areas(supplement_path, pages = 4)\n\nnms <- x[1,]\nx <- x %>%\n  tail(-1) %>%\n  head(-1) %>%\n  as_tibble()\nnames(x) <- nms\nx\n\n# A tibble: 15 × 9\n   taxon             Control `0.8 PC` `0.8 PES` `0.45` `0.22` `17000g`\n   <chr>             <chr>   <chr>    <chr>     <chr>  <chr>  <chr>   \n 1 \"Circovirus\"      114327… 105066 … 467836 (… 28819… 37063… 162676 …\n 2 \"Parvovirus\"      902175… 929352 … 2304080 … 16456… 31897… 1008512…\n 3 \"Polyomavirus\"    130761… 72391 (… 88285 (0… 29444… 11187… 47493 (…\n 4 \"Pepino mosaic\\r… 146832… 639869 … 2660872 … 16232… 22791… 3710870…\n 5 \"Rotavirus\"       113219… 535543 … 1751352 … 10783… 97106… 2619165…\n 6 \"Coronavirus\"     44067 … 15438 (… 79082 (0… 38486… 40884… 104041 …\n 7 \"LIMEstone virus\" 960756… 2125479… 4860630 … 52520… 66120… 3024671…\n 8 \"Herpesvirus\"     11958 … 4007 (0… 32653 (0… 25407… 23196… 7903 (0…\n 9 \"Mimivirus\"       103091… 211129 … 3711 (0.… 2809 … 5240 … 3133 (0…\n10 \"Bacteroides rRN… 179167… 18923 (… 1694 (0.… 966 (… 559 (… 1026 (0…\n11 \"Bacteroides\"     465543… 4172023… 34282 (0… 27823… 26408… 12152 (…\n12 \"Bifidobacterium\" 38488 … 2474 (0… 2748 (0.… 2935 … 2508 … 988 (0.…\n13 \"Lactobacillus\"   107010… 5289 (0… 5170 (0.… 4693 … 5894 … 3957 (0…\n14 \"E.coli\"          572592… 357029 … 51191 (0… 44481… 55943… 29121 (…\n15 \"unmapped\"        859446… 403677 … 1152082 … 91492… 16292… 998115 …\n# ℹ 2 more variables: `17000g+0.8 PES` <chr>, `17000g+0.45` <chr>\n\nShow code\n\nx0 <- x\n# Parse\nx <- x0 %>%\n  pivot_longer(-taxon, names_to = 'sample') %>%\n  separate(value, into = c('reads', 'percent'), sep = '\\\\s+') %>%\n  mutate(\n    across(reads, as.numeric),\n    across(percent, ~str_extract(.x, '\\\\d+\\\\.\\\\d+')),\n    across(taxon, ~str_replace(.x, '\\\\r', ' ')),\n    taxon_type = case_when(\n      taxon == 'unmapped' ~ 'unmapped',\n      str_detect(taxon, 'virus') ~ 'virus',\n      TRUE ~ 'bacteria'\n    ),\n    filter = str_extract(sample, '0.22|0.45|0.8 PES|0.8 PC') %>%\n      fct_explicit_na('None'),\n    centrifuge = str_detect(sample, '17000g')\n  ) %>%\n  glimpse\n\nRows: 120\nColumns: 7\n$ taxon      <chr> \"Circovirus\", \"Circovirus\", \"Circovirus\", \"Circov…\n$ sample     <chr> \"Control\", \"0.8 PC\", \"0.8 PES\", \"0.45\", \"0.22\", \"…\n$ reads      <dbl> 114327, 105066, 467836, 288199, 370639, 162676, 1…\n$ percent    <chr> \"1.01\", \"1.09\", \"3.47\", \"2.56\", \"2.42\", \"1.39\", \"…\n$ taxon_type <chr> \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"vir…\n$ filter     <fct> None, 0.8 PC, 0.8 PES, 0.45, 0.22, None, 0.8 PES,…\n$ centrifuge <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TR…\n\nPlots\n\n\nShow code\n\ngm_mean <- function(x) exp(mean(log(x)))\ncenter_elts <- function(x) x / gm_mean(x)\n\n\nRelative abundances as centered ratios (abundance of taxon / gm mean of abundances of all taxa in that sample)\n\n\nShow code\n\nx %>%\n  filter(taxon != 'unmapped') %>%\n  mutate(.by = sample, value = center_elts(reads)) %>%\n  ggplot(aes(x = taxon, y = value, color = sample)) +\n  scale_color_manual(values = colors_oi %>% unname) +\n  facet_grid(~taxon_type, scales = 'free_x', space = 'free') +\n  scale_y_log10() +\n  geom_point() +\n  # rotate x-axis labels\n  labs(y = 'Centered ratio') +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\nRelative abundances doubly centered: First, centered within samples as above. Then, center the taxon abundance across samples.\nThis view helps to focus on the effect of protocols by adjusting for the difference in abundance among taxa.\n\n\nShow code\n\nx %>%\n  filter(taxon != 'unmapped') %>%\n  mutate(.by = sample, value = center_elts(reads)) %>%\n  mutate(.by = taxon, value = center_elts(value)) %>%\n  ggplot(aes(x = taxon, y = value, color = sample)) +\n  scale_color_manual(values = colors_oi %>% unname) +\n  facet_grid(~taxon_type, scales = 'free_x', space = 'free') +\n  scale_y_log10() +\n  geom_point() +\n  labs(y = 'Doubly-centered ratio') +\n  # rotate x-axis labels\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\nLet’s look at the gm-average relative abundance of viruses to bacteria across samples,\n\n\nShow code\n\nratios <- x %>%\n  summarize(.by = c(sample, taxon_type), value = gm_mean(reads)) %>%\n  select(taxon_type, sample, value) %>%\n  pivot_wider(names_from = taxon_type, values_from = value) %>%\n  mutate(ratio = virus / bacteria)\n\n\n\n\nShow code\n\np1 <- ratios %>%\n  ggplot(aes(x = ratio, y = sample)) +\n  scale_x_log10() +\n  geom_point()\np1\n\n\n\nThis figure suggests that the .45 filter and the 0.8 PES filter perform similarly.\nInteresting that the .8 PC ratio is so much lower than the other filters.\nWhat if we don’t care about mimivirus?\nDoes the advantage of 0.8PES over .45 with centrifugation still hold?\n\n\nShow code\n\nratios_no_mimi <- x %>%\n  filter(taxon != 'Mimivirus') %>%\n  summarize(.by = c(sample, taxon_type), value = gm_mean(reads)) %>%\n  select(taxon_type, sample, value) %>%\n  pivot_wider(names_from = taxon_type, values_from = value) %>%\n  mutate(ratio = virus / bacteria)\n\n\n\n\nShow code\n\np2 <- ratios_no_mimi %>%\n  ggplot(aes(x = ratio, y = sample)) +\n  scale_x_log10() +\n  geom_point()\n# p1 / p2\np2\n\n\n\nThe modest advantage remains and overall everything looks very similar.\nMight be useful to look at the variance at the level of viruses.\nCould normalize everything relative to the GM of bacteria, and quantify the increase relative to the control.\nDoing this could give us some sense of variability, using variation among viruses as a proxy for variation among samples.\nIt also let’s us look for the effect on larger viruses.\nHere, I’ll look at viral abundance as the ratio of that virus relative to the gm-average of bacterial species.\n\n\nShow code\n\nratios_spp <- x %>%\n  mutate(.by = sample,\n    ref = reads[taxon_type == 'bacteria'] %>% gm_mean,\n    value = reads / ref\n  ) %>%\n  filter(taxon_type == 'virus')\navg <- ratios_spp %>%\n  summarize(.by = sample, across(value, gm_mean))\n\n\n\n\nShow code\n\nratios_spp %>%\n  ggplot(aes(x = value, y = sample, color = taxon)) +\n  scale_color_brewer(type = 'qual', palette = 3) +\n  scale_x_log10() +\n  # geom_point() +\n  geom_quasirandom(width = 0.1) +\n  geom_point(data = avg, color = 'black', shape = 3)\n\n\n\n\n\nShow code\n\navg <- ratios_spp %>%\n  filter(filter %in% c('0.45', '0.8 PES')) %>%\n  summarize(.by = c(sample, filter, centrifuge), across(value, gm_mean))\nratios_spp %>%\n  filter(filter %in% c('0.45', '0.8 PES')) %>%\n  ggplot(aes(x = value, y = sample, color = taxon)) +\n  facet_wrap(~centrifuge, ncol = 1, scales = 'free_y', \n             labeller = 'label_both') +\n  scale_color_brewer(type = 'qual', palette = 3) +\n  scale_x_log10() +\n  geom_quasirandom(width = 0.1) +\n  geom_point(data = avg, color = 'black', shape = 3)\n\n\n\nWhat is the magnitude of increase due to 0.8PES versus 0.45?\n\n\nShow code\n\nratios_spp_filter <- ratios_spp %>%\n  filter(filter %in% c('0.45', '0.8 PES')) %>%\n  select(taxon, taxon_type, centrifuge, filter, value) %>%\n  pivot_wider(names_from = filter) %>%\n  mutate(ratio = `0.8 PES` / `0.45`) %>%\n  glimpse\n\nRows: 18\nColumns: 6\n$ taxon      <chr> \"Circovirus\", \"Circovirus\", \"Parvovirus\", \"Parvov…\n$ taxon_type <chr> \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"vir…\n$ centrifuge <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRU…\n$ `0.8 PES`  <dbl> 55.5849922, 66.2281995, 273.7546254, 373.9504363,…\n$ `0.45`     <dbl> 41.3399182, 55.6332580, 236.0586371, 256.1308699,…\n$ ratio      <dbl> 1.3445840, 1.1904426, 1.1596891, 1.4599975, 0.248…\n\n\n\nShow code\n\nratios_spp_filter %>%\n  ggplot(aes(x = ratio, y = taxon, color = centrifuge)) +\n  theme_minimal_grid() +\n  # facet_wrap(~centrifuge, ncol = 1, scales = 'free_y') +\n  scale_color_brewer(type = 'qual', palette = 2) +\n  scale_x_log10(breaks = c(0.3, 1, 1.5, 3)) +\n  expand_limits(x = 3) +\n  # geom_vline(xintercept=1.4, color = 'darkred', size = 0.2) +\n  geom_point() +\n  labs(\n    x = 'Ratio of virus to gm-mean of all bacteria'\n  ) +\n  plot_annotation(\n    title = 'Advantage of 0.8 PES over 0.45, with and without centrifugation',\n  )\n\n\n\nWe’re generally talking about a ~1.4-fold difference, not something huge.\nI’m not sure what is going on with the Polyomavirus without centrifugation, but without replication we shouldn’t treat it too seriously.\n\n\n\nConceição-Neto, Nádia, Mark Zeller, Hanne Lefrère, Pieter De Bruyn, Leen Beller, Ward Deboutte, Claude Kwe Yinda, et al. 2015. “Modular Approach to Customise Sample Preparation Procedures for Viral Metagenomics: A Reproducible Protocol for Virome Analysis.” Scientific Reports 5 (1): 16532. https://doi.org/10.1038/srep16532.\n\n\n\n\n",
    "preview": "posts/2023-06-14-conceicaoneto2015modu-reanalysis/main_files/figure-html5/unnamed-chunk-7-1.svg",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-06-basic-detection-theory/",
    "title": "WWTP basic detection theory",
    "description": "Notes exploring the factors that determine our ability to perform _basic detection_ in a WWTP setting. \nBy basic detection, I mean detection based on seeing a sufficient number of distinguishing reads from the pathogen, rather than from a spatiotemporal pattern such as exponential growth.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-06-03",
    "categories": [
      "theory"
    ],
    "contents": "\n\nContents\nDetection under deterministic exponential growth\nEpidemic dynamics\nShedding and sequencing\nDetection\nCumulative reads over a given period\n\nFurther discussion\n\nApplications\nExample: SARS2 reads in Southern California WWTP data\n\nDetails and complications\nComplex disease time courses\nSEIR model\n(Random) variation in transmission\nRelevant references\n\nOrigination of the outbreak\nShedding\nTransport\nEffect of transport time\nChanges in waste composition during transport\n\nSample collection\nPartitioning into solid and liquid fractions\n\nDetection methods\nMetagenomic sequencing\nTargeted detection using qPCR or dPCR\nAmplicon sequencing\n\n\nNotes\nTransportation time\nEffective number of shedders contributing to a WWTP sample\nNon-human contributions to WWTP samples\nNoise/variability in the model\n\n\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(here)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n# Okabe Ito color scheme with amber for yellow; see https://easystats.github.io/see/reference/scale_color_okabeito.html\ncolors_oi <- grDevices::palette.colors()  \ncolors_oi['yellow'] <- \"#F5C710\"\n\n\n\nDetection under deterministic exponential growth\nI will assume a very simple model, but which can represent more complex cases so long as the outbreak has has established, and reached the asymptotic exponential growth rate in the number of infections, cumulative infections, and shedding amount, by using ‘effective parameters’.\nMotivation: Mathematical simplicity for developing intuition; also, I assume that detection has a negligible chance of occurring before the outbreak reaches the exponential phase.\nCurrently considers MGS-based detection only; future versions may add targeted detection and amplicon sequencing.\nDescription of the basic model:\nDeterministic exponential growth in the number of infected individuals.\nIntended to model the early stage of an outbreak where less than \\(\\sim 1/3\\) of the population have been infected.\n\nThe ratios of infected to infectious and to the total shedding rate assumed constant.\nIndividual variation and randomness in disease time course and shedding are treated as affecting these ratios, and otherwise ignored.\n\nMicrobiome background shedding assumed constant and independent of infection status.\nMGS results only depend on the relative abundances in the sample.\nEffect of WW transport, sample collection, and sample processing on relative abundance of pathogen to background can be represented by a constant bias parameter.\nVariability/noise in MGS read counts due to transport, sample collection, processing, and sequencing is ignored.\nIn this section, we will use this basic model to derive the time, current infected fraction, and cumulative incidence when \\(M^*\\) reads of a pathogen (or a particular k-mer in the pathogen’s genome) are first seen on a single day or cumulatively.\nTODO\nabove, better distinguish between motivation/justification of the model and statement of the model.\nabove, better describe the purpose/message of this section\nbelow, review and consider which details to move to later sections or an appendix\nEpidemic dynamics\nNotation warning: Here, I’m taking \\(I\\) to be the number of infections, which includes exposed but non-yet-infectious individuals.\nIn other words, this would be \\(E + I\\) in the SEIR model.\nAssume deterministic exponential growth of the current number of infected, \\(I(t)\\), from some initial number \\(I_0 = I(0)\\).\nThe number of infected at time \\(t\\) (measured in days) has derivative\n\\[\\begin{align}\n  \\frac{dI}{dt} &= (b - d) I = r I,\n\\end{align}\\]\nwhere \\(r > 0\\) is the exponential growth rate.\nI further assume that \\(r\\) can be broken into separate ‘birth’ and ‘death’ rates, \\(b\\) and \\(d\\), describing the rate at which infections beget new infections, and which infected individuals recover.\nUnder this model, the number of infections at time \\(t\\) grows as\n\\[\\begin{align}\n  I(t) &\\approx I_0 e^{rt}.\n\\end{align}\\]\nThe cumulative number of infection-days at day \\(t\\) is\n\\[\\begin{align}\n  \\int_0^t I(u)\\;du\n    &= \\int_0^t I_0 e^{ru}\\;du\n  \\\\&= \\frac{I_0}{r} \\left(e^{rt} - 1 \\right)\n  \\\\&= \\frac{1}{r} \\left[I(t) - I(0) \\right]\n  \\\\&\\approx \\frac{1}{r} I(t) \\quad \\text{for $t \\gg 1/r$}.\n\\end{align}\\]\nWe can calculate the cumulative number of infections \\(C(t)\\) at day \\(t\\) by noting that new infections occur at rate \\(b I(t)\\); hence\n\\[\\begin{align}\n  C(t)\n    &= \\int_0^t b I(u)\\;du\n  \\\\&= \\int_0^t b I_0 e^{ru}\\;du\n  \\\\&= \\frac{b I_0}{r} \\left(e^{rt} - 1 \\right)\n  \\\\&= \\frac{b}{r} \\left[I(t) - I(0) \\right]\n  \\\\&\\approx \\frac{b}{r} I(t) \\quad \\text{for $t \\gg 1/r$}.\n\\end{align}\\]\nThe cumulative number of infections is \\(b\\) times the cumulative number of infection-days, and is approximately \\(b/r\\) the number of current infections.\nNote: Arguably, we should be adding the initial number of infected, \\(I_0\\), in which case we get\n\\[\\begin{align}\n  C(t)\n    &= \\int_0^t b I(u)\\;du  + I_0\n  \\\\&= \\frac{b}{r} \\left[I(t) - I_0 \\right] + I_0\n  \\\\&= \\frac{b}{r} I(t) - \\frac{d}{r} I_0.\n  \\\\&\\approx \\frac{b}{r} I(t) \\quad \\text{for $I(t) \\gg I_0$}.\n\\end{align}\\]\nI will typically assume \\(I_0\\) is a tiny fraction of all current and cumulative infections in the regimes of interest, so this distinction shouldn’t matter.\nThe condition \\(I(t) \\gg I_0\\) will be true for \\(t \\gg 1/r\\).\nDefine cumulative incidence \\(c(t)\\) as the fraction of the population that have been infected by time \\(t\\),\n\\[\\begin{align}\n  c(t)\n    &= C(t) / N\n  \\\\&\\approx \\frac{b}{r} i(t) \\quad \\text{for $t \\gg 1/r$}.\n\\end{align}\\]\nHow long is the exponential approximation good for?\nSuppose there is lasting immunity, such that each person can only be infected once.\nIn that case, we expect the rate of new infections per capita, \\(b\\), to be in proportional to \\(1 - c(t)\\); i.e., \\(b(t) = [1 - c(t)] b_0\\).\nThe exponential approximation is good so long as \\(c(t) \\ll 1\\).\nA good rule of thumb could be to take the point when \\(c(t) \\approx 1/e\\) as the end of the exponential phase.\nThis occurs when the currently infected fraction of the population \\(i(t)\\) reaches \\(\\tfrac{r}{b} e^{-1}\\), or when \\(t \\approx \\tfrac{1}{r} [\\log (\\tfrac{r}{i_0 b}) - 1]\\).\n(Needs to be checked)\nThe peak of the epidemic (max value of \\(I\\)) will occur when \\(b\\) decreases to \\(d\\).\nThis occurs when \\(c(t) = 1 - \\tfrac{d}{b_0} = \\tfrac{r_0}{b_0}\\).\nThe number of susceptible (those not infected) is \\(1 - c = d/b_0\\), matching the standard SIR result (e.g. equation 4a here).\nShedding and sequencing\nSuppose the number of infected at time \\(t\\) is then\n\\[\\begin{align}\n  I(t) &\\approx I_0 e^{rt}.\n\\end{align}\\]\nwhere \\(I_0\\) is the initial number of infected.\nLet \\(N\\) be the total population (assumed fixed) and \\(i(t) = I(t) / N\\) be the fraction of the population that is infected.\nAssume a constant amount of background shedding in each person regardless of infection status.\nSuppose a background shedding rate of \\(s_0\\) of background microbiome in each person, regardless of infection status.\nLet \\(s\\) be the relative rate of shedding of pathogen to background in an infected person, so that \\(s s_0\\) is the rate of pathogen shedding in an infected person (note potential name clash with the fraction of the population that is susceptible).\nLet \\(B\\) be the relative measurement effiency of the pathogen to the background (MGS bias).\nIn the regime I consider below, where total threat reads are a small fraction of total reads, we can use \\(B\\) to also account for the fact that perhaps only a small fraction of reads from the pathogen are useful for detecting it as a novel threat.\nThe expected fraction (i.e. proportion) of the threat in the MGS reads in a sample collected on day \\(t\\) is\n\\[\\begin{align}\nE [ P(t) ]\n  &\\approx \\frac{I(t) s s_0 B}{I(t) s s_0  B + N s_0 }\n\\\\&= \\frac{I(t) s B}{I(t) s B + N }\n\\\\&= \\frac{i(t) s B}{i(t) s B + 1 }\n\\\\&\\approx i(t) s B.\n\\end{align}\\]\nThe first approximation comes from treating \\(I(t)\\) as constant over the 24h the sample is collected (more on this below).\nThe second approximation comes from assuming that the pathogen is always a small fraction of the reads (\\(i s B \\ll 1\\)).\nOne way for this condition to arise is that it holds even for a sample from an infected individual or an entirely infected population (i.e., \\(s B \\ll 1\\)).\nI expect that to be true for subtle pathogens, but not necessary all pathogens (e.g., gastroenteric pathogens).\nFor these latter cases I expect \\(i(t) \\ll 1\\) and for the broader condition \\(i s B \\ll 1\\) to still hold.\nTo determine the absolute copy number or concentration of the pathogen in the sample, we’d need to say something about the overall shedding amount and water system. However, to analyze MGS detection, it is enough to consider the relative abundance as above.\nThe expected number of sequencing reads of the threat on day \\(t\\) is\n\\[\\begin{align}\nE [ M(t) ] = E [ P(t) ] \\cdot \\mathcal M \\approx i(t) s B \\mathcal M.\n\\end{align}\\]\nwhere \\(\\mathcal M\\) is the total sequencing depth, which I’m treating as a value that is determined by the experimenter and is the same for each day.\nMore generally, if the sample is collected over a particular period of time \\(\\Delta t\\), we might want to model the pathogen contribution to the sample as an integral of shedding over that period,\n\\[\\begin{align}\nE [ M(t) ]\n  &= s B \\mathcal M \\int_{t-\\Delta t}{t} i(u) \\; du\n\\\\&= s B \\mathcal M \\frac{1}{r} i_0 e^{rt} \\left[1 - e^{-r \\Delta t} \\right]\n\\\\&= s B \\mathcal M \\frac{1}{r} i(t) \\left[1 - e^{-r \\Delta t} \\right].\n\\\\&\\approx i(t) (\\Delta t) s B \\mathcal M \\quad \\text{for $r \\Delta t \\ll 1$}.\n\\end{align}\\]\nDetection\nWarning: The below assumes that \\(r \\ll 1\\), so that we can ignore the growth in \\(i(t)\\) over the sampling period when computing the contribution of pathogen to the sample.\nSuppose there is a threshold number of reads (of the pathogen or the distinguishing subsequence), \\(M^*\\), we need to see in a single day to detect the threat (e.g. 5).\nI’ll take \\(\\Delta t = 1\\) (24-h composite sampling) and suppose that \\(r \\ll 1\\), so that we can ignore growth over the sampling window (otherwise, just multiply the number of expected reads by the constant factor \\((1 - e^r)/r\\)).\nAs a first approximation, let’s ignore noise in shedding and sequencing measurement, and define detection as occurring when \\(E [M(t^*)] = M^*\\).\nDetection then occurs at \\(i(t) s B \\mathcal M = M^*\\) or\n\\[\\begin{align}\n  i(t^*) = \\frac{M^*}{s B \\mathcal M}.\n\\end{align}\\]\nIn the exponential regime where \\(i(t) = i_0 e^{rt}\\) and cumulative incidence is \\(c(t) \\approx \\tfrac{b}{r} i(t)\\), we have that\n\\[\\begin{align}\n  t^* = \\frac{1}{r} \\log \\frac{M^*}{i_0 s B \\mathcal M}\n\\end{align}\\]\n\\[\\begin{align}\n  c(t^*) = \\frac{b}{r} \\cdot \\frac{M^*}{s B \\mathcal M}.\n\\end{align}\\]\nNote that the detection threshold and efficiency appear together as \\(M^* / B\\); this means we’ll get the same answer whether we think of\n\\(M^*\\) is the number of reads hitting a particular identifying subsequence, and \\(B\\) contains a factor \\(x < 1\\) to account for the fact that only a fraction \\(x\\) of the pathogen’s reads hit the subsequence\n\\(M^*\\) is the number of reads hitting the pathogen, and we increase it by a factor \\(1/x\\) to account for the fact that only \\(x\\) of the reads are useful for identification; \\(B\\) does not contain the factor \\(x\\).\nHowever, when we start to consider noise in the read counts, we need to be more careful about this.\nJoint dependence of many factors:\nThe effect of \\(s\\), \\(B\\), \\(M^*\\) and \\(\\mathcal M\\) all occurs through the term \\(\\frac{M^*}{s B \\mathcal M}\\).\n(This should match our preexisting intuition.)\nThus, for example, we can lower sequencing by 2-fold if we can enrich for the pathogen 2-fold (by, for exampling, doing a better job of removing bacterial NA for viral detection).\nEffect of sequencing effort:\nThese results explain the pattern seen in the example simulations presented by Charlie Whittaker last December, in which we saw that the time of detection decreases logarithmically \\(\\mathcal M\\) and cumulative incidence at time of detection decreases as \\(1 / \\mathcal M\\).\nAs per the previous point, we see the same behavior when we vary \\(s\\), \\(B\\), or \\(1/M^*\\).\nCumulative reads over a given period\nWhat if it is sufficient to see \\(R^*\\) reads cumulatively over the last \\(T\\) days?\nCall \\(Q\\) the cumulative reads over the last \\(T\\) days.\nThen\n\\[\\begin{align}\nE [ Q(t) ]\n  &= s B \\mathcal M \\int_{t-T}{t} i(u) \\; du\n\\\\&= s B \\mathcal M i(t) \\frac{1}{r} \\left[1 - e^{-r T} \\right].\n\\end{align}\\]\nSetting \\(T = t\\) corresponds to all reads since time \\(0\\); in that case, we have that \\(E [ Q(t) ]\\) equals \\(s B \\mathcal M\\) times the cumulative infection hours, or approximately\n\\[\\begin{align}\nE [ Q(t) ]\n  &\\approx s B \\mathcal M \\frac{i(t)}{r}\n  \\\\&\\approx s B \\mathcal M \\frac{c(t)}{b}\n\\end{align}\\]\nConsidering the factor \\(1 - e^{-rT}\\) indicates that it is only the samples from the past few characteristic growth periods that contributes significantly to the total reads.\nHow much better do we do, detection wise, when we only require cumulative reads to reach \\(M^*\\)?\nDetection now occurs at \\(E[Q(t)] = M^*\\) or\n\\[\\begin{align}\n  i(t^*) &= \\frac{r M^*}{s B \\mathcal M} \\\\\n  c(t^*) &= b \\cdot \\frac{M^*}{s B \\mathcal M};\n\\end{align}\\]\nthese are both a factor \\(r\\) greater than before.\nAs expected, the effect on time is only logarithmic,\n\\[\\begin{align}\n  t^* = \\frac{1}{r} \\log \\frac{r M^*}{i_0 s B \\mathcal M}.\n\\end{align}\\]\nThe effect overall is as if we increased any of \\(\\mathcal M\\), \\(s\\), or \\(B\\) by a factor of \\(1/r\\).\nFurther discussion\nMGS monitoring performance is independent of WWTP catchment size and flow rate\nWWTP catchment size and flow rate do not affect the quantities of interest at the time of detection.\nThis is because, in the model, the MGS reads are determined by the relative abundance of the pathogen in the sample, which is determined by the relative abundance of pathogen in the catchment; and we were interested in the prevalence (relative abundance of infected) in the catchment at time of detection, rather than absolute infection numbers.\nAlso, I assume detection occurs in a regime where stochastic effects from small infection numbers are negligible.\nThis would be very useful to know if true, since it would allow us to use estimates from a variety of WWTPs or other WW monitoring locations interchangeably.\nContrast with targeted detection, where the absolute concentration of the pathogen matters, and so total infected, flow rate, and effective concentration factor of the protocol, become relevant.\nSome caveats\nFor these results to hold, it is important that detection occurs in the exponential quasi-equilibrium phase. This implies that the population must be large enough and detection ability is not too high.\nI also assumed we can ignore noise from transmission and shedding; this similarly requires that there is enough absolute numbers of infections at the time of detection.\nApplications\nExample: SARS2 reads in Southern California WWTP data\nRothman et al. (2021) perform untargeted viral RNA sequencing of samples from several WWTPs in Southern California, with and without enrichment for specific viruses including SARS2.\nJeff previously estimated that the proportion of SARS2 reads in a WWTP sample measured without enrichment is 3e-7 when the daily incidence (fraction of the population infected in a day) is 1e-3.\nWhat can we say in general about the proportion of SARS2 reads at a given prevalence in the population?\nFirst, we need to connect the current daily incidence 1e-3 to the current prevalence.\nIn our baseline model, the new infections per day is approximately \\(b \\: i(t)\\), where \\(b\\) is the rate of new infections per infected and \\(i(t)\\) is the fraction of the population that is infected.\nWe can estimate \\(b\\) from the rough estimates of \\(R_0\\) and the (average) generation time compiled by Lenni last summer, where generation time is defined as\n\nGeneration time is a modelling term describing the time duration from the onset of infectiousness in a primary case to the onset of infectiousness in a secondary case infected by the primary case. source\n\nIn our model, in which infectiousness is constant over the course of the disease, the average generation time is \\(1/d\\) and \\(R_0 = b/d\\), so that \\(b = R_0 d\\).\nLenni’s point estimates were \\(\\hat R_0 = 3\\) and an average generation time of \\(5.1\\) days (corresponding to \\(\\hat d = 1/5.1 \\approx 0.2\\), giving a point estimate for \\(b\\) of \\(\\hat b = 3 / 5.1 \\approx 0.59\\).\n(A future improvement would consider estimates specifically for this population and time period.)\nAt this value of \\(b\\), a daily incidence of \\(10^{-3}\\) corresponds to a prevalence of \\(i(t) = 10^{-3}/\\hat b \\approx 1.7 \\cdot 10^{-3}\\).\nTherefore we have \\(P = 3\\cdot 10^{-7}\\) when \\(i = 1.7 \\cdot 10^{-3}\\), so that \\(sB = P / i = 1.8 \\cdot 10^{-4} \\sim 10^{-4}\\).\nFrom\n\\[\\begin{align}\n  \\mathcal M = \\frac{M^*}{s B i(t^*) },\n\\end{align}\\]\nwe can compute that we would need to sequence a single sample to a depth of \\(\\sim 10^{9}\\) fragments to see 10 fragments of SARS2 when \\(i = 10^{-4}\\) of the population is infected.\nWhat about seeing an individual 40-mer in the SARS2 genome 10 times when \\(i = 10^{-4}\\), for 1x150bp sequencing?\nIn this case, roughly \\(10^{-2.5}\\) of SARS2 reads will cover the 40-mer (\\((150 - 40 + 1) / 30000\\)), so that we’d need to increase our sequencing depth by \\(10^{2.5}\\)-fold relative to the case of just seeing the genome 10 times, resulting in a total sequencing depth of \\(10^{11.5}\\) reads.\nDetails and complications\nGeneral assumptions\nLocal outbreak — cases come in from elsewhere (or a local release), then spread locally\nLocal wastewater treatment plant (WWTP)\nAssume that the WWTP is a good proxy for the local population\nConsider the exponential phase of the outbreak.\nThus we need to model\nexponential growth (possibly noisy)\nshedding (possibly noisy)\ntransport — possible delay, noise\nsample collection\nmeasurement via qPCR, amplicon sequencing, metagenomic sequencing\nEven under the simplified scenario of a local outbreak and monitoring system (closed system), there is still complication from stochasticity and complex disease time courses.\nTo make progress we need to look for ways to approximate the dynamics that still gives an accurate picture for the purposes of detection.\nGoal of this section is to explain when and why we can approximate the dynamics of cumulative infections, active infections, and the amount of collected NA as growing deterministically and exponentially.\nComplex disease time courses\nQualitative model:\n\n\nShow code\n\ntribble(\n  ~name, ~x, ~xend,\n  'symptomatic', 8, 16,\n  'infectious', 5, 12,\n  'shedding', 3, 20\n) %>%\n  mutate(\n    across(name, fct_inorder),\n    y = name %>% as.integer\n  ) %>%\n  ggplot(aes(x = x, xend = xend, color = name, label = name)) +\n  theme_minimal_hgrid() +\n  theme(legend.position = 'none') +\n  scale_x_continuous(breaks = c(0, 10, 20)) +\n  expand_limits(x = 0, xend = 20) +\n  scale_y_discrete(limits = rev) +\n  scale_color_brewer(type = 'qual', palette = 2) +\n  labs(x = 'time since exposed (days)', y = NULL) +\n  geom_segment(aes(y = name, yend = name), size = 1)\n\n\n\nQuantitative model would be to have each of these represented by a continuous function of time.\nCould further have a (joint) distribution of functions.\nCan tackle this by considering the impact on the exponential growth rate and the scaling factors relating exposed to infected to recovered to shedding amounts.\nA key interest is the relationship between cumulative exposures (i.e. infected) to currently infectious to shedding.\nAt large enough numbers, we should be able to approximate the effect of these complex dynamics through effective parameters linking the number of infected, number of infectious, and current rate of shedding; all will be growing asymptotically exponentially.\nThere are various plausible models here.\nAs a simple illustration, suppose that….\n\nA main issue left unaddressed by the SEIR model is when shedding occurs.\nAlso, we might not be that happy with the exponential waiting times for the disease stages.\nWe can address these issues by considering a more complex disease time course which explicitly includes shedding.\nTo remain tractable in the present framework, what is important is that it is still safe to assume that we quickly reach approximate exponential growth in the number of infected and that daily shedding is proportional to the number of infected in this asymptotic regime.\nQuestions to investigate here include\nwhat are the asymptotic growth rates and shedding proportionality constants?\nrelated: how does the disease time course affect growth rate and the proportionality constant between shedding and the number of infected?\nwhat is the time scale for reaching the asymptotic exponential growth regime?\nSEIR model\nTODO: Consider reparameterizing to the other common form of \\(dS/dt = -(\\beta/N) SI\\), so that we can write \\(R_0 = \\beta / \\gamma\\) independent of \\(N\\).\nI’d need to adjust the code accordingly.\nA typical way to consider the effect of a latent (non-infectious) period is using the SEIR model.\nDefine the SEIR model as\n\\[\\begin{align}\n  \\frac{dS}{dt} &= -\\beta S I \\\\\n  \\frac{dE}{dt} &= \\beta S I - \\theta E \\\\\n  \\frac{dI}{dt} &= \\theta E - \\gamma I \\\\\n  \\frac{dR}{dt} &= \\gamma I\n\\end{align}\\]\nwhere \\(S\\) is the number of susceptible, \\(E\\) is the number of exposed, \\(I\\) is the number of infected, and \\(R\\) is the number of recovered; \\(\\beta\\) is the transmission rate, \\(\\theta\\) is the incubation rate, and \\(\\gamma\\) is the recovery rate.\n(This parameterization is from Diekmann, Heesterbeek, and Britton (2012).)\nI expect that we’ll typically be able to use a quasi-equilibrium approximation where \\(E\\) and \\(I\\) each grow exponentially.\nLet’s work out that situation.\nEarly in the pandemic, we can take \\(S \\approx 1\\) and \\(R \\approx 0\\), giving the approximate system\n\\[\\begin{align}\n  \\frac{dE}{dt} &= - \\theta E + B I \\\\\n  \\frac{dI}{dt} &= \\theta E - \\gamma I,\n\\end{align}\\]\n(where \\(B = \\beta N\\))\nwhich converges to exponential growth at rate given by\n\\[\\begin{align}\n  r = \\frac{-(\\theta + \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2}.\n\\end{align}\\]\n(given by the leading eigenvector \\(\\lambda_1\\) of the Jacobian matrix) (see Ma (2020)).\nWe can solve for the equilibrium ratio of exposed to infected, \\(E/I\\), by solving for \\(d/dt(E/I) = 0\\); this gives\n\\[\\begin{align}\n  \\frac{E}{I}\n  &= \\frac{-(\\theta - \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2 \\theta}\n\\\\&= \\frac{r + \\gamma}{\\theta}.\n\\end{align}\\]\n(Alternatively, we could get the equilibrium ratio from the leading eigenvector of the Jacobian; see below).\nFor \\(\\theta (B - \\gamma) \\ll (\\theta + \\gamma)^2\\), we can approximate \\(r\\) as\n\\[\\begin{align}\n  r \\approx \\frac{\\theta (\\theta - \\gamma)}{\\theta + \\gamma},\n\\end{align}\\]\nwhich is consistent with the results of Heng and Althaus (2020).\nHeng and Althaus (2020) focus on an approximation in which the SEIR model’s dynamics are approximately the same as the corresponding SIR model, but scaled by a factor of \\(\\alpha = \\theta / (\\theta + \\gamma)\\).\nMore explicitly, the eigenvalues are (Wolfram Alpha)\n\\[\\begin{align}\n  \\lambda_1 &= \\frac{-(\\theta + \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2} \\\\\n  \\lambda_2 &= \\frac{-(\\theta + \\gamma) - \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2}.\n\\end{align}\\]\nThe eigenvectors are\n\\[\\begin{align}\n  v_1 &= \\left(\\frac{-(\\theta - \\gamma) + \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2\\theta}, 1 \\right) \\\\\n  v_2 &= \\left(\\frac{-(\\theta - \\gamma) - \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}}{2\\theta}, 1 \\right).\n\\end{align}\\]\nI believe the (exponential) rate of convergence to the exponential quasi-equilibrium is given by the difference \\(\\lambda_1 - \\lambda_2\\),\n\\[\\begin{align}\n  \\lambda_1 - \\lambda_2 = \\sqrt{(\\theta - \\gamma)^2 + 4 \\theta B}.\n\\end{align}\\]\n\n\nShow code\n\n# modified from http://epirecip.es/epicookbook/chapters/seir/r_desolve\n\n# Function to return derivatives of SEIR model\nseir_ode <- function(t,Y,par){\n  S <- Y[1]\n  E <- Y[2]\n  I <- Y[3]\n  R <- Y[4]\n  \n  beta <- par[1]\n  theta <- par[2]\n  gamma <- par[3]\n  \n  dYdt <- vector(length=3)\n  dYdt[1] <- -beta*I*S\n  dYdt[2] <- beta*I*S - theta*E\n  dYdt[3] <- theta*E - gamma*I\n  \n  return(list(dYdt))\n}\n\n# Set parameter values\ntheta <- 1/5\ngamma <- 1/10\n# Set beta from the basic reproduction number, since R0 = beta * N / gamma\nR0 <- 3\nN <- 1e6\nbeta <- R0 * gamma / N\n\n# Set initial conditions\nE0 <- 1\nI0 <- 0\ninit <- c(N - E0 - I0, E0, I0)\nt <- seq(0, 250)\npar <- c(beta, theta, gamma)\n# Solve system using lsoda\nsol <- deSolve::lsoda(init, t, seir_ode, par) %>% \n  as_tibble %>%\n  set_names(c(\"time\", \"S\",\"E\",\"I\")) %>%\n  mutate(R = N - (S + E + I))\n\nr_pred = (-(theta + gamma) + sqrt((theta - gamma)^2 + 4 * theta * beta * N)) / 2\n\nx <- sol %>%\n  mutate(\n    across(everything(), as.numeric),\n    I_pred = pmin(exp(r_pred * time), N),\n  ) %>%\n  pivot_longer(-time) %>%\n  mutate(across(name, fct_inorder))\n\nclrs <- c(\n  S = colors_oi[['orange']],\n  E = colors_oi[['skyblue']],\n  I = colors_oi[['bluishgreen']],\n  R = colors_oi[['vermillion']],\n  I_pred = colors_oi[['gray']] \n)\n\n\n\n\nShow code\n\nx %>%\n  filter(time <= 200, name != 'I_pred') %>%\n  ggplot(aes(time, value, color = name)) +\n  scale_color_manual(values = clrs) +\n  geom_line() +\n  labs(y = \"Number of individuals\")\n\n\n\n\n\nShow code\n\nx %>%\n  filter(time <= 200, name != 'S') %>%\n  ggplot(aes(time, value, color = name)) +\n  scale_color_manual(values = clrs) +\n  scale_y_log10(limits = c(1, N), breaks = c(1, 1e2, 1e4, 1e6)) +\n  geom_line() +\n  labs(y = \"Number of individuals\")\n\n\n\nNote, I haven’t calculated the constant for the asymptotic number of infected. But we can see that we have the correct asymptotic growth rate.\nLet’s also check our prediction for the asymptotic ratio of \\(E/I\\) in the exponential regime,\n\n\nShow code\n\nratio_pred <- (-(theta - gamma) + sqrt((theta - gamma)^2 + 4 * theta * beta * N))/(2 * theta)\nx %>%\n  pivot_wider(time) %>%\n  ggplot(aes(time, E / I)) +\n  scale_y_log10() +\n  geom_hline(yintercept = ratio_pred, color = 'grey', \n             size = 0.8, linetype = 2) +\n  geom_line() +\n  labs(y = \"E / I\")\n\n\n\nThe simulation starts with 1 exposed individual; hence, the ratio of exposed to infected starts high but quickly drops to the quasi-equilibrium ratio of 1, then eventually starts to decline as the susceptible population declines and causes a drop in the rate of new infections, before appearing to asymptote at a positive value in the waning days of the epidemic.\nPossible additions\nExpression for the peak susceptible, and total infected\n\n\nShow code\n\n# from https://cran.r-project.org/web/packages/odin/vignettes/discrete.html\nlibrary(odin)\n\n## Core equations for transitions between compartments:\nupdate(S) <- S - beta * S * I / N\nupdate(E) <- S - beta * S * I / N\nupdate(I) <- I + beta * S * I / N - gamma * I\nupdate(R) <- R + gamma * I\n\n## Total population size (odin will recompute this at each timestep:\n## automatically)\n# N <- S + I + R\nN <- 1\n\n## Initial states:\ninitial(S) <- S_ini # will be user-defined\ninitial(E) <- E_ini # will be user-defined\ninitial(I) <- I_ini # will be user-defined\ninitial(R) <- 0\n\n## User defined parameters - default in parentheses:\nS_ini <- user(1 - 1e-4)\nE_ini <- user(0)\nI_ini <- user(1e-4)\nbeta <- user(0.2)\ntheta <- user(0.1)\ngamma <- user(0.1)\n\n\n(Random) variation in transmission\nOne important form of randomness is in the number of secondary infections caused by a single infection.\nDiscrete generation models make it particularly simple to model inter-individual variation in this number.\nThe Gamma-Poisson (GP) distributrion (i.e., Negative Binomial) is a convenient (and seemingly common) distributrion to use for this.\nA possible justification of a Poisson mixture distribution like this is to suppose that each individual varies in how infectious they are, but conditional on their infectiousness they infect a random number of others according to the standard Poisson assumption.\nChoosing a Gamma distribution for the first step is convenient because it gives us just one parameter to tune the CV in the infectiousness among individuals.\nSee below for more about the GP distribution and the parameterization in terms of the mean and CV of the underlying gamma distribution.\nVery small CVs for the underlying gamma distribution amount to little variation in infectiousness.\nA CV of 1 in the Gamma distribution corresponds to an exponential; higher CVs will be even fatter tailed.\nIn these cases, there will tend to be ‘super spreader’ infections with much higher infectiousness than the typical infection.\nWe can see the effect of stochasticity with tunable variation in infectiousness by simulating a discrete generation branching process with a GP offspring distribution.\nSuppose there are \\(I\\) infectious individuals, and each infects an iid GP number of others, with parameters \\(\\mu\\) and \\(\\kappa\\).\n(The mean \\(\\mu\\) is also the \\(R_0\\) of the disease.)\nTo simulate, we can exploit the fact that the sum of \\(n\\) iid GP random variables with mean \\(\\mu\\) and CV of the gamma distributrion \\(\\kappa\\) is a GP random variable with mean \\(n \\mu\\) and gamma CV of \\(\\kappa / \\sqrt{n}\\).\n(Note, this is somewhat a guess based on the fact that for the sum of iid Gamma rvs, the scale parameter remains the same; and the sum of Poissons is Poisson with appropriately increased mean, so we can think of summing up all the gamma’s first, then feeding into a single Poisson.)\n\n\nShow code\n\nnext_generation <- function(I0, mean, cv) {\n  n <- length(I0)\n  mean_sum <- I0 * mean\n  cv_sum <- cv / sqrt(I0)\n  scale_sum = cv_sum^2 * mean_sum\n  shape_sum = cv_sum^(-2)\n\n  rgamma(n = n, scale = scale_sum, shape = shape_sum) %>%\n    {rpois(n = n, lambda = .)}\n}\n\n# Simulate many runs over many generations\nset.seed(42)\nnum_runs <- 100\nnum_gens <- 20\nmat <- matrix(0, nrow = num_gens, ncol = num_runs)\nmat[1,] <- 1\nfor (i in seq(2, nrow(mat))) {\n  mat[i, ] <- next_generation(mat[i-1, ], mean = 2, cv = 0.3)\n}\nsims <- mat %>% as_tibble %>%\n  mutate(time = row_number() - 1) %>%\n  pivot_longer(-time, names_to = '.id') %>%\n  mutate(\n         across(.id, str_sub, 2),\n         across(.id, as.numeric)\n  )\nrm(mat)\n\n\n\n\nShow code\n\nsims %>%\n  filter(.id <= 20, time <= 15) %>%\n  ggplot(aes(time, value, color = as.factor(.id))) +\n  theme(legend.position = 'none') +\n  labs(x = 'Time (generations)', y = 'Infections') +\n  scale_y_log10(breaks = c(1, 10, 1e2, 1e3, 1e4)) +\n  geom_line(aes(y = exp(log(2) * time)), color = 'black', linetype = 4) +\n  geom_line()\n\n\n\nIn the simulations above (R0 of 2, CV of 0.3), we see that for at or below ~10 infections, stochasticity is important; but for much above, we have approximately deterministic exponential growth.\nIn a superspreader scenario (R0 of 2, CV of 2),\n\n\nShow code\n\n# Simulate many runs over many generations\nset.seed(42)\nnum_runs <- 100\nnum_gens <- 20\nmat <- matrix(0, nrow = num_gens, ncol = num_runs)\nmat[1,] <- 1\nfor (i in seq(2, nrow(mat))) {\n  mat[i, ] <- next_generation(mat[i-1, ], mean = 2, cv = 2)\n}\nsims <- mat %>% as_tibble %>%\n  mutate(time = row_number() - 1) %>%\n  pivot_longer(-time, names_to = '.id') %>%\n  mutate(\n         across(.id, str_sub, 2),\n         across(.id, as.numeric)\n  )\nrm(mat)\n\n\n\n\nShow code\n\nsims %>%\n  filter(.id <= 20, time <= 15) %>%\n  ggplot(aes(time, value, color = as.factor(.id))) +\n  theme(legend.position = 'none') +\n  labs(x = 'Time (generations)', y = 'Infections') +\n  scale_y_log10(breaks = c(1, 10, 1e2, 1e3, 1e4)) +\n  geom_line(aes(y = exp(log(2) * time)), color = 'black', linetype = 4) +\n  geom_line()\n\n\n\nwe see that stochasticity remains relevant for larger sizes of 100 to 300 infections.\nWe can also more strongly see another expected effect of randomness, which is that conditional on survival, the outbreak grows faster than the asymptotic rate initially.\nNotably, however, in both cases the asymptotic exponential growth rate is the same (\\(\\log R_0\\)).\nRemaining questions/todos include\nEstablishment size above which an outbreak is approximately guaranteed and can ignore stochastic effects; what is this size for real epidemics?\nI’m used to models where the point at which success is assured and which can ignore stochastic effects are the same; is that also true with very skewed offspring/infection distributions?\nExpression for the CV of \\(I(t)\\)\nRelevant references\nMorán-Tovar et al. (2022) section 2.2 discusses stochasticity in transmission, but without variation in infectiousness among individuals.\nThe model there is equivalent to what Charlie, Dan, and Jake have been using.\nMorán-Tovar et al. (2022) section 2.3 uses a contact network model to consider heterogeneity in individuals.\nSuper-spreaders emerge from highly-connected nodes.\nOrigination of the outbreak\nSuppose new infections coming in at rate \\(M\\).\nCan understand the point at which we have a deterministic local outbreak and can ignore new imports, based on what we know about this from evolutionary theory.\nShedding\nSimple model used in the modeling literature links cases with shedding by supposing that there is a fixed distribution of shedding as a function of time post symptom onset.\nThe idea that everyone sheds the same can be misleading; and is particularly sketchy in cases where there is substantial heterogeneity in how people respond to the disease.\nStill, it is a fair starting point and useful for illustration.\nIn our case, we’re interested in shedding that occurs even without clinical presentation, so I’ll consider that there is a shedding distribution \\(s(t)\\) indicating the amount of shedding from a person \\(t\\) days after infection (exposure).\nIf the number of exposures is \\(E(t)\\), then the total shedding \\(S(t)\\) at time \\(t\\) is given by the convolution of \\(E\\) with \\(s\\),\n\\[\\begin{align}\n  S(t) = \\int_{-\\infty}^{\\infty} E(u) \\; s(t - u) \\; du.\n\\end{align}\\]\nSuppose the number of exposed grows exponentially at rate \\(r\\), \\(E(t) = E_0 e^{rt}\\).\nWe know that \\(s(t) = 0\\) for \\(t \\le 0\\), and we further expect \\(s\\) to initially increase, peak at some time \\(t^*\\), and then decay to 0.\nLet’s take it to be Gamma distributed with a mean of 10 days and a CV of 0.5,\n\n\nShow code\n\nshedding <- tibble(mean = 10, cv = 0.5) %>%\n  mutate(\n    sd = cv * mean,\n    var = sd^2,\n    scale = var / mean,\n    shape = mean / scale\n  ) %>%\n  crossing(t = seq(0, 30, by = 0.1)) %>%\n  mutate(\n    density = dgamma(x = t, shape = shape, scale = scale)\n  )\nshedding %>%\n  ggplot(aes(t, density)) +\n  labs(x = 'Time (days) after exposure', y = 'Density of shedding') +\n  geom_line()\n\n\n\nHere, total area under the curve is 1, so we can think of the units as in terms of the total shedding over a single infection.\nNow let’s view the total shedding rate over time, starting from a single exposed and under a growth rate of \\(r = 0.1\\),\n\\[\\begin{align}\n  S(t) = \\int_{0}^{t} e^{r u} \\; s(t - u) \\; du.\n\\end{align}\\]\n\n\nShow code\n\ntotal_shedding <- function(t, r, gamma_mean, gamma_cv) {\n  scale = gamma_cv^2 * gamma_mean\n  shape = gamma_cv^(-2)\n  f <- function(u) { exp(r * u) * dgamma(t - u, shape = shape, scale = scale) }\n  integrate(f, lower = 0, upper = t)\n}\n# total_shedding(10, 0.1, 10, 0.5)$value\n\n\n\n\nShow code\n\nr <- 0.1\nx <- tibble(t = seq(0, 100)) %>%\n  mutate(\n    Exposed = exp(r * t),\n    Shed = map(t, total_shedding, r = r, gamma_mean = 10, gamma_cv = 0.5),\n    across(Shed, map_dbl, 'value')\n  )\n\n\n\n\nShow code\n\nx %>%\n  pivot_longer(-t) %>%\n  ggplot(aes(t, value, color = name)) +\n  scale_y_log10(limits = c(1e-2, NA)) +\n  geom_line()\n\n\n\nAfter an initial lag, the shedding rate too grows exponentially, as expected.\nWe can attempt a rough calculation of the ratio ratio \\(S/E\\):\nThe total shedding amount over an infection is 1\nThe bulk of the shedding of the infected is roughly between days 5 and 15;\nSo \\(S(t)\\) is roughly the sum of contributions from those infected over 5-15 days in the past, and each of them are contributing 1/10. (The mean \\(\\pm\\) the standard deviation)\n5-15 days in the past is (in this case) a period of \\(-1/2r\\) to \\(-3/2r\\); the integral of \\(e^{rt}\\) over that period is \\(1/(10 r) e^{rt} (e^{-1/2} - e^{-3/2})\\)\nThis suggests a ratio of \\((e^{-1/2} - e^{-3/2})/(10 r)\\) or 0.38.\n\n\nShow code\n\nratio_guess <- (exp(-1/2) - exp(-3/2)) / (10 * r)\nx %>%\n  mutate(Guess = Exposed * ratio_guess) %>%\n  pivot_longer(-t) %>%\n  ggplot(aes(t, value, color = name)) +\n  scale_y_log10(limits = c(1e-2, NA)) +\n  geom_line()\n\n\n\nWe also expect there to be substantial variance in shedding among and within individuals; what effect does this have on total shedding over the course of a day?\nThe distinction between variance within versus between individuals is not fully defined outside the context of a specific model, but roughly I mean the following.\nBy variance among individuals, I mean the fact that individuals may respond differently to the pathogen in a manner that causes some to shed much more than others, or to have a significantly different time course.\nBy variance within individuals, I mean that how much any one person sheds on a given day (or exactly when within the day) might show additional variation beyond what can be explained from these broader differences.\nRoughly speaking, we expect the variance in shedding to be negligible when the number of individuals who contribute to a sample is large enough.\nHow large is enough?\nIt is reasonable to suppose that each infected person sheds independentally, and therefore the CV in the total amount shed on a given day scales as \\(1/\\sqrt{n}\\), where \\(n\\) is the number of infected shedders.\nHowever, each infected shedder does not contribute equally to shedding — any ‘super shedders’ or people who are in the peak shedding period will contribute (on average) much more than others.\nFor a given assumption about the shedding timecourse and how it varies among infected individuals, there will be some effective number of shedders, \\(n_e\\), which will be less than the total number of shedders \\(n\\), that captures how the \\(CV\\) decreases.\nAt quasi-equilibrium, there will be a fixed distribution of shedders and \\(n_e \\propto n\\).\nWhat we ultimately care about is the shedding contributing to an individual WWTP sample.\nA 24h composite sample will integrate over shedding from more than the past 24h, which will tend to buffer the day to day variation and thus increase the corresponding \\(n_e\\) to some degree.\nOur current guess is that most shedding will be arrive at the sampling site within 24h, in which case this effect will be small.\nWe can get a sense of the effective number of shedders contributing to a WWTP sample from studies that have attempted to link SARS2 or influenza WWTP levels to local prevalence, divided by the mean daily shedding rate observed in studies that assess fecal shedding in individuals.\nTransport\nIt takes time for the waste to travel to the WWTP, (I think) typically on the order of hours to days.\nThis time depends on the location and its distance from the WWTP, along with other factors such as the typical flow rates in the various pipes and effects on the flow such as usage and rain.\nAt a most basic level, the effect of transport is to create a lag between shedding and MGS signal and therefore to delay when an outbreak is detected by a corresponding amount.\nBut perhaps more important is what happens to the waste during this time.\nI address both in turn.\nEffect of transport time\nIt takes time for the waste to travel to the WWTP.\nHow does this affect the pathogen relative abundance at the point of sampling?\nComplications include\n* Shedding will be taking place all over the city, at different times, randomly.\n* The signal from a given shedding event may spread out over days.\n* But in the quasi-exponential regime, I guess that we can capture these complications by an effective recovery parameter.\n* Rain and/or other large perturbations to the WW system. Can we capture this with an effective noise (CV) parameter?\nAs a way to get intuition, suppose that the amount being shed into the WW system at time \\(t\\) is given by \\(S(t)\\) and that the fraction of NA collected in the sample a time \\(t\\) after it is shed is given by \\(w(t)\\).\nThe amount of NA collected at time \\(t\\), \\(W(t)\\), is given by the convolution of \\(S\\) and \\(w\\),\n\\[\\begin{align}\n  W(t) = \\int_{-\\infty}^{\\infty} S(u) \\; w(t - u) \\; du.\n\\end{align}\\]\nOur main interest is in \\(W(t)\\) when the shedding amount grows exponentially at rate \\(r\\), \\(S(t) = S_0 e^{rt}\\).\nWe know that \\(w(t) = 0\\) for \\(t \\le 0\\), and we further expect \\(w\\) to initially increase, peak at some time \\(t^*\\), and then decay to 0.\nFor WWTPs, I expect the peak to be on the order of a day or less (\\(t^* \\lesssim 1\\)), and the timescale of the decay to be on the order of days, e.g. the half life around 0.5 to 3 days, but I haven’t looked into this carefully and this intuition is mostly based on the Helsinki study.\nIntuition for shape of \\(w(t)\\):\nCould try modeling as flow with turbulent diffusion -> model concentration with Brownian motion with a constant ‘drift’ term representing the average flow rate. Should be fairly easy to simulate and understand the timescale and tail shape with formal solution or heuristics for this approximation.\nComplication is that the transit length will vary by shedding location, and that the flow rate will vary by shedding location as well as along the transport path. The diffusion rate would also vary.\nMay be a long tail of arrival due to low-turnover reservoirs in the system where things might hang out; however, I don’t expect these to be very important contributors in an exponential growth scenario.\nPerturbances such as a rain storm may also be important.\nIt still seems useful to consider the simple model where there is a given transit length \\(l\\).\nBrownian motion at rate \\(\\sigma\\) with drift at rate \\(v\\), where the transit length is \\(l\\), gives1 that the concentration at time \\(t\\) is normally distributed with mean \\(vt\\) and standard deviation \\(\\sigma \\sqrt{t}\\).\nThe concentration at the sampling point (location \\(l\\) from the shedding location) is\n\\[\\begin{align}\n  w_l(t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 t}} \\exp \\left[\\frac{(l - v t)^2}{2\\sigma^2 t} \\right].\n\\end{align}\\]\nThe concentration peak occurs at \\(t^* = l / v\\).\nTODO: Determine the decay behavior\n\n\nShow code\n\nv = 1\nl = 1\nsigma = 0.2\nt_star = l/v\nsigma_star = sigma * sqrt(t_star)\n\ntibble(\n  t = seq(0, 2, by = 0.01)\n) %>%\n  mutate(\n    w = dnorm(l, mean = v * t, sd = sigma * sqrt(t))\n  ) %>%\n  ggplot(aes(t, w)) +\n  geom_line() +\n  geom_vline(xintercept = t_star, color ='grey') +\n  annotate(geom = 'segment', color = 'grey',\n           x = t_star - sigma_star,\n           xend = t_star + sigma_star,\n           y = 1, yend = 1)\n\n\n\nNote that the distribution is skewed, with more weight on the right side of the peak.\nLet’s now combine this with the shedding function \\(S(t)\\) to consider the total amount of NA collected at \\(t\\),\n\\[\\begin{align}\n  W(t) &= \\int_{-\\infty}^{\\infty} S(u) \\; w(t - u) \\; du \\\\\n       &= \\int_{-\\infty}^{\\infty} S_0 \\frac{1}{\\sqrt{2 \\pi \\sigma^2 (t-u)}}\n          \\exp \\left[ru + \\frac{(l - v (t-u))^2}{2\\sigma^2 (t-u)}\\right] \\; du\n\\end{align}\\]\nMy guess is we can do an ok job of understanding the behavior with a heuristic approximation, where we consider the lag time \\(t^* = l/v\\) and the spread at this time approximated by the standard deviation \\(\\sigma \\sqrt{t^*}\\).\nMy current very rough guess is that typically, most of the shed material passes through the WWTP influent within 24h, and that the vast majority (i.e., \\(\\ge 90\\%\\)) passes through the WWTP within 4 days.\nThis guess is based on\nI recall the Helsinki poliovirus deposition experiment saw a peak within 1-2 days\nStudies that attempt to link WWTP qPCR levels of SARS2 or flu to community infections so far seem to have largely ignored the WWTP transit time, focusing instead on the delay caused by the timecourse of shedding\nhttps://www.sciencedirect.com/science/article/pii/S0048969721051962\nhttps://ehp.niehs.nih.gov/doi/full/10.1289/EHP10050\nhttps://www.medrxiv.org/content/10.1101/2023.01.23.23284894v1\n\nSoller et al. (2022) fit a lognormal distribution to the times reported by Kapo et al. (2017), finding a lognormal distribution in hours with parameters (1.2, 0.85), which are the mean and variance of the underlying normal distribution and have units of natural-log hours. This corresponds to a geometric mean of 3.3h and a geometric sd of 2.3h.\nAn on-the-fly calculation done by Lenni using this site to estimate it would take ~4h for WW to flow from Kendall Square to Deer Island WWTP\nSoller et al. (2022) notes that, for samples of settled solids, the time the pathogen spends in the WWTP before sample collection may be larger than the time spent traveling to the WWTP.\n2023-03-31: I’ve updated towards transit times being significantly longer than what is suggested above for our local WWTP, around 12h for Cambridge and up to ~3 days for the farthest locations (Framingham).\nChanges in waste composition during transport\nDuring transport, there will be live biological and chemical activity, which can affect the composition of the sample collected and in particular the relative abundance of the pathogen.\nThe relative abundance of the pathogen NA (the proportion of pathogen NA among total NA) will change due to\nGrowth of the pathogen is possible but does not seem relevant for most pathogens, which I expect to only replicate in human cells, but could be true for some pathogens that can replicate in the GI tract or the environment\nDegredation or decay of the pathogen, either due to chemical reactions, physical forces, or biological factors (viruses and their NA may be destroyed or consumed by microbes; microbes may be killed by viruses or other microbes).\nGrowth and other biological activity of the background and sewage microbiome. This will tend to decrease the relative abundance of the pathogen.\nDegredation or decay of the background. This too could be substantial and would serve to increase the relative abundances of the pathogen.\nGrowth and metabolism of members of the microbiome that are more adapted to the sewer environment may may substantially lower the relative abundance of the pathogen (and thus its MGS reads) via purely compositional effects (increase in background abudance lowers pathogen relative abundance) and via increasing the decay of the pathogen through destruction/consumption.\nThere may also be a resident sewer microbiome, which could be contributing an additional amount of non-pathogen NA and adding to the decay of the pathogen.\nNote, the pathogen and background will be changing physical form and it isn’t obvious how that will affect the resulting MGS data. E.g. a virus can be killed or inactivated but still yield NA.\nThe period in the primary clarifier may also be important for solids samples, depending on how long that time is and how more/less hospitable that setting is.\nWe can write out the relevant math. The contribution of the native sewage microbiome can be modeled by a fixed additional contribution, which will have different MGS bias. The differences in growth and decay can be modeled to first approximation with organism-dependent bias factors that scale with transport time, though this approximation might have important limitations.\nWe can try lit review to consider how we can expect various organisms to behave during transport.\nI expect that the math will tell us that it is the pathogen’s growth/decay, and the most abundant (weighted in terms of \\(sB\\)) of the background organisms that most matter here.\nSample collection\nConsider implications of different sample types, particularly influent versus settled solids.\nPartitioning into solid and liquid fractions\nHow the virus and the background partition into the solid versus liquid fraction will influence the relative bias parameter \\(B\\).\nFurther, if the partitioning is non-linear, then it will effectively make \\(B\\) no longer a constant.\nKim et al. (2022) fit a version of the Freundlich isotherm model,\n\\[\\begin{align}\n  C_s = K_f C_l^{1/n},\n\\end{align}\\]\nwhere\n\\(C_s\\) is the concentration in the solid fraction (copies per g)\n\\(C_l\\) is the concentration in the liquid fraction (copies per mL)\n\\(K_f\\) is a coefficient (ml per g)\n\\(n\\) is a coefficient (unitless) that determines the exponent \\(1/n\\) and hence linearity of the relationship between \\(C_s\\) and \\(C_l\\)\nThey fit this model using measured concentration in influent for \\(C_l\\) and the measured concentration in settled solids for \\(C_s\\), using linear regression of \\(\\log C_s\\) against \\(\\log C_l\\).\nThey do so separately for 5 treatment plants, finding results consistent with \\(n \\in [2,3]\\) and \\(K_f \\in [10^3, 10^5]\\) mL g\\(^{-1}\\).\nNotably, the fact that \\(\\hat n \\ge 2\\) suggests that the relationship between \\(C_s\\) and \\(C_l\\) is not linear; instead, \\(C_s\\) increases more slowly than \\(C_l\\).\nNote: When I examined the data by eye, I observed a poor fit which suggests we shouldn’t take these results too literally.\nIt is hard to say what this implies about MGS measurements without also knowing how the concentration of the background behaves.\nThis study also performs measurments of PMMoV; it and related Tobamaviruses are important members of the background for RNA-based protocols.\nHowever, non-enveloped viruses as well as bacteria are important background members which we might expect to behave quite differently.\nFuture studies that use metagenomics of paired influent and solids samples can consider how the \\(K_f\\) and \\(n\\) parameters vary across different types of organisms.\nDetection methods\nMetagenomic sequencing\nA few features of metagenomic sequencing to be aware of\nTo first approximation under standard MGS protocols, the number of reads of the pathogen depends on how deeply we sequence overall and on the relative abundance of the pathogen NA in the sample, not the absolute amount or concentration of pathogen NA.\nMGS measurements are taxonomically biased; there can also be bias within a genome\nMGS measurements are noisy and come as discrete counts.\nThe noise (or variability) associated with the random counting process associated with sequencing is typically modeled as Poisson or multinomial.\nWhich isn’t very important, as the multinomial is what you get if you suppose that there are a bunch of different species or other taxonomic features, all of which are independent Poisson counts, and you’ve conditioned on the sum (total count).\nFor the purposes of this document, I only consider the reads mapping to a single pathogen or subsequence and so ignore the multivariate nature of the MGS measurement.\nThe Poisson model only captures a part of the variance associated with the MGS measurement.\nIt is helpful (and more accurate) to think of the read count of the pathogen as coming from a two step process.\nStarting from a WW sample, we process the sample to generate a sequencing library.\nThen, we sequence the sample, giving a Poisson-distributed number of reads.\nEach step entails some bias; the first step also adds noise beyond the Poisson noise.\nIf we model the bias in the Poisson step as a binomial filter, the result remains Poisson and is equivalent to multiplying the mean (\\(\\lambda\\)) parameter of the Poisson distribution by a bias factor.\nA convenient and perhaps natural choice (explained below) is to model the first step by a Gamma distribution, and the second step as a Poisson distribution with mean (\\(\\lambda\\)) parameter equal to the Gamma rv.\nIn this context, it is natural to parameterize the Gamma distribution by its mean\nmean \\(\\mu\\) and coefficient of variation \\(\\kappa\\).\n(The more common shape and scale parameters are shape of \\(1/\\kappa^2\\) and scale of \\(\\mu \\kappa^2\\).)\nThis parameterization allows us to separately consider bias (or effect on the expected count) through \\(\\mu\\) and the added noise from sample processing through \\(\\kappa\\).\n\nWe can plot the density function of the Gamma distribution for a fixed mean and different CVs ranging from above and below 1,\n\n\nShow code\n\nmu <- 1\ngamma_dists <- tibble(mean = mu, cv = c(0.1, 0.3, 1, 3)) %>%\n  mutate(\n    # convert from mean and cv to shape and scale params\n    # sd = cv * mean,\n    # var = sd^2,\n    # scale = var / mean,\n    # shape = mean / scale\n    scale = cv^2 * mean,\n    shape = 1 / cv^2\n  ) %>%\n  crossing(x = seq(0, 3 * mu, by = 0.02 * mu)) %>%\n  mutate(\n    density = dgamma(x = x, shape = shape, scale = scale)\n  )\n\nclrs <- RColorBrewer::brewer.pal(name=\"Greens\", n=5)[2:5]\ngamma_dists %>%\n  mutate(across(cv, as.factor)) %>%\n  ggplot(aes(x / mu, density, color = cv, linetype = cv)) +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_color_manual(values = clrs) +\n  # scale_color_brewer(type = 'seq', palette = 5,\n  #                    values = ~scales::rescale(.x, to = c(0, 0.5))) +\n  labs(x = 'x / E[x]') +\n  geom_line(size = 0.8)\n\n\n\nThis plot was generated with a mean of 1; however, the shape of the distribution does not depend on the mean, only the CV (which determines the ‘shape’ parameter under the traditional parameterization).\nA CV of 1 corresponds to an exponential distribution, which has it’s mode at 0.\nHigher CVs corresponds to distributions that are even more sharply peaked at 0.\nAs the CV decreases, the distribution becomes like a normal distribution with a shrinking standard deviation.\nChanging the mean without changing the CV rescales the entire distribution, keeping the same overall shape.\nThe CV of 1 looks quite different in form, and a CV of 3 is already quite extreme.\nFor CVs of 3 or more, we begin to have large consequences for probability of detection (see below).\nIt will be helpful to find upper bounds on the CV to ensure we can ignore complications that arise when the CV is similar or larger than 1.\nThe read count \\(M\\) then has a Gamma-Poisson (GP) distribution, also known as a Negative Binomial distribution.\nThe mean \\(E[M]\\) under the GP model is the same as that of the underlying Gamma distribution, \\(\\mu\\).\nThe CV of \\(M\\) is\n\\[\\begin{align}\n  \\text{CV}[M] &= \\sqrt{\\frac{1}{\\mu} + \\kappa^2}\n  \\\\&=\n  \\begin{cases}\n    \\frac{1}{\\sqrt{\\mu}} & \\text{for $\\mu \\ll \\kappa$} \\\\\n    \\kappa & \\text{for $\\mu \\gg \\kappa$}.\n  \\end{cases}\n\\end{align}\\]\nThe CV is approximately the larger of \\(\\kappa\\), the CV of the Gamma distribution, and \\(1/\\sqrt{mu}\\), which is the CV of a Poisson variable with mean \\(\\mu\\).\nNext\nAdd in bias and sequencing depth\nExplain how bias in sample prep and after can both be captured by the \\(B\\) parameter\nPerhaps down below, consider the effect of noise on detection i.e. the probability of seeing a certain number of reads\nCovering a target\nIn future version, perhaps put GP definition and features in an Appendix\nProbability of seeing zero reads as a function of the mean or CV parameter.\nCan look at using the negative binomial distribution in R; set the ‘mu’ parameter equal to the mean of the gamma rv, and the ‘size’ parameter equal to the shape (or \\(1/\\kappa^2\\)) of the gamma rv.\n\n\nShow code\n\nnb_prob <- crossing(mean = 10^seq(-2, 3, by = 0.02), cv = c(0.01, 0.1, 1, 3, 10)) %>%\n  mutate(\n    # Standard gamma params\n    scale = cv^2 * mean,\n    shape = 1 / cv^2,\n    # neg binom params\n    mu = mean,\n    size = shape\n  ) %>%\n  mutate(\n    prob_0 = dnbinom(x = 0, mu = mean, size = shape),\n    prob_ge_1 = pnbinom(q = 0, mu = mean, size = shape, lower.tail = FALSE),\n    prob_ge_10 = pnbinom(q = 9, mu = mean, size = shape, lower.tail = FALSE),\n  )\n\n\n\n\nShow code\n\nclrs <- RColorBrewer::brewer.pal(name=\"Greens\", n=5)[2:5]\nnb_prob %>%\n  filter(cv < 10, mean <= 1e2) %>% \n  mutate(across(cv, as.factor)) %>%\n  ggplot(aes(mean, prob_0, color = cv, linetype = cv)) +\n  scale_x_log10() +\n  # scale_y_log10(limits = c(1e-10, 1)) +\n  theme(\n    # axis.text.y = element_blank(),\n    # axis.ticks.y = element_blank()\n  ) +\n  scale_color_manual(values = clrs) +\n  # scale_color_brewer(type = 'seq', palette = 5,\n  #                    values = ~scales::rescale(.x, to = c(0, 0.5))) +\n  labs(y = 'Pr(X = 0)') +\n  geom_line(size = 0.8)\n\n\n\n\n\nShow code\n\nclrs <- RColorBrewer::brewer.pal(name=\"Greens\", n=5)[2:5]\nnb_prob %>%\n  filter(cv < 10, mean <= 1e2) %>% \n  mutate(across(cv, as.factor)) %>%\n  ggplot(aes(mean, prob_ge_1, color = cv, linetype = cv)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  theme(\n    # axis.text.y = element_blank(),\n    # axis.ticks.y = element_blank()\n  ) +\n  scale_color_manual(values = clrs) +\n  # scale_color_brewer(type = 'seq', palette = 5,\n  #                    values = ~scales::rescale(.x, to = c(0, 0.5))) +\n  labs(y = 'Pr(X >= 1)') +\n  geom_hline(yintercept = 0.5, color = 'grey') +\n  geom_line(size = 0.8)\n\n\n\nSee that, for probability of 0 reads, a CV of 0.1 or less is indistinguishable from Poisson, but a CV on the order of 1 or more has a significant effect.\nThe point where detection becomes more likely than not (grey line) is similar for CV <= 1, but becomes much larger for CV = 3 (about 100-fold larger).\nThese plots also show that there is a monotonic relationship of CV with probability of detection via a single read, where a larger CV lowers the probabability for every mean.\nNext, consider the probability of detection in 10 or more reads,\n\n\nShow code\n\nclrs <- RColorBrewer::brewer.pal(name=\"Greens\", n=5)[2:5]\nnb_prob %>%\n  filter(cv < 10, mean >= 1e-1) %>% \n  mutate(across(cv, as.factor)) %>%\n  ggplot(aes(mean, prob_ge_10, color = cv, linetype = cv)) +\n  scale_x_log10() +\n  scale_y_log10(limits = c(1e-4, 1)) +\n  theme(\n    # axis.text.y = element_blank(),\n    # axis.ticks.y = element_blank()\n  ) +\n  scale_color_manual(values = clrs) +\n  # scale_color_brewer(type = 'seq', palette = 5,\n  #                    values = ~scales::rescale(.x, to = c(0, 0.5))) +\n  labs(y = 'Pr(X >= 10)') +\n  geom_hline(yintercept = 0.5, color = 'grey') +\n  geom_line(size = 0.8)\n\n\n\nHere, the relationship of detection probability vs CV is more complicated, and depends on the mean.\nAt low means, where detection is extremely unlikely, then the larger CV helps by making these unlikely tail events more probable.\nFairly unprobably events, on the order of say 1e-2 or 1e-3, may still be observed when we are testing hundreds or thousands of samples.\nRegarding when detection becomes more likely than not (grey line), we see something similar to single-read detection.\nThere is only a small increase for CV of 1 relative to Poisson, and a roughly 100-fold increase for CV of 3.\nIt should be easy to use the probability mass function of the neg binomial to examine how the mean and cv affect the probability of seeing 0 reads.\nReads fully covering a target (sub-)sequence\nIn some cases, we might be interested in how many reads that fully cover a particular subsequence, such as a particular 40-mer in the pathogen’s genome.\nSupose we have\n\\(\\mathcal M\\) total single-end reads, each of length \\(l\\)\nA fraction \\(P\\) are from the pathogen’s genome, or \\(M = \\mathcal M P\\) reads\nThe pathogen’s genome has length \\(G\\) bp\nWe are interested in the number or fraction of reads that cover a particular k-mer (of lenth \\(k\\), with \\(k \\le l\\)).\nReads that cover the k-mer are those whose start coincides with that of the k-mer or the \\(l - k\\) bp before it.\nTherefore, in the absence of within-genome bias in the read start, the fraction of pathogen reads that cover the focal k-mer is \\((l - k + 1) / G\\).\nThe expected number of reads covering a k-mer is (at least approximately)\n\\[\\begin{align}\n  \\mathcal M \\cdot P \\cdot \\frac{l - k + 1}{G}\n\\end{align}\\]\nNote, \\(P\\) is itself expected to be proportional to the pathogen’s genome, so genome size is not expected to have an effect on the number of times we see a particular k-mer.\nThis equation tells us something about the type of sequencing we should prefer depending on the circumstances.\nSuppose we are in a situation where reads that are at least 50bp long are sufficient to assign to the pathogen, and we just need to see the pathogen 10 times period to identify it.\nIn that case, we should prefer sequencing a sequencing option that maximizes the total number of reads, provided the reads are 50bp or longer.\n(This observation explains why you sometimes read/here that it is better to go with shorter-read options for certain ‘counting’ applications, such as gene expression studies)\nBut if we want to see a particular k-mer, then we want to maximize \\(\\mathcal M (l - k + 1)\\).\nFor \\(l \\gg k\\), this looks like \\(\\mathcal M l\\), or the total number of sequenced basepairs; at the values of \\(l\\) we are often considering with Illumina sequencing, there is also an additional premium beyond this on having longer reads, due to the need to completely cover the k-mer.\nFor Illumina sequencing, the sequencing options that maximize total bp also tend to maximize total read length.\nThere are options that give more reads, but shorter reads with total bp.\nThese options may make sense for certain applications, but I suspect we’ll often want longer reads as long as this maximizes total bp, as long as we can generate sequencing libraries with sufficiently long inserts.\nI believe this same basic tradeoff/choice behavior also occurs with Ultima Genomics’ sequencing platform.\nWith Ultima’s platform, you decide how many cycles (bp) to sequence.\nRunning for less cycles gets you shorter reads, but is cheaper per read, though not per bp.\nI have less of a handle on how this works with ONT, though I wouldn’t be surprised if a similar trade also exists there.\nTODO: find my notes from Metasub on this topic\nTargeted detection using qPCR or dPCR\nIn future, I’ll add notes on qPCR and dPCR, as standard methods for targeted detection of pathogens.\nAmplicon sequencing\nIn future, I’ll add some notes on amplicon sequencing, which is the primary method used for sequencing SARS-CoV-2 in wastewater (to identify and monitor variants).\nAmplicon sequencing involves using PCR to amplify one or more regions of the target organism’s genome, and then sequencing the resulting amplicons.\nFor SARS-CoV-2, it is common to use a tiling amplicon approach, where the genome is divided into overlapping amplicons of a fixed length, and each amplicon is amplified and sequenced simultaneously.\nWe are interested in questions such as at what point in its spread do we detect a new mutation or variant. The key difference from MGS is that what matters for amplicon sequencing is the relative abundance of the variant relative to the amplified genomes, rather than all genomes.\nThe same more detailed MGS model, described in my previous work, can be used to derive appropriate simplified models for MGS and amplicon sequencing, by appropriate choice of the efficiency parameters\nFor the simplified amplicon model, we can restrict the ‘background’ to just the target species (assuming negligible off-target amplification). This amounts to setting \\(B=0\\) for other species. We need to consider bias among variants within the target species, which can arise for example due to mutations in the primer-binding regions.\nNotes\nTransportation time\nEffective number of shedders contributing to a WWTP sample\nNon-human contributions to WWTP samples\nAbove, I assume that the WWTP samples are made up entirely from human microbiome shedding.\nBut there may be other contributions to the sample, including\ncontributions of microbial and non-microbial NA from other sources, such as pets, livestock, or wildlife, flushed or carried in by runoff or stormwater\ngrowth in the sewer system (including in the primary clarifier, for settled solids)\nTODO: consider whether it is plausible that these contribute a signficant relative abundance of MGS reads.\nFor growth in the sewers, review a recent college campus study that suggested this was a significant facter in campus samples at the start of the school year, when the system had been in low use.\nCan also look for literature on microbial dynamics in clarifiers, and talk to the scientists who study this stuff, for the case of settled solids.\nNoise/variability in the model\nThere is noise or randomness injected from many sources including transmission, shedding, sample processing, and sequencing.\nMy working hypothesis is that variability for basic detection in the WWTP setting is negligible because we’ll be detecting the epidemic at a sufficiently late point in time in a sufficiently large catchment that the random effects will largely average out in predictions of the point when detection occurs.\nIt will be important to assess the relevant parameters and see if/when we are likely in a regime where this hypothesis holds.\n\n\n\nDiekmann, Odo, Hans Heesterbeek, and Tom Britton. 2012. Mathematical Tools for Understanding Infectious Disease Dynamics. First. Princeton University Press. https://doi.org/10.23943/princeton/9780691155395.001.0001.\n\n\nHeng, Kevin, and Christian L. Althaus. 2020. “The Approximately Universal Shapes of Epidemic Curves in the Susceptible-Exposed-Infectious-Recovered (SEIR) Model.” Scientific Reports 10 (1): 19365. https://doi.org/10.1038/s41598-020-76563-8.\n\n\nKapo, Katherine E., Michael Paschka, Raghu Vamshi, Megan Sebasky, and Kathleen McDonough. 2017. “Estimation of U.S. Sewer Residence Time Distributions for National-Scale Risk Assessment of down-the-Drain Chemicals.” Science of The Total Environment 603–604 (December): 445–52. https://doi.org/10.1016/j.scitotenv.2017.06.075.\n\n\nKim, Sooyeol, Lauren C. Kennedy, Marlene K. Wolfe, Craig S. Criddle, Dorothea H. Duong, Aaron Topol, Bradley J. White, et al. 2022. “SARS-CoV-2 RNA Is Enriched by Orders of Magnitude in Primary Settled Solids Relative to Liquid Wastewater at Publicly Owned Treatment Works.” Environmental Science: Water Research & Technology 8 (4): 757–70. https://doi.org/10.1039/D1EW00826A.\n\n\nMa, Junling. 2020. “Estimating Epidemic Exponential Growth Rate and Basic Reproduction Number.” Infectious Disease Modelling 5 (January): 129–41. https://doi.org/10.1016/j.idm.2019.12.009.\n\n\nMorán-Tovar, Roberto, Henning Gruell, Florian Klein, and Michael Lässig. 2022. “Stochasticity of Infectious Outbreaks and Consequences for Optimal Interventions.” Journal of Physics A: Mathematical and Theoretical 55 (38): 384008. https://doi.org/10.1088/1751-8121/ac88a6.\n\n\nRothman, Jason A., Theresa B. Loveless, Joseph Kapcia, Eric D. Adams, Joshua A. Steele, Amity G. Zimmer-Faust, Kylie Langlois, et al. 2021. “RNA Viromics of Southern California Wastewater and Detection of SARS-CoV-2 Single-Nucleotide Variants.” Applied and Environmental Microbiology 87 (23): e01448–21. https://doi.org/10.1128/AEM.01448-21.\n\n\nSoller, Jeffrey, Wiley Jennings, Mary Schoen, Alexandria Boehm, Krista Wigginton, Raul Gonzalez, Katherine E. Graham, Graham McBride, Amy Kirby, and Mia Mattioli. 2022. “Modeling Infection from SARS-CoV-2 Wastewater Concentrations: Promise, Limitations, and Future Directions.” Journal of Water and Health 20 (8): 1197–1211. https://doi.org/10.2166/wh.2022.094.\n\n\nhttps://www.randomservices.org/random/brown/Drift.html↩︎\n",
    "preview": "posts/2023-02-06-basic-detection-theory/basic-detection-theory_files/figure-html5/unnamed-chunk-2-1.svg",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-22-concentration-experiments/",
    "title": "Concentration tests",
    "description": "Tests of wastewater concentration methods performed by Ari in May 2023.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-06-03",
    "categories": [
      "R"
    ],
    "contents": "\n\nContents\nImport data\nTests 1 and 2\nTest 3\n\nPlots\n\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(here)\n\nlibrary(furrr)\nplan(multisession, workers = 3)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n# Okabe Ito color scheme with amber for yellow; see https://easystats.github.io/see/reference/scale_color_okabeito.html\ncolors_oi <- grDevices::palette.colors()  \ncolors_oi['yellow'] <- \"#F5C710\"\n\ntoday <- format(Sys.time(), '%Y-%m-%d')\n\n\nImport data\nTests 1 and 2\n\n\nShow code\n\nsheet_url <- 'https://docs.google.com/spreadsheets/d/1hRsWlFYTywcZvYOLiidpFj7ywYbag2YJ8oVDO5MTRFs'\n\ntest1_raw <- googlesheets4::read_sheet(sheet_url, 'Test 1',\n  col_types = 'iccc')\ntest2_raw <- googlesheets4::read_sheet(sheet_url, 'Test 2',\n  col_types = 'icc') %>%\n  glimpse\n\nRows: 3\nColumns: 3\n$ Round    <int> 1, 2, 3\n$ Amicon   <chr> \"160 & 170\", \"210 & 260\", \"335 & 350\"\n$ Vivaspin <chr> \"400 & 450\", \"1000 & 1340\", \"2150 & 2480\"\n\nLet’s format these results in tidy format by pivoting the treatments (from columns to rows) and splitting the replicate measurements into their own rows.\n\n\nShow code\n\nparse_values <- function(x) {\n  x %>% \n    str_split(' & ') %>%\n    map(~set_names(., nm = seq_along(.)))\n}\n# parse_values('123 & 523')\nformat_results <- function(x) {\n  x %>%\n    rename(round = Round) %>%\n    pivot_longer(-round, names_to = 'treatment') %>%\n    mutate(\n      across(value, parse_values),\n      replicate_id = map(value, names)\n    ) %>%\n    unnest(c(replicate_id, value)) %>%\n    rename(volume = value) %>%\n    mutate(\n      across(volume, as.numeric)\n    )\n}\ntest1 <- test1_raw %>% format_results %>% glimpse\n\nRows: 24\nColumns: 4\n$ round        <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,…\n$ treatment    <chr> \"Influent without Reagents\", \"Influent without …\n$ volume       <dbl> 150, 150, 170, 170, 150, 150, 160, 160, 190, 19…\n$ replicate_id <chr> \"1\", \"2\", \"1\", \"2\", \"1\", \"2\", \"1\", \"2\", \"1\", \"2…\n\nShow code\n\ntest2 <- test2_raw %>% format_results %>% glimpse\n\nRows: 12\nColumns: 4\n$ round        <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n$ treatment    <chr> \"Amicon\", \"Amicon\", \"Vivaspin\", \"Vivaspin\", \"Am…\n$ volume       <dbl> 160, 170, 400, 450, 210, 260, 1000, 1340, 335, …\n$ replicate_id <chr> \"1\", \"2\", \"1\", \"2\", \"1\", \"2\", \"1\", \"2\", \"1\", \"2…\n\nTest 3\nThe results for this test follow a different format.\n\n\nShow code\n\nsheet_url <- 'https://docs.google.com/spreadsheets/d/1ChpXwS0azsDmoeHfyPYLBD158bHq7op-ld2EeukE_5g'\n\ntest3_samples_raw <- googlesheets4::read_sheet(sheet_url, 'Samples', col_types = 'cccccc')\ntest3_rounds_raw <- googlesheets4::read_sheet(sheet_url, 'Centrifugal filtration rounds', col_types = 'in')\ntest3_results_raw <- googlesheets4::read_sheet(sheet_url, 'Results', col_types = 'icn')\n\n\nTODO: Improve\n\n\nShow code\n\ntest3_samples <- test3_samples_raw %>%\n  janitor::clean_names() %>%\n  rename(\n    filter_size = filter_size_k_da\n  ) %>%\n  mutate(\n    across(c(treatment_group_id, filter_size), factor),\n  ) %>%\n  glimpse\n\nRows: 6\nColumns: 6\n$ sample_id          <chr> \"Amicon_30_None\", \"Amicon_30_Tween\", \"Ami…\n$ treatment_group_id <fct> 1, 2, 3, 4, 5, 6\n$ filter_type        <chr> \"Amicon\", \"Amicon\", \"Amicon\", \"Amicon\", \"…\n$ filter_size        <fct> 30, 30, 30, 50, 50, 50\n$ reagent            <chr> \"None\", \"Tween\", \"BE+NaNO3\", \"None\", \"Twe…\n$ notes              <chr> NA, NA, NA, NA, NA, NA\n\nShow code\n\n# test3_rounds_raw\ntest3_results <- test3_results_raw %>%\n  janitor::clean_names() %>%\n  rename(\n    retentate_volume = retentate_volume_u_l\n  ) %>%\n  glimpse\n\nRows: 30\nColumns: 3\n$ round            <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3…\n$ sample_id        <chr> \"Amicon_30_None\", \"Amicon_30_Tween\", \"Amico…\n$ retentate_volume <dbl> 190, 210, 400, 100, 160, 200, 330, 200, 580…\n\nShow code\n\ntest3 <- test3_results %>%\n  left_join(test3_samples, by = 'sample_id') %>%\n  mutate(\n    # treatment = str_glue('{reagent}:{filter_type}:{filter_size}'),\n    # treatment = str_c(reagant, filter_type, filter_size, sep = ':'),\n  ) %>%\n  glimpse\n\nRows: 30\nColumns: 8\n$ round              <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3,…\n$ sample_id          <chr> \"Amicon_30_None\", \"Amicon_30_Tween\", \"Ami…\n$ retentate_volume   <dbl> 190, 210, 400, 100, 160, 200, 330, 200, 5…\n$ treatment_group_id <fct> 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2,…\n$ filter_type        <chr> \"Amicon\", \"Amicon\", \"Amicon\", \"Amicon\", \"…\n$ filter_size        <fct> 30, 30, 30, 50, 50, 50, 30, 30, 30, 50, 5…\n$ reagent            <chr> \"None\", \"Tween\", \"BE+NaNO3\", \"None\", \"Twe…\n$ notes              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\nPlots\n\n\nShow code\n\ntest1 %>%\n  mutate(across(round, as.ordered)) %>%\n  ggplot(aes(y = round, x = volume, color = replicate_id)) +\n  theme_minimal_hgrid() +\n  theme(legend.position = 'bottom') +\n  scale_color_manual(values = colors_oi[2:3] %>% unname) +\n  expand_limits(x = 0) +\n  facet_wrap(~treatment, ncol = 1) +\n  labs(x = 'Volume (uL)', y = 'Round', color = 'Replicate') +\n  plot_annotation('Test 1') +\n  geom_vline(xintercept = 150, color = 'grey') +\n  geom_quasirandom(groupOnX = FALSE)\n\n\n\n\n\nShow code\n\ntest2 %>%\n  mutate(across(round, as.ordered)) %>%\n  ggplot(aes(y = round, x = volume, color = replicate_id)) +\n  theme_minimal_hgrid() +\n  theme(legend.position = 'bottom') +\n  scale_color_manual(values = colors_oi[2:3] %>% unname) +\n  expand_limits(x = 0) +\n  facet_wrap(~treatment, ncol = 1) +\n  labs(x = 'Volume (uL)', y = 'Round', color = 'Replicate') +\n  plot_annotation('Test 2') +\n  geom_vline(xintercept = 150, color = 'grey') +\n  geom_quasirandom(groupOnX = FALSE)\n\n\n\n\n\nShow code\n\ntest3 %>%\n  mutate(\n         across(round, as.ordered),\n         across(c(filter_size, reagent), as.factor)\n         ) %>%\n  ggplot(aes(y = round, x = retentate_volume)) +\n  theme_minimal_hgrid() +\n  theme(legend.position = 'bottom') +\n  scale_color_manual(values = colors_oi[2:3] %>% unname) +\n  expand_limits(x = 0) +\n  facet_wrap(~ filter_size : reagent, ncol = 1) +\n  labs(x = 'Volume (uL)', y = 'Round', color = 'Replicate') +\n  plot_annotation('Test 3') +\n  geom_vline(xintercept = 150, color = 'grey') +\n  geom_quasirandom(groupOnX = FALSE)\n\n\n\nTODO: standardsize col names\ntreatment\nretentate vol\n\n\n\n",
    "preview": {},
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-31-quantify-phagemid-stock/",
    "title": "Analyze phagemid qPCR data for creating spike-in stock",
    "description": {},
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-06-03",
    "categories": [
      "r",
      "qPCR"
    ],
    "contents": "\n\nContents\nFirst qPCR run\nPlot the amplification curves\n\nSecond qPCR run\nPlot the amplification curves\nExamine the Ct values\nBayesian estimate of efficiency\n\nEstimate the Ct for the experiment dilution factor\n\n\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(here)\n\nlibrary(knitr)\n\nlibrary(broom)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n# Okabe Ito color scheme with amber for yellow; see https://easystats.github.io/see/reference/scale_color_okabeito.html\ncolors_oi <- grDevices::palette.colors()  \ncolors_oi['yellow'] <- \"#F5C710\"\n\n# Custon qPCR helpers\nsource('_functions.R')\n\n\nWe are interested in using some old stocks of phagemid tracers as spike-in controls for an upcoming experiment.\nFor that, we need an estimate of the tracer concentration in the stocks, in terms of Cq values and/or particle concentration.\nAri did two qPCRs on 2023-05-31; see the experiment folder in Drive.\nFirst qPCR run\nStore in local data file for now; could perhaps put in top level data directory.\n\n\nShow code\n\nurl <- \"https://docs.google.com/spreadsheets/d/1VV7B6m-gy-VqTVpi9feP0f-8QUd5ZLNR\"\ndir_create('_data')\ndata_file <- '_data/5-31 28 Test.xls'\nif (!file_exists(data_file)) {\n  googledrive::drive_download(url, path = data_file, overwrite = FALSE)\n}\n\n\nImport and clean as in previous analyses,\n\n\nShow code\n\nresults <- data_file %>%\n  read_qpcr_results %>%\n    # Remove some unused columns\n  select(-c(omit, task, reporter, quencher, starts_with('quantity'),\n            y_intercept:efficiency, target_color)) %>%\n  glimpse\n\nRows: 9\nColumns: 21\n$ well                   <int> 1, 2, 3, 13, 14, 15, 25, 26, 27\n$ well_position          <chr> \"A1\", \"A2\", \"A3\", \"B1\", \"B2\", \"B3\", \"…\n$ sample_name            <chr> \"Blank\", \"NTC_10\", \"28\", \"Blank\", \"NT…\n$ target_name            <fct> Blank, B1_NTC, 10, Blank, B1_NTC, 10,…\n$ ct                     <dbl> 33.604317, NA, 4.515600, 6.221345, NA…\n$ ct_mean                <chr> \"16.076345443725586\", NA, \"4.59870433…\n$ ct_sd                  <chr> \"15.218825340270996\", NA, \"0.08602836…\n$ automatic_ct_threshold <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ct_threshold           <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n$ automatic_baseline     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ baseline_start         <int> 3, 3, 3, 3, 3, 3, 3, 3, 3\n$ baseline_end           <int> 27, 39, 3, 6, 20, 3, 6, 39, 3\n$ amp_status             <chr> \"No Amp\", \"No Amp\", \"Amp\", \"Inconclus…\n$ comments               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA\n$ cq_conf                <dbl> 0.0000000, 0.0000000, 0.9364234, 0.24…\n$ cqconf                 <chr> \"Y\", \"Y\", \"N\", \"Y\", \"Y\", \"N\", \"Y\", \"Y…\n$ expfail                <chr> \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"Y…\n$ highsd                 <chr> \"Y\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"Y\", \"N…\n$ spike                  <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N…\n$ noamp                  <chr> \"N\", \"Y\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"Y…\n$ prfdrop                <chr> \"Y\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N…\n\nCheck the threshold settings,\n\n\nShow code\n\nresults %>% count(automatic_ct_threshold, ct_threshold)\n\n# A tibble: 1 × 3\n  automatic_ct_threshold ct_threshold     n\n  <lgl>                         <dbl> <int>\n1 FALSE                           0.2     9\n\nShow code\n\nct_threshold = 0.2\n\n\nLoad the amplification data — the relative florescence values (Rn and Delta Rn) — and remove the unused wells (based on those with no sample name).\nFor future reference, the line at which the data starts can vary between files; here it is 43, but in a previous experiment it was 40.\n\n\nShow code\n\namp <- data_file %>%\n  read_qpcr_amplification(start = 43, results = TRUE) %>%\n  # Remove some unused columns\n  select(-c(omit, task, reporter, quencher, starts_with('quantity'),\n            y_intercept:efficiency, target_color)) %>%\n  filter(!is.na(sample_name)) %>%\n  glimpse\n\nRows: 360\nColumns: 24\n$ well                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ well_position          <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"…\n$ cycle                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n$ target_name            <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"…\n$ rn                     <dbl> 3.744115, 3.723282, 3.720500, 3.73922…\n$ delta_rn               <dbl> -1.644028e-02, -3.789984e-02, -4.1309…\n$ sample_name            <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"…\n$ ct                     <dbl> 33.60432, 33.60432, 33.60432, 33.6043…\n$ ct_mean                <chr> \"16.076345443725586\", \"16.07634544372…\n$ ct_sd                  <chr> \"15.218825340270996\", \"15.21882534027…\n$ automatic_ct_threshold <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ct_threshold           <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n$ automatic_baseline     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ baseline_start         <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ baseline_end           <int> 27, 27, 27, 27, 27, 27, 27, 27, 27, 2…\n$ amp_status             <chr> \"No Amp\", \"No Amp\", \"No Amp\", \"No Amp…\n$ comments               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cq_conf                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cqconf                 <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ expfail                <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ highsd                 <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ spike                  <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ noamp                  <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ prfdrop                <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n\nAnd get the baseline coordinates for plotting,\n\n\nShow code\n\nbaselines <- results %>%\n  pivot_longer(\n    cols = c(baseline_start, baseline_end),\n    names_to = 'baseline_boundary',\n    values_to = 'cycle',\n    names_prefix = 'baseline_',\n  ) %>%\n  left_join(\n    amp %>% select(well_position, cycle, rn, delta_rn), \n    by = c('well_position', 'cycle')\n  ) %>%\n  glimpse\n\nRows: 18\nColumns: 23\n$ well                   <int> 1, 1, 2, 2, 3, 3, 13, 13, 14, 14, 15,…\n$ well_position          <chr> \"A1\", \"A1\", \"A2\", \"A2\", \"A3\", \"A3\", \"…\n$ sample_name            <chr> \"Blank\", \"Blank\", \"NTC_10\", \"NTC_10\",…\n$ target_name            <fct> Blank, Blank, B1_NTC, B1_NTC, 10, 10,…\n$ ct                     <dbl> 33.604317, 33.604317, NA, NA, 4.51560…\n$ ct_mean                <chr> \"16.076345443725586\", \"16.07634544372…\n$ ct_sd                  <chr> \"15.218825340270996\", \"15.21882534027…\n$ automatic_ct_threshold <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ct_threshold           <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n$ automatic_baseline     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ amp_status             <chr> \"No Amp\", \"No Amp\", \"No Amp\", \"No Amp…\n$ comments               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cq_conf                <dbl> 0.0000000, 0.0000000, 0.0000000, 0.00…\n$ cqconf                 <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"Y\", \"Y…\n$ expfail                <chr> \"N\", \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N…\n$ highsd                 <chr> \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"Y…\n$ spike                  <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ noamp                  <chr> \"N\", \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N…\n$ prfdrop                <chr> \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"Y…\n$ baseline_boundary      <chr> \"start\", \"end\", \"start\", \"end\", \"star…\n$ cycle                  <int> 3, 27, 3, 39, 3, 3, 3, 6, 3, 20, 3, 3…\n$ rn                     <dbl> 3.720500, 3.710590, 5.606404, 5.66603…\n$ delta_rn               <dbl> -0.0413092077, -0.0662675351, -0.0023…\n\n\n\nShow code\n\nresults %>%\n  count(sample_name) %>%\n  arrange(desc(n)) %>% \n  print(n = Inf)\n\n# A tibble: 3 × 2\n  sample_name     n\n  <chr>       <int>\n1 28              3\n2 Blank           3\n3 NTC_10          3\n\nPlot the amplification curves\n\n\nShow code\n\ndelta_rn_min <- 1e-3\n\np1 <- amp %>%\n  filter(!is.na(target_name)) %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  scale_color_brewer(type = 'qual') +\n  geom_point(data = baselines, aes(shape = baseline_boundary), size = 3) +\n  scale_shape_manual(values = c(1, 4)) +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nNote the weird stuff going on with the blanks, particularly the one that appears to grow steadily. This may be an artefact of baseline subtraction, so let’s look at the raw Rn.\n\n\nShow code\n\np3 <- amp %>%\n  filter(!is.na(target_name)) %>%\n  ggplot(aes(cycle, rn, color = target_name)) +\n  geom_line(aes(group = well)) +\n  scale_color_brewer(type = 'qual') +\n  geom_point(data = baselines, aes(shape = baseline_boundary), size = 3) +\n  scale_shape_manual(values = c(1, 4)) +\n  labs(y = 'Rn', x = 'Cycle', color = 'Target')\np4 <- p3 +\n  scale_y_log10()\np4 / p2\n\n\n\nComparing Delta Rn to Rn shows that the blanks didn,t have amplication and the apparent growth is just an artefact of baseline subtraction, as I hypothesized.\nNote, the dRn trajectories of the target samples are already saturating at the threshold point.\n\n\nShow code\n\ndelta_rn_min <- 1e-3\n\np1 <- amp %>%\n  filter(target_name == '10', cycle <= 8) %>%\n  pivot_longer(c(rn, delta_rn)) %>%\n  ggplot(aes(cycle, value)) +\n  facet_wrap(~name, scales = 'free_y') +\n  geom_line(aes(group = well)) +\n  # geom_hline(yintercept = expected_ct_threshold, alpha = 0.3) +\n  scale_color_brewer(type = 'qual') +\n  scale_shape_manual(values = c(1, 4)) +\n  labs(y = '(Delta) Rn', x = 'Cycle')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nI’m confused about why the rn curves aren’t being affected by the log scaling; something seems to be off with the faceting and y-axis scaling interaction.\nI’m concerned that baseline subtraction may not be functioning properly given the high amount of phagemid that is present.\nThe Ct mean as reported by the software is 4.599.\nSecond qPCR run\nStore in local data file for now; could perhaps put in top level data directory.\n\n\nShow code\n\nurl <- \"https://docs.google.com/spreadsheets/d/1NdmA43mYqXuHeu3E_QJJ40NijfXPVWwc\"\ndata_file <- '_data/5-31 28 Test 2.xls'\nif (!file_exists(data_file)) {\n  googledrive::drive_download(url, path = data_file, overwrite = FALSE)\n}\n\n\nImport and clean as in previous analyses.\nHere, some sample names are numerical and indicate the dilution factor, so I’ll parse those.\n\n\nShow code\n\nresults <- data_file %>%\n  read_qpcr_results %>%\n  # Remove some unused columns\n  select(-c(omit, task, reporter, quencher, starts_with('quantity'),\n            y_intercept:efficiency, target_color)) %>%\n  mutate(\n    dilution_factor = str_replace(sample_name, ',', '') %>% as.numeric,\n  ) %>%\n  glimpse\n\nRows: 21\nColumns: 20\n$ well                   <int> 1, 2, 3, 4, 5, 6, 7, 13, 14, 15, 16, …\n$ well_position          <chr> \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"…\n$ sample_name            <chr> \"Blank\", \"NTC_10\", \"1\", \"10\", \"100\", …\n$ target_name            <fct> Blank, B1_NTC, 10, 10, 10, 10, 10, Bl…\n$ ct                     <dbl> NA, 24.986382, 4.538310, 7.290561, 10…\n$ ct_mean                <chr> NA, \"25.81987190246582\", \"4.574169158…\n$ ct_sd                  <chr> NA, \"0.73780524730682373\", \"0.0433460…\n$ automatic_ct_threshold <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ct_threshold           <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n$ automatic_baseline     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ baseline_start         <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ baseline_end           <int> 39, 21, 3, 4, 6, 10, 12, 16, 19, 3, 4…\n$ amp_status             <chr> \"Inconclusive\", \"Amp\", \"Amp\", \"Amp\", …\n$ comments               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cq_conf                <dbl> 0.0000000, 0.9817477, 0.9352701, 0.95…\n$ cqconf                 <chr> \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"Y…\n$ expfail                <chr> \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ highsd                 <chr> \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ spike                  <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"Y…\n$ dilution_factor        <dbl> NA, NA, 1, 10, 100, 1000, 10000, NA, …\n\n\n\nShow code\n\nresults %>%\n  count(sample_name, dilution_factor) %>%\n  arrange(desc(n)) %>% \n  print(n = Inf)\n\n# A tibble: 7 × 3\n  sample_name dilution_factor     n\n  <chr>                 <dbl> <int>\n1 1                         1     3\n2 1,000                  1000     3\n3 10                       10     3\n4 10,000                10000     3\n5 100                     100     3\n6 Blank                    NA     3\n7 NTC_10                   NA     3\n\nCheck the threshold settings,\n\n\nShow code\n\nresults %>% count(automatic_ct_threshold, ct_threshold)\n\n# A tibble: 1 × 3\n  automatic_ct_threshold ct_threshold     n\n  <lgl>                         <dbl> <int>\n1 FALSE                           0.2    21\n\nShow code\n\nct_threshold = 0.2\n\n\nCheck the well breakdown by sample name,\nLoad the amplification data — the relative florescence values (Rn and Delta Rn) — and remove the unused wells (based on those with no sample name).\nFor future reference, the line at which the data starts can vary between files; here it is 43, but in a previous experiment it was 40.\n\n\nShow code\n\namp <- data_file %>%\n  read_qpcr_amplification(start = 43, results = TRUE) %>%\n  # Remove some unused columns\n  select(-c(omit, task, reporter, quencher, starts_with('quantity'),\n            y_intercept:efficiency, target_color)) %>%\n  filter(!is.na(sample_name)) %>%\n  mutate(\n    dilution_factor = str_replace(sample_name, ',', '') %>% as.numeric,\n  ) %>%\n  glimpse\n\nRows: 840\nColumns: 23\n$ well                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ well_position          <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"…\n$ cycle                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n$ target_name            <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"…\n$ rn                     <dbl> 3.925871, 3.929747, 3.918463, 3.89655…\n$ delta_rn               <dbl> 0.136036664, 0.136512622, 0.121829681…\n$ sample_name            <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"…\n$ ct                     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ct_mean                <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ct_sd                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ automatic_ct_threshold <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ct_threshold           <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n$ automatic_baseline     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ baseline_start         <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ baseline_end           <int> 39, 39, 39, 39, 39, 39, 39, 39, 39, 3…\n$ amp_status             <chr> \"Inconclusive\", \"Inconclusive\", \"Inco…\n$ comments               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cq_conf                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cqconf                 <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ expfail                <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n$ highsd                 <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ spike                  <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ dilution_factor        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\nAnd get the baseline coordinates for plotting,\n\n\nShow code\n\nbaselines <- results %>%\n  pivot_longer(\n    cols = c(baseline_start, baseline_end),\n    names_to = 'baseline_boundary',\n    values_to = 'cycle',\n    names_prefix = 'baseline_',\n  ) %>%\n  left_join(\n    amp %>% select(well_position, cycle, rn, delta_rn), \n    by = c('well_position', 'cycle')\n  ) %>%\n  glimpse\n\nRows: 42\nColumns: 22\n$ well                   <int> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7…\n$ well_position          <chr> \"A1\", \"A1\", \"A2\", \"A2\", \"A3\", \"A3\", \"…\n$ sample_name            <chr> \"Blank\", \"Blank\", \"NTC_10\", \"NTC_10\",…\n$ target_name            <fct> Blank, Blank, B1_NTC, B1_NTC, 10, 10,…\n$ ct                     <dbl> NA, NA, 24.986382, 24.986382, 4.53831…\n$ ct_mean                <chr> NA, NA, \"25.81987190246582\", \"25.8198…\n$ ct_sd                  <chr> NA, NA, \"0.73780524730682373\", \"0.737…\n$ automatic_ct_threshold <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ct_threshold           <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n$ automatic_baseline     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ amp_status             <chr> \"Inconclusive\", \"Inconclusive\", \"Amp\"…\n$ comments               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cq_conf                <dbl> 0.0000000, 0.0000000, 0.9817477, 0.98…\n$ cqconf                 <chr> \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ expfail                <chr> \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ highsd                 <chr> \"N\", \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N…\n$ spike                  <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ dilution_factor        <dbl> NA, NA, NA, NA, 1, 1, 10, 10, 100, 10…\n$ baseline_boundary      <chr> \"start\", \"end\", \"start\", \"end\", \"star…\n$ cycle                  <int> 3, 39, 3, 21, 3, 3, 3, 4, 3, 6, 3, 10…\n$ rn                     <dbl> 3.918463, 4.017810, 6.315196, 6.36331…\n$ delta_rn               <dbl> 0.1218296811, 0.0988022089, -0.000196…\n\nPlot the amplification curves\nPlot with the baselines,\n\n\nShow code\n\ndelta_rn_min <- 1e-3\n\np1 <- amp %>%\n  filter(!is.na(target_name)) %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  scale_color_brewer(type = 'qual') +\n  geom_point(data = baselines, aes(shape = baseline_boundary), size = 3) +\n  scale_shape_manual(values = c(1, 4)) +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nExamine the Ct values\n\n\nShow code\n\nresults %>%\n  mutate(rel_conc = 1 / dilution_factor) %>%\n  ggplot(aes(rel_conc, ct)) +\n  labs(x = 'Concentration relative to stock') +\n  scale_x_log10() +\n  stat_smooth(method = 'lm') +\n  geom_point()\n\n\n\nExamine the linear fit,\n\n\nShow code\n\nresults1 <- results %>%\n  mutate(\n    rel_conc = 1 / dilution_factor,\n    rel_conc_log10 = log10(rel_conc),\n  )\n\nfit <- lm(ct ~ rel_conc_log10, data = results1)\nfit %>% summary\n\n\nCall:\nlm(formula = ct ~ rel_conc_log10, data = results1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43246 -0.16149  0.03244  0.14476  0.44235 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.43792    0.10797   41.10 3.77e-15 ***\nrel_conc_log10 -3.28510    0.04408  -74.53  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2414 on 13 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.9977,    Adjusted R-squared:  0.9975 \nF-statistic:  5554 on 1 and 13 DF,  p-value: < 2.2e-16\n\nEstimate the efficiency,\n\n\nShow code\n\nx <- fit %>% broom::tidy()\nslope <- coef(fit)['rel_conc_log10']\nefficiency_estimate <- 10^(-1/slope) - 1\nefficiency_estimate \n\nrel_conc_log10 \n        1.0156 \n\nBayesian estimate of efficiency\n\n\nShow code\n\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\n\nlibrary(ggdist)\n\n\n\n\nShow code\n\nstan_fit <- stan_glm(\n  ct ~ rel_conc_log10, \n  data = results1,\n)\nstan_fit\n\nstan_glm\n family:       gaussian [identity]\n formula:      ct ~ rel_conc_log10\n observations: 15\n predictors:   2\n------\n               Median MAD_SD\n(Intercept)     4.4    0.1  \nrel_conc_log10 -3.3    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.3    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nUse the posterior samples of the slope param to get the posterior of the efficiency,\n\n\nShow code\n\nslope_post <- rstan::extract(stan_fit$stanfit)$beta\nefficiency_post <- 10^(-1/slope_post) - 1\n\n\n\n\nShow code\n\nefficiency_post %>% median_hdci(.width = 0.9)\n\n         y      ymin    ymax .width .point .interval\n1 1.015849 0.9779378 1.04701    0.9 median      hdci\n\n\n\nShow code\n\nefficiency_post %>% qplot +\n  labs(x = 'Efficiency', y = 'Posterior density') +\n  geom_vline(xintercept = median(efficiency_post), color = 'darkred')\n\n\n\nThe median is slighty larger than 1 and a value of 1 or less is quite plausible.\nHowever, it would be useful to think about whether my concern with the undiluted sample above would tend to cause the efficiency estimate to increase or decrease.\nnotes\nCould consider a better prior on the slope using domain info.\nEstimate the Ct for the experiment dilution factor\nIn the actual experiment, we used a dilution factor of 20000 for the spike-in stock.\nOf this spike-in stock, 100 ul was added to each 40 mL sample (an additional 400-fold dilution), for a total dilution factor of 8e6 in the experiment samples relative to the initial stock.\nThe sample processing added some concentration:\nOn the order of 40 mL of sample was used (with little loss), and eluted into 80 uL, for a concentration factor of ~500.\nHence the final elutant is only diluted by a factor of 1.6e4 relative to the initial stock.\nWhat are the corresponding Ct values?\n\n\nShow code\n\nnewdata <- tibble(\n  rel_conc_log10 = log10(1 / c(2e4, 8e6, 1.6e4))\n)\npredict(stan_fit, newdata = newdata)\n\n       1        2        3 \n18.56548 27.11276 18.24715 \n\nTherefore we expect Ct values of around 18.6 for the spike-in stock, 27.1 for the spiked sample, and 18.2 for the extracted NA solution.\n\n\n\n",
    "preview": "posts/2023-05-31-quantify-phagemid-stock/main_files/figure-html5/unnamed-chunk-8-1.svg",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-25-stock-quantification/",
    "title": "Review the 2023-04 stock quantification experiment",
    "description": {},
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-05-01",
    "categories": [],
    "contents": "\n\nContents\nQuant-iT data\nTODO: Combine the sample meta\nVisualize\nPlate format\nData from different chromatics\n\nAnalysis\nStandard curve\n\nDiscussion\n\n\n\nshow\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(here)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n# Okabe Ito color scheme with amber for yellow; see https://easystats.github.io/see/reference/scale_color_okabeito.html\ncolors_oi <- grDevices::palette.colors()  \ncolors_oi['yellow'] <- \"#F5C710\"\n\n\nExperiment described in Drive here\nQuant-iT data\nNote that the file extension indicates a CSV file, but the file is not a properly formulated tabular file.\nSo our first task is to parse the data in this file.\nIn addition, there are different sets of data in this file, which we will need to understand and document.\nIt will also be helfpul to plot a plate-based view of the florescence values to compare to our expected plate layout.\nWe’ll also want to import the info for each well, to link to the data\n\n\nshow\n\nfn <- here('_data/nao/2023-04-24-stock-quantification/QuantIt Protocol_230425_1137.csv')\n\ndata_raw <- read_lines(fn)\ndata_raw %>% head(20)\n\n [1] \"Testname: QuantIt Protocol\"                              \n [2] \"Date: 4/25/2023  Time: 11:28:38 AM\"                      \n [3] \"ID1: 107960  ID2:   ID3: \"                               \n [4] \"No. of Channels / Multichromatics: 21\"                   \n [5] \"No. of Intervals: 1\"                                     \n [6] \"Configuration: Fluorescence\"                             \n [7] \"Used filter settings and gain values:\"                   \n [8] \"  No. of scan procedures: 1                      \"       \n [9] \"  1: 480-10/510-10 --> 480-10/530-10                2598\"\n[10] \"  1: No. of scan values: 21                         \"    \n[11] \"  2: -                                              \"    \n[12] \"  2: -                                              \"    \n[13] \"Focal height [mm]: 13.5\"                                 \n[14] \"End_of_header\"                                           \n[15] \"\"                                                        \n[16] \"Chromatic: 1\"                                            \n[17] \"Interval: 1\"                                             \n[18] \"Time [s]: 0\"                                             \n[19] \"A01:\\t151015.0000\"                                       \n[20] \"A02:\\t154240.0000\"                                       \n\nThe data starts with a header, which ends with a line ‘End_of_header’, adn then has multiple blocks of data delimited by blank lines.\nEach block of data starts with three lines giving values for Chromatic, Intervla, and Time variables, and then gives the values for each well.\nThe values for the well are in the format [Well id]:\\t[Value].\nOur process for importing the data might look like:\nSplit the data into blocks based on empty lines; the first block contains the header/metadata.\nSave and (optionally) process the block of header data for useful metadata\nApply a parsing function to the main data blocks to return a single data frame\nLet’s work within a tibble,\n\n\nshow\n\ndata_raw_tbl <- tibble(raw = data_raw)\n\n\nFirst, I’ll collapse the different data blocks into separate data frames,\n\n\nshow\n\ndata_nest <- data_raw_tbl %>%\n  mutate(\n    is_blank = nchar(raw) == 0L,\n    block = cumsum(is_blank)\n  ) %>%\n  filter(!is_blank) %>%\n  select(-is_blank) %>%\n  group_by(block) %>%\n  nest()\n\n\nI’ll save the header information in a separate data frame in case we wish to pull any info from this later on.\n\n\nshow\n\nheader <- data_nest %>% filter(block == 0) %>% pull(data) %>% .[[1]]\n\n\nNext, I’ll define a function to import a data block,\n\n\nshow\n\nimport_block <- function(x) {\n  # metadata is contained in the first three columns, in the format\n  # 'Chromatic: 1'\n  meta <- x %>% \n    slice_head(n = 3) %>% \n    separate(col = raw, into = c('name', 'value'), sep = \": \") %>%\n    pivot_wider() %>%\n    janitor::clean_names()\n  # main data follows, in the format \"A01:\\t151015.0000\"\n  main <- x %>% \n    slice_tail(n = -3) %>% \n    separate(col = raw, into = c('well', 'value'), sep = \":\\t\") %>%\n    mutate(across(value, as.numeric))\n  crossing(meta, main)\n}\n\ntest_block <- data_nest %>% filter(block == 1) %>% pull(data) %>% .[[1]]\n\nimport_block(test_block)\n\n# A tibble: 96 × 5\n   chromatic interval time_s well   value\n   <chr>     <chr>    <chr>  <chr>  <dbl>\n 1 1         1        0      A01   151015\n 2 1         1        0      A02   154240\n 3 1         1        0      A03     1400\n 4 1         1        0      A04     7566\n 5 1         1        0      A05     1363\n 6 1         1        0      A06     1557\n 7 1         1        0      A07     1521\n 8 1         1        0      A08     1444\n 9 1         1        0      A09     1605\n10 1         1        0      A10     1681\n# … with 86 more rows\n\nProcess all data blocks into one data frame,\n\n\nshow\n\ndata_proc <- data_nest %>% \n  filter(block != 0) %>%\n  mutate(data = map(data, import_block)) %>%\n  unnest(data) %>%\n  separate(well, into = c('row', 'column'), sep = 1, remove = FALSE) %>%\n  ungroup %>%\n  mutate(\n    across(chromatic, ~fct_inseq(.x, ordered = TRUE)),\n    # for column, coerce to integer first to remove leading zeros\n    across(column, ~as.integer(.x) %>% ordered),\n    across(row, ordered, levels = LETTERS[1:8]),\n  )\n\n\nTODO: Combine the sample meta\n\n\nshow\n\nss <- 'https://docs.google.com/spreadsheets/d/1HXdFckwlqdy4NQ0NSfUcB9_nN-n3xPJH2fAsK9Mi-Uc'\n\nmeta <- googlesheets4::read_sheet(ss, sheet = 2, na = 'NA') %>% glimpse\n\nRows: 13\nColumns: 7\n$ well        <chr> \"A01\", \"B01\", \"C01\", \"D01\", \"E01\", \"A02\", \"B02\",…\n$ name_ari    <chr> \"Std 1\", \"Std 2\", \"Std 3\", \"Std 4\", \"Std 5\", \"St…\n$ type        <chr> \"standard\", \"standard\", \"standard\", \"standard\", …\n$ vol_te      <dbl> 0, 50, 90, 99, 100, 0, 50, 90, 99, 100, 98, 98, …\n$ vol_na      <dbl> 100, 50, 10, 1, 0, 100, 50, 10, 1, 0, 2, 2, 2\n$ vol_reagent <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100…\n$ conc_ari    <dbl> 1000, 500, 100, 10, 0, 1000, 500, 100, 10, 0, NA…\n\n\n\nshow\n\ndata_proc <- data_proc %>%\n  left_join(meta, by = 'well')\n\n\nVisualize\nPlate format\nVisualize the first chromatic\n\n\nshow\n\ndata_proc %>%\n  filter(chromatic == 1) %>%\n  mutate(\n    # Reverse the row order so that the plate is oriented correctly in the plot\n    across(row, fct_rev)\n  ) %>%\n  ggplot(aes(y = row, x = column, fill = log10(value))) +\n  coord_fixed() +\n  geom_tile() +\n  geom_text(aes(label = log10(value) %>% round(1)), size = 5, color = 'white') +\n  # scale_fill_brewer(type = \"qual\", palette = 2) + \n  labs(title = \"Plate layout\") +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\nData from different chromatics\n\n\nshow\n\ndata_proc %>%\n  filter(column %in% c(1, 2, 3, 4)) %>%\n  ggplot(aes(x = value, y = well, color = chromatic)) +\n  scale_x_log10() +\n  geom_point()\n\n\n\nCheck for the standard curve\n\n\nshow\n\np1 <- data_proc %>%\n  filter(type == 'standard') %>%\n  ggplot(aes(x = conc_ari, y = value, color = chromatic)) +\n  geom_point()\np2 <- p1 +\n  scale_x_log10() +\n  scale_y_log10()\np1 / p2\n\n\n\nNow let’s look at the SC for each chromatic, with the (geometric) mean value of the target sample marked.\n\n\nshow\n\nmean_gm <- function(x) {\n  x %>% log %>% mean %>% exp\n}\n\ntarget_mean <- data_proc %>%\n  filter(type == 'target') %>%\n  summarize(.by = chromatic,\n    across(value, mean_gm)\n  )\ntarget_mean %>% \n  knitr::kable(digits = 0, title = 'Gm. mean of target sample') %>%\n  kableExtra::kable_styling(full_width = FALSE)\n\n\nchromatic\n\n\nvalue\n\n\n1\n\n\n7522\n\n\n2\n\n\n7816\n\n\n3\n\n\n8295\n\n\n4\n\n\n8340\n\n\n5\n\n\n8380\n\n\n6\n\n\n8712\n\n\n7\n\n\n8827\n\n\n8\n\n\n9177\n\n\n9\n\n\n9431\n\n\n10\n\n\n9475\n\n\n11\n\n\n9564\n\n\n12\n\n\n9415\n\n\n13\n\n\n9415\n\n\n14\n\n\n9146\n\n\n15\n\n\n9567\n\n\n16\n\n\n9401\n\n\n17\n\n\n9624\n\n\n18\n\n\n9610\n\n\n19\n\n\n9738\n\n\n20\n\n\n9998\n\n\n21\n\n\n10174\n\n\n\n\nshow\n\ndata_proc %>%\n  filter(type == 'standard') %>%\n  ggplot(aes(x = conc_ari, y = value, color = chromatic)) +\n  facet_wrap(~chromatic) +\n  scale_x_log10() +\n  scale_y_log10() +\n  geom_hline(data = target_mean, aes(yintercept = value), linetype = 3) +\n  geom_point()\n\n\n\nNote, I have not estimated and subtracted the background; possibly should, but want to understand why this is needed/suggested.\nFrom these plots, it does seem like there is a non-linear relationship between the concentration and the flourescence signal.\nWe should not see this if the signal was RFU = background + slope * concentration.\nSimply subtracting the value of the blank from each sample will not fix this issue.\nFor each chromatic, the mean florescence value for the target sample appears in the non-linear part of the standard curve.\nTo me, this suggests that the SC seen here may not be appropriate for a target sample in this range.\nThe target sample was diluted 50-fold; but if it were not diluted, it would be in a good part of the SC.\nAnalysis\nStandard curve\nThe assay manual shows using a linear fit of non-transformed values.\nIs this appropriate?\n\n\n\nDiscussion\nTODO Question - what are the other chromatics?\nTODO Baseline subtraction\nTODO Standard curve estimation\nTODO Power analysis - value of replicates for the SC and the target (?)\n\n\n\n",
    "preview": "posts/2023-04-25-stock-quantification/main_files/figure-html5/unnamed-chunk-10-1.svg",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-08-effect-of-noise-on-egd/",
    "title": "Examining the effect of noise on Exponential Growth Detection",
    "description": {},
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "EGD",
      "R",
      "theory"
    ],
    "contents": "\n\nContents\nBackground\nAnalysis\nNext steps\nDiscussion\nSession info\n\n\n\n# set of data analysis and viz tools\nlibrary(tidyverse)\nlibrary(furrr)\nplan(multisession, workers = 3)\n# file system helpers\nlibrary(fs)\n# specifying locations within a project\nlibrary(here)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\nlibrary(ggdist)\ntheme_set(theme_cowplot())\n\n# stats\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\nlibrary(broom.mixed)\n\n\nBackground\nWe’ve done some examination of the ability to detect exponential increase when there is no noise beyond the Poisson error process associated with sequencing.\nBut we know there is additional noise due to processes such as shedding, sample collection, and sample processing.\nQuestions this analysis considers:\nHow does increasing the amount of noise in the relative abundance of pathogen in the sequencing data reduce our power to estimate its growth rate or infer that it is exponentially increasing?\nHow much additional sequencing effort is required to make up for a given increase in noise/dispersion? this is a naive question — we know from statistical principles that you can’t really make up for more noise upstream of sequencing by doing more sequencing; you need to make new measurements that have independent noise. But I consider it here for illustration purposes.\nAnalysis\nAssume a Gamma-Poisson (also known as Negative Binomial) model for both the real data and the statistical inference model.\nI will start with a simple model.\nFor now, I will assume that the fraction of the population that is infected grows exponentially without any noise.\n\\[\\begin{align}\n  i(t) = i(0) \\exp(r t),\n\\end{align}\\]\nand that number of reads of the pathogen in the sample from time \\(t\\), \\(M(t)\\), has expectation\n\\[\\begin{align}\n  E[M(t)] &= i(t) \\cdot s B \\mathcal M\n        \\\\&\\equiv i(s) \\cdot a,\n\\end{align}\\]\nwhere\n\\(s\\) is the rate of pathogen shedding in infected, relative to background microbiome\n\\(B\\) is the measurement efficiency (bias), relative to background microbiome\n\\(\\mathcal M\\) is the total number of sequencing reads\nThis formula for the mean read count of the pathogen is derived elsewhere.\nFor simplicity I’m treating all of \\(s\\), \\(B\\), and \\(\\mathcal M\\) as fixed parameters.\nSince all of these parameters are constants that multiply together, it is convenient for later calculations to define \\(a \\equiv sB\\mathcal M\\).\nTODO\nExplain nuance regarding noise etc\nConsider using more nuanced model for expected pathogen reads based on integral of \\(i(t)\\).\nI assume a Negative Binomial model for the number of reads of the pathogen in the sample from time \\(t\\), using the ‘alternative parameterization’ used described in the Stan docs; however, I’ll follow Rstanarm and use \\(\\theta\\) in place of \\(\\phi\\) for the reciprocal dispersion parameter.\nIf \\(E[M] = \\mu\\), then the variance of \\(M\\) is\n\\[\\begin{align}\n  \\text{Var}[M]\n    &= \\mu + \\frac{\\mu^2}{\\theta}\n  \\\\&= \\mu \\left(1 + \\frac{\\mu}{\\theta} \\right),\n\\end{align}\\]\nand the coefficient of variation is\n\\[\\begin{align}\n  \\text{CV}[M]\n    &= \\frac{\\sqrt{\\text{Var}[M]}}{E[M]}\n  \\\\&= \\sqrt{\\frac{1}{\\mu} + \\frac{1}{\\theta}}.\n\\end{align}\\]\n(speculative) Intuitively, the coefficient of variation is the measure of noise relevant for our power to infer exponential growth rate.\nThe CV is Poisson-like when \\(\\mu \\ll \\theta\\) and Gamma-like when \\(\\mu \\gg \\theta\\).\nIn the Poisson regime, we can reduce the CV by increasing sequencing effort and hence increasing \\(\\mu\\);\nhowever, once \\(\\mu \\gg \\theta\\), increasing the sequencing effort will have a negligible benefit.\nInstead, we need to collect additional samples and/or re-measure existing samples so as to effectively average out the extra-Poisson noise.\nI will simulate using stats::rnbinom with the parameterization matching\nThe size argument in rnbinom is the (reciprocal) dispersion parameter \\(\\phi\\) in stan docs, but which is called \\(\\theta\\) in rstanarm::neg_binomial_2.\n\n\n# Total time of monitoring, in days\ntotal_time <- 20\nsampling_days <- seq(0, total_time - 1, by = 1)\n# Initial fraction of population infected\ni_0 <- 1e-4\n# Growth rate of infections to correspond to 4 doublings\ndoubling_time <- 5\nr <- log(2) / doubling_time\n# Multiplier a s.t. expected number of reads spans 1 over the range\na <- 0.4 / i_0\n# a * i_0 * exp(r * sampling_days)\n\n# reciprocal-dispersion parameter; smaller values = lower variance\ntheta_sim <- 5e-1\n\nset.seed(42)\nsim <- tibble(t = sampling_days) %>%\n  mutate(\n    i_t = i_0 * exp(r * t),\n    M_t_expected = a * i_t,\n    # lambda_t = rgamma(n(), shape = 1, rate = 1 / M_t_expected),\n    # M_t = rpois(n(), lambda_t)\n    M_t = rnbinom(n(), size = theta_sim, mu = M_t_expected)\n  )\n\n\n\n\nsim %>%\n  ggplot(aes(t)) +\n    geom_line(aes(y = M_t_expected)) +\n    geom_point(aes(y = M_t))\n\n\n\nNow we want to repeat the simulations many times for a range of values of \\(\\theta\\) and \\(a\\).\n\n\nsimulate_monitoring <- function(theta, a) {\n  tibble(t = sampling_days) %>%\n    mutate(\n      i_t = i_0 * exp(r * t),\n      M_t_expected = a * i_t,\n      M_t = rnbinom(n(), size = theta, mu = M_t_expected)\n    )\n}\n\nset.seed(42)\n\nsims <- crossing(\n  theta = c(3e-2, 1e-1, 3e-1, 1e0),\n  # a = 4000 * c(1, 3, 10, 30),\n  a = 4000 * c(1, 10, 100),\n  rep = 1:40\n) %>%\n  mutate(\n    data = map2(theta, a, simulate_monitoring),\n    M_total = map_dbl(data, ~ sum(.x$M_t))\n  )\n\n\n\n\nsims %>%\n  unnest(data) %>%\n  ggplot(aes(t, M_t)) +\n  facet_grid(a ~ theta, scales = 'free_y') +\n  geom_line(aes(group = rep))\n\n\n\nCHECK: Do any simulations have no observations?\n\n\nsims %>%\n  count(theta, M_total == 0)\n\n# A tibble: 5 × 3\n  theta `M_total == 0`     n\n  <dbl> <lgl>          <int>\n1  0.03 FALSE            113\n2  0.03 TRUE               7\n3  0.1  FALSE            120\n4  0.3  FALSE            120\n5  1    FALSE            120\n\nYes! We need to handle these separately, since they will cause errors when we try to fit the GLM.\n\n\nFALSE\n\n[1] FALSE\n\nLet’s try fitting on a subset, for now using the ‘optimizing’ algorithm to speed things up.\nWe must restrict ourselves to cases where the total count is at least 1, to be able to fit.\nWe might consider restricting ourselves to a higher count than this.\nTODO: disable centering the predictors; otherwise, need to change our intercept prior to be based on the midpoint of the simulation.\n\n\nsims_fit <- sims %>%\n  # filter(rep <= 2, M_total > 0) %>%\n  filter(M_total > 0) %>%\n  mutate(\n    prior_intercept_mean = log(a * i_0),\n    # Note: For testing with 'optimizing', we can fit in parallel\n    fit = map(data, ~stan_glm(\n        M_t ~ t,, \n        data = .x,\n        family = neg_binomial_2,\n        prior = normal(0, 0.5, autoscale = FALSE),\n        # prior_intercept = normal(prior_intercept_mean, 2.5, autoscale = FALSE),\n        prior_aux = exponential(2, autoscale = FALSE),\n        algorithm = 'optimizing',\n    ))\n  )\n\n\nTODO\nLook at a single model and its fit against the ‘real’ data.\nConsider the prior on theta, and explicitly code it in. For the purposes of this study, I can make the prior accurately reflect the actual range of theta values that I’m using. In fact, it would even make sense to set the prior tightly around the correct value of theta, which should also speed up the inference.\nreconsider the prior on the intercept; could set to a range like with theta though that doesn’t make too much sense if we think of the fact that we’re modeling an increase in sequencing effort and so we have extra info that the mean should increase accordingly\n\n\nfit <- sims_fit %>% pull(fit) %>% pluck(1)\nfit\n\nstan_glm\n family:       neg_binomial_2 [log]\n formula:      M_t ~ t\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept)  2.9    2.0  \nt           -0.4    0.3  \n\nAuxiliary parameter(s):\n                      Median MAD_SD\nreciprocal_dispersion 0.1    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nfit %>% tidy(conf.int = TRUE)\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    2.86      1.96    -0.536    6.58  \n2 t             -0.429     0.272   -0.914    0.0584\n\nfit %>% prior_summary\n\nPriors for model '.' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n ~ normal(location = 0, scale = 0.5)\n\nAuxiliary (reciprocal_dispersion)\n ~ exponential(rate = 2)\n------\nSee help('prior_summary.stanreg') for more details\n\nOur goal is to assess how dispersion impacts our ability to infer the exponential trend.\nOne way we can do that is plot credible intervals for the growth rate \\(r\\), for all simulations, group by theta, against the actual growth rate.\nAn easy way to get CIs is with broom.mixed::tidy(),\n\n\nfit %>% tidy(conf.int = TRUE, conf.level = 0.9)\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    2.86      1.96    -0.536    6.58  \n2 t             -0.429     0.272   -0.914    0.0584\n\nBesides correctly inferring the growth rate, we are also interested in the our posterior that the sequence is increasing (possibly above a certain rate of increase; here I’ll just consider a rate above 0).\nWe can do this for a single fit like\n\n\n# post <- fit %>% as.matrix %>% as_tibble %>% janitor::clean_names() %>%\n  # glimpse\nfit %>% as.matrix %>% {mean(.[, 't'] > 0)}\n\n[1] 0.075\n\n\n\nx <- sims_fit %>%\n  mutate(\n    prob_increasing = map_dbl(fit, ~ .x %>% as.matrix %>% {mean(.[, 't'] > 0)}),\n    fit = map(fit, tidy, conf.int = TRUE, conf.level = 0.9),\n  ) %>%\n  unnest(fit) %>%\n  filter(term == 't')\n\n\n\n\nx %>%\n  ggplot(aes(y = rep, x = estimate)) +\n  facet_grid(a ~ theta) +\n  geom_pointinterval(\n    aes(xmin = conf.low, xmax = conf.high),\n    fatten_point = 1\n  ) +\n  geom_vline(xintercept = 0, color = 'grey') +\n  geom_vline(xintercept = r, linetype = 2, color = 'darkred')\n\n\n\nNote that for lambda=0.03, some intervals are missing; these are cases where the dispersion was so high that we never saw the pathogen.\nIn those cases, our estimate of the growth rate is simply the prior; perhaps we can show that?\nFrom this graph, it looks like a 10X increase in dispersion requires more than a 10X increase in sequencing effort to achieve the same power.\nLet’s check how calibrated these CIs are, by comparing the proportion of fits that contain the true value of \\(r\\) against the expected 90%.\nNote, the case where \\(\\theta = 0.03\\) is currently not adjusted for the missing data.\n\n\nx %>%\n  mutate(\n    true_value_in_ci = r >= conf.low & r <= conf.high\n  ) %>%\n  summarize(.by = c(theta, a),\n    proportion = mean(true_value_in_ci)\n  )\n\n# A tibble: 12 × 3\n   theta      a proportion\n   <dbl>  <dbl>      <dbl>\n 1  0.03   4000      0.853\n 2  0.03  40000      0.744\n 3  0.03 400000      0.8  \n 4  0.1    4000      0.775\n 5  0.1   40000      0.825\n 6  0.1  400000      0.725\n 7  0.3    4000      0.85 \n 8  0.3   40000      0.85 \n 9  0.3  400000      0.875\n10  1      4000      0.85 \n11  1     40000      0.925\n12  1    400000      0.95 \n\nThese seem to be fairly calibrated; would need to do a binomial test to look for evidence of deviation.\nNow let’s look at the power to detect exponential growth, by considering the posterior probability that \\(r>0\\).\n\n\nx %>%\n  ggplot(aes(y = as.factor(theta), x = prob_increasing)) +\n  facet_wrap(~a, labeller = label_both) +\n  labs(x = 'Pr(r > 0)', y = 'reciprocal dispersion, theta') +\n  geom_dots(binwidth = 0.01)\n\n\n  # stat_dotsinterval()\n\n\nTODO: in revising, do the above calculation at the same time as the inteval extraction. Could also do the interval in the same manner, from the posterior.\n\n\nx %>%\n  ggplot(aes(y = as.factor(a), x = prob_increasing)) +\n  facet_wrap(~theta, labeller = label_both) +\n  labs(x = 'Pr(r > 0)', y = 'reciprocal dispersion, theta') +\n  geom_dots(binwidth = 0.01)\n\n\n  # stat_dotsinterval()\n\n\nHow often do we infer that \\(r>0\\) is more likely than not? Or with 80% certainty?\nNote: We have not filled in the missing rows where there were no observations; in these cases, we cannot infer increase.\n\n\nx %>%\n  summarize(.by = c(theta, a),\n    prob_0.5 = mean(prob_increasing > 0.5),\n    prob_0.6 = mean(prob_increasing > 0.6),\n    prob_0.8 = mean(prob_increasing > 0.8),\n  )\n\n# A tibble: 12 × 5\n   theta      a prob_0.5 prob_0.6 prob_0.8\n   <dbl>  <dbl>    <dbl>    <dbl>    <dbl>\n 1  0.03   4000    0.618    0.588    0.324\n 2  0.03  40000    0.513    0.513    0.282\n 3  0.03 400000    0.55     0.525    0.325\n 4  0.1    4000    0.775    0.75     0.525\n 5  0.1   40000    0.7      0.65     0.475\n 6  0.1  400000    0.8      0.675    0.575\n 7  0.3    4000    0.95     0.9      0.85 \n 8  0.3   40000    0.9      0.9      0.825\n 9  0.3  400000    0.95     0.95     0.825\n10  1      4000    0.975    0.975    0.95 \n11  1     40000    1        1        1    \n12  1    400000    1        1        1    \n\nNext steps\nwrite out the model, and compute the coefficient of variation under it.\nwrite some background\nmake the model more concrete, perhaps by framing as having a random relative abundance and Poisson sampling.\nstart with a lower value of a, s.t. can see that sequencing depth matters until we’re seeing a large enough expected count\ninvestigate the errors during fitting\nDiscussion\nIncreasing the sequencing depth cannot make up for an increase in overdispersion.\nThis makes sense — once we’re in a regime where the expected read count is above 0 and there is lots of overdispersion relative to Poisson, sequencing more doesn’t help much; it just helps us get a more precise measurement of the latent noisy (relative) abundance, and what we need is to reduce noise in that latent abundance.\nTo do this, we need to measure more samples with independent relative abundances.\nWe can reduce the noise from sample processing by processing the same sample repeatedly; however, for other noise sources we’d need to collect new samples from different sources or from more days.\nNote that because I use the correct model to fit the data, increasing the dispersion does not make the fit overconfident; the credible intervals are still tending to cover the true value of r the expected 90% of the time.\nIn contrast, if I fit using Poisson regression instead, I expect the fit to be overconfident, that is that the credible intervals will be too small and we’ll be missing the true r more than 90% of the time, for the cases there theta is significantly less than 1.\nSession info\n\nClick for session info\n\n\nsessioninfo::session_info()\n\n─ Session info ─────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       Arch Linux\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-02-09\n pandoc   3.0 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ─────────────────────────────────────────────────────────────\n package        * version  date (UTC) lib source\n assertthat       0.2.1    2019-03-21 [1] CRAN (R 4.0.0)\n backports        1.4.1    2021-12-13 [1] CRAN (R 4.1.2)\n base64enc        0.1-3    2015-07-28 [1] CRAN (R 4.0.0)\n bayesplot        1.9.0    2022-03-10 [1] CRAN (R 4.2.0)\n beeswarm         0.4.0    2021-06-01 [1] CRAN (R 4.1.0)\n boot             1.3-28   2021-05-03 [2] CRAN (R 4.2.2)\n broom            1.0.1    2022-08-29 [1] CRAN (R 4.2.1)\n broom.mixed    * 0.2.9.4  2022-04-17 [1] CRAN (R 4.2.0)\n bslib            0.4.1    2022-11-02 [1] CRAN (R 4.2.2)\n cachem           1.0.6    2021-08-19 [1] CRAN (R 4.1.1)\n callr            3.7.3    2022-11-02 [1] CRAN (R 4.2.1)\n cellranger       1.1.0    2016-07-27 [1] CRAN (R 4.0.0)\n cli              3.4.1    2022-09-23 [1] CRAN (R 4.2.1)\n codetools        0.2-18   2020-11-04 [2] CRAN (R 4.2.2)\n colorspace       2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n colourpicker     1.2.0    2022-10-28 [1] CRAN (R 4.2.1)\n cowplot        * 1.1.1    2021-08-27 [1] Github (wilkelab/cowplot@555c9ae)\n crayon           1.5.2    2022-09-29 [1] CRAN (R 4.2.1)\n crosstalk        1.2.0    2021-11-04 [1] CRAN (R 4.1.2)\n DBI              1.1.3    2022-06-18 [1] CRAN (R 4.2.1)\n dbplyr           2.2.1    2022-06-27 [1] CRAN (R 4.2.1)\n digest           0.6.30   2022-10-18 [1] CRAN (R 4.2.1)\n distill          1.5.2    2022-11-10 [1] Github (rstudio/distill@9c1a1a2)\n distributional   0.3.1    2022-09-02 [1] CRAN (R 4.2.1)\n downlit          0.4.2    2022-07-05 [1] CRAN (R 4.2.1)\n dplyr          * 1.1.0    2023-01-29 [1] CRAN (R 4.2.2)\n DT               0.26     2022-10-19 [1] CRAN (R 4.2.1)\n dygraphs         1.1.1.6  2018-07-11 [1] CRAN (R 4.0.2)\n ellipsis         0.3.2    2021-04-29 [1] CRAN (R 4.1.0)\n evaluate         0.18     2022-11-07 [1] CRAN (R 4.2.2)\n fansi            1.0.3    2022-03-24 [1] CRAN (R 4.2.1)\n farver           2.1.1    2022-07-06 [1] CRAN (R 4.2.1)\n fastmap          1.1.0    2021-01-25 [1] CRAN (R 4.0.4)\n forcats        * 0.5.2    2022-08-19 [1] CRAN (R 4.2.1)\n fs             * 1.5.2    2021-12-08 [1] CRAN (R 4.1.2)\n furrr          * 0.3.1    2022-08-15 [1] CRAN (R 4.2.1)\n future         * 1.28.0   2022-09-02 [1] CRAN (R 4.2.1)\n gargle           1.2.1    2022-09-08 [1] CRAN (R 4.2.1)\n generics         0.1.3    2022-07-05 [1] CRAN (R 4.2.1)\n ggbeeswarm     * 0.6.0    2017-08-07 [1] CRAN (R 4.0.0)\n ggdist         * 3.2.0    2022-07-19 [1] CRAN (R 4.2.1)\n ggplot2        * 3.3.6    2022-05-03 [1] CRAN (R 4.2.0)\n ggridges         0.5.4    2022-09-26 [1] CRAN (R 4.2.1)\n globals          0.16.1   2022-08-28 [1] CRAN (R 4.2.1)\n glue             1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n googledrive      2.0.0    2021-07-08 [1] CRAN (R 4.1.0)\n googlesheets4    1.0.1    2022-08-13 [1] CRAN (R 4.2.1)\n gridExtra        2.3      2017-09-09 [1] CRAN (R 4.0.2)\n gtable           0.3.1    2022-09-01 [1] CRAN (R 4.2.1)\n gtools           3.9.3    2022-07-11 [1] CRAN (R 4.2.1)\n haven            2.5.1    2022-08-22 [1] CRAN (R 4.2.1)\n here           * 1.0.1    2020-12-13 [1] CRAN (R 4.0.5)\n highr            0.9      2021-04-16 [1] CRAN (R 4.1.0)\n hms              1.1.2    2022-08-19 [1] CRAN (R 4.2.1)\n htmltools        0.5.3    2022-07-18 [1] CRAN (R 4.2.1)\n htmlwidgets      1.5.4    2021-09-08 [1] CRAN (R 4.1.1)\n httpuv           1.6.6    2022-09-08 [1] CRAN (R 4.2.1)\n httr             1.4.4    2022-08-17 [1] CRAN (R 4.2.1)\n igraph           1.3.5    2022-09-22 [1] CRAN (R 4.2.1)\n inline           0.3.19   2021-05-31 [1] CRAN (R 4.1.0)\n jquerylib        0.1.4    2021-04-26 [1] CRAN (R 4.1.0)\n jsonlite         1.8.3    2022-10-21 [1] CRAN (R 4.2.1)\n knitr            1.40     2022-08-24 [1] CRAN (R 4.2.1)\n labeling         0.4.2    2020-10-20 [1] CRAN (R 4.0.3)\n later            1.3.0    2021-08-18 [1] CRAN (R 4.1.1)\n lattice          0.20-45  2021-09-22 [2] CRAN (R 4.2.2)\n lifecycle        1.0.3    2022-10-07 [1] CRAN (R 4.2.1)\n listenv          0.8.0    2019-12-05 [1] CRAN (R 4.0.0)\n lme4             1.1-31   2022-11-01 [1] CRAN (R 4.2.1)\n loo              2.5.1    2022-03-24 [1] CRAN (R 4.2.0)\n lubridate        1.9.0    2022-11-06 [1] CRAN (R 4.2.2)\n magrittr         2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n markdown         1.3      2022-10-29 [1] CRAN (R 4.2.1)\n MASS             7.3-58.1 2022-08-03 [1] CRAN (R 4.2.1)\n Matrix           1.5-1    2022-09-13 [1] CRAN (R 4.2.1)\n matrixStats      0.62.0   2022-04-19 [1] CRAN (R 4.2.0)\n memoise          2.0.1    2021-11-26 [1] CRAN (R 4.1.2)\n mime             0.12     2021-09-28 [1] CRAN (R 4.1.1)\n miniUI           0.1.1.1  2018-05-18 [1] CRAN (R 4.0.2)\n minqa            1.2.5    2022-10-19 [1] CRAN (R 4.2.1)\n modelr           0.1.9    2022-08-19 [1] CRAN (R 4.2.1)\n munsell          0.5.0    2018-06-12 [1] CRAN (R 4.0.0)\n nlme             3.1-160  2022-10-10 [2] CRAN (R 4.2.2)\n nloptr           2.0.3    2022-05-26 [1] CRAN (R 4.2.0)\n nvimcom        * 0.9-142  2022-12-22 [1] local\n parallelly       1.32.1   2022-07-21 [1] CRAN (R 4.2.1)\n patchwork      * 1.1.2    2022-08-19 [1] CRAN (R 4.2.1)\n pillar           1.8.1    2022-08-19 [1] CRAN (R 4.2.1)\n pkgbuild         1.3.1    2021-12-20 [1] CRAN (R 4.1.2)\n pkgconfig        2.0.3    2019-09-22 [1] CRAN (R 4.0.0)\n plyr             1.8.7    2022-03-24 [1] CRAN (R 4.2.0)\n prettyunits      1.1.1    2020-01-24 [1] CRAN (R 4.0.0)\n processx         3.8.0    2022-10-26 [1] CRAN (R 4.2.1)\n promises         1.2.0.1  2021-02-11 [1] CRAN (R 4.0.4)\n ps               1.7.2    2022-10-26 [1] CRAN (R 4.2.1)\n purrr          * 0.3.5    2022-10-06 [1] CRAN (R 4.2.1)\n R6               2.5.1    2021-08-19 [1] CRAN (R 4.1.1)\n Rcpp           * 1.0.9    2022-07-08 [1] CRAN (R 4.2.1)\n RcppParallel     5.1.5    2022-01-05 [1] CRAN (R 4.1.2)\n readr          * 2.1.3    2022-10-01 [1] CRAN (R 4.2.1)\n readxl           1.4.1    2022-08-17 [1] CRAN (R 4.2.1)\n reprex           2.0.2    2022-08-17 [1] CRAN (R 4.2.1)\n reshape2         1.4.4    2020-04-09 [1] CRAN (R 4.0.0)\n rlang            1.0.6    2022-09-24 [1] CRAN (R 4.2.1)\n rmarkdown      * 2.18     2022-11-09 [1] CRAN (R 4.2.2)\n rprojroot        2.0.3    2022-04-02 [1] CRAN (R 4.2.2)\n rstan            2.21.7   2022-09-08 [1] CRAN (R 4.2.1)\n rstanarm       * 2.21.3   2022-04-09 [1] CRAN (R 4.2.0)\n rstantools       2.2.0    2022-04-08 [1] CRAN (R 4.2.0)\n rvest            1.0.3    2022-08-19 [1] CRAN (R 4.2.1)\n sass             0.4.2    2022-07-16 [1] CRAN (R 4.2.1)\n scales           1.2.1    2022-08-20 [1] CRAN (R 4.2.1)\n sessioninfo      1.2.2    2021-12-06 [1] CRAN (R 4.1.2)\n shiny            1.7.3    2022-10-25 [1] CRAN (R 4.2.1)\n shinyjs          2.1.0    2021-12-23 [1] CRAN (R 4.1.2)\n shinystan        2.6.0    2022-03-03 [1] CRAN (R 4.2.0)\n shinythemes      1.2.0    2021-01-25 [1] CRAN (R 4.0.4)\n StanHeaders      2.21.0-7 2020-12-17 [1] CRAN (R 4.0.3)\n stringi          1.7.8    2022-07-11 [1] CRAN (R 4.2.2)\n stringr        * 1.4.1    2022-08-20 [1] CRAN (R 4.2.1)\n survival         3.4-0    2022-08-09 [2] CRAN (R 4.2.2)\n threejs          0.3.3    2020-01-21 [1] CRAN (R 4.0.2)\n tibble         * 3.1.8    2022-07-22 [1] CRAN (R 4.2.1)\n tidyr          * 1.2.1    2022-09-08 [1] CRAN (R 4.2.1)\n tidyselect       1.2.0    2022-10-10 [1] CRAN (R 4.2.1)\n tidyverse      * 1.3.2    2022-07-18 [1] CRAN (R 4.2.1)\n timechange       0.1.1    2022-11-04 [1] CRAN (R 4.2.2)\n tzdb             0.3.0    2022-03-28 [1] CRAN (R 4.2.0)\n utf8             1.2.2    2021-07-24 [1] CRAN (R 4.1.0)\n vctrs            0.5.2    2023-01-23 [1] CRAN (R 4.2.2)\n vipor            0.4.5    2017-03-22 [1] CRAN (R 4.0.0)\n withr            2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun             0.34     2022-10-18 [1] CRAN (R 4.2.1)\n xml2             1.3.3    2021-11-30 [1] CRAN (R 4.1.2)\n xtable           1.8-4    2019-04-21 [1] CRAN (R 4.0.0)\n xts              0.12.2   2022-10-16 [1] CRAN (R 4.2.1)\n yaml             2.3.6    2022-10-18 [1] CRAN (R 4.2.1)\n zoo              1.8-11   2022-09-17 [1] CRAN (R 4.2.1)\n\n [1] /home/michael/.local/lib/R/library\n [2] /usr/lib/R/library\n\n────────────────────────────────────────────────────────────────────────\n\n\n\n\n",
    "preview": "posts/2023-02-08-effect-of-noise-on-egd/main_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-28-egd-theory-notes/",
    "title": "EGD theory notes",
    "description": "Captured notes on exponential growth detection.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2022-11-28",
    "categories": [
      "EGD",
      "theory"
    ],
    "contents": "\n\nContents\nStandard error of growth rate under IID multiplicative noise\n2022-03-28\n2022-08-06\n2022-11-28\n\n\nStandard error of growth rate under IID multiplicative noise\n2022-03-28\nUnder IID multiplicative noise, the exponential growth (EG) model is equivalent to the standard linear regression model applied to log abundance.\nThe standard error in the growth rate estimate is\n\\[\\begin{align}\n  se(\\hat r) = \\frac{\\sigma(\\varepsilon)}{\\sigma(t) \\sqrt{n}},\n\\end{align}\\]\nwhere \\(\\sigma(\\varepsilon)\\) is the standard deviation of the residual log measurement, \\(t\\) is the sampling times, and \\(n\\) is the number of samples.\nFor daily samples from \\(t=1\\) to \\(t=T\\), we have that \\(n = T\\) and that \\(\\sigma(t) = \\sqrt{(T^2-1)/12}\\).\nIn this case, the standard error is\n\\[\\begin{align}\n  se(\\hat r)\n    &= \\frac{\\sqrt{12} \\; \\sigma(\\varepsilon)}{\\sqrt{T (T^2 -2)}}\n  \\\\&\\approx \\frac{\\sqrt{12} \\; \\sigma(\\varepsilon)}{T^{3/2}} \\quad \\text{for $T \\gg 1$}.\n\\end{align}\\]\nThis formula tells us that our uncertainty decreases with more days of sampling as \\(T^{3/2}\\), with a factor \\(T\\) coming from the increased temporal spread of sampling days and a factor \\(T^{1/2}\\) coming from the increased number of samples.\nIf we included technical replicates, we could increase precision without requiring more days; however, an equal increase in precision requires processing and sequencing more samples from the current range of days than adding additional samples from additional days to the range.\n2022-08-06\nThis calculation just tells us about the standard error; it might be interesting to extend it to consider our ability to detect positive growth.\nincreasing the number of samples on a given day can only reduce the fraction of variance that is not day-by-day, e.g. due to sample processing and sequencing.\nshould think on a graphical / analogy / schematic representation of this for the team\n2022-11-28\nConsider a mixture model of lognormal + Poisson noise on the counts.\nI expect that the above applies in the regime where counts are >> 1 with very high probability, and the pure Poisson theory to apply when the counts are below some threshold defined by the (geometric) standard dev of the lognormal noise.\nHowever, I suspect we will often be in an intermediate regime for EGD when we are first able to detect an emerging pathogen, where the multiplicative noise is sufficient that it is not uncommon for the counts to be \\(\\lesssim 1\\) and \\(\\gg 1\\) on adjacent days.\nEven if that is so, perhaps the results for the two regimes still give us useful bounds.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-05-developing-a-qpcr-data-analysis-workflow/",
    "title": "Developing a qPCR data analysis workflow",
    "description": "In-progress qPCR data analysis workflow in R, using data from the 2022-Q2 Sprint spike-in experiment.",
    "author": [
      {
        "name": "Mike",
        "url": {}
      }
    ],
    "date": "2022-11-05",
    "categories": [
      "qPCR",
      "R"
    ],
    "contents": "\nSetup\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fs)\n# library(generics)\n\nlibrary(cowplot)\ntheme_set(theme_cowplot())\nlibrary(patchwork)\n\n\nThe data is from a folder in the NAO Drive currently called ‘Spike-in experiments’, which I’ve downloaded locally.\n\n\ndata_path <- here( '_data/nao/qpcr', '2022-06-29-spike-in-experiment/results')\ndir_ls(data_path) %>% path_file\n\n [1] \"!README.docx\"                          \n [2] \"2022-06-29-trip01.eds\"                 \n [3] \"2022-06-29-trip01.txt\"                 \n [4] \"2022-06-29-trip01.xlsx\"                \n [5] \"2022-06-29-trip01_2.eds\"               \n [6] \"2022-06-29-trip02.eds\"                 \n [7] \"2022-06-29-trip02.xlsx\"                \n [8] \"2022-06-30-trip02_009_seq_barcode.xlsx\"\n [9] \"2022-06-30_trip03.xlsx\"                \n[10] \"Results.ipynb\"                         \n[11] \"plate_layout_trip1.txt\"                \n[12] \"plate_layout_trip2.txt\"                \n[13] \"results.md5\"                           \n[14] \"trip1-template.edt\"                    \n\nTODO: try loading the data directly from Google Drive\nLoad qPCR data\nThis file assumes that the data is within a folder ‘data/’ within the root project folder.\nFile info:\n- Excel files have the raw and processed florescence measurements (Rn and Delta Rn), as well as the softwares autothreshold stuff in another sheet.\n- Also some sample metadata is here; however, we might want want to take that from the .txt file, since that is (I believe) closer to the original supplied table.\n\nTODO: Talk to Ari about whether we have/can save the files used to set up the qPCR experiment\nFirst, we can read in the relevant sections of the Excel file and clean up thedata a bit,\nFirst, load in the sample metadata.\nNOTE: Throughout, I’m using janitor::clean_names() to standardize the format of the column names\n\n\n\nsam <- path(data_path, '2022-06-29-trip01.txt') %>%\n  read_tsv(skip = 43) %>%\n  janitor::clean_names() %>%\n  mutate(\n    row = str_sub(well_position, 1, 1) %>% as.ordered,\n    column = str_sub(well_position, 2) %>% as.integer %>% as.ordered,\n  ) %>%\n  # relocate(row, column, .after = well_position) %>%\n  glimpse\n\nRows: 96\nColumns: 15\n$ well           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ well_position  <chr> \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8…\n$ sample_name    <chr> \"Blank\", \"NTC\", \"Trip1_010_Neg_Ctrl\", \"Trip1_…\n$ sample_color   <chr> \"RGB(0,139,69)\", \"RGB(142,56,142)\", \"RGB(139,…\n$ biogroup_name  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ biogroup_color <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ target_name    <chr> \"Blank\", \"Trip1_010_0\", \"Trip1_010_Neg_Ctrl\",…\n$ target_color   <chr> \"RGB(0,139,69)\", \"RGB(142,142,56)\", \"RGB(238,…\n$ task           <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"…\n$ reporter       <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FA…\n$ quencher       <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"…\n$ quantity       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ comments       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ row            <ord> A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, …\n$ column         <ord> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, …\n\nTODO: pre-specify the column types\nNOTE: In this experiment, the target name was incorrectly set to be identical to the sample name; instead, each non-blank sample should have the target corresponding to the primer/probe pair (here, 009 or 010). The import chunk therefore replaces the target name accordingly. The blank is ste to NA since no primers/master mix is added.\nTODO: confirm this with Anjali and Ari; and in future, make sure target corresponds to the relevant primer/probe set for that well.\nTODO: Suggest that we use more descriptive target names than ‘010’\n\nTODO: Suggest we find some way of directly adding the dilution and perhaps the concentration to the sample data, rather than having to parse from the sample name\n\nThere is additional sample data hidden in the sample names, which we’ll need to fix in future runs (it should be in its own columns in a table).\nBut for now we can parse it from the sample names.\nI’ll overwrite the faulty target names.\n\n\nsam <- sam %>%\n  separate(sample_name, \n    into = c('ww_triplicate', 'target_name', 'dilution_name'),\n    sep = '_', extra = 'merge',\n    remove = FALSE\n  )\n\n\nNOTE: The target name of the NTC is now NA, which is incorrect, but I’m not going to worry about that now.\nThe proper fix is for the target name to be fixed in the source data.\nNOTE: We need to use the actual starting concentrations for the standard curve. Need to talk to Anjali about how these should be supplied.\nHere I will assume the following.\n\n\n# concentration in copies per microliter\nconc_low <- 0.1\ndilution_step <- 20\nnum_samples <- 7\nconc_max <- conc_low * dilution_step^(num_samples - 1)\n\ndilution_df <- tibble(\n  dilution_power = 0:6,\n  dilution_factor = dilution_step^dilution_power,\n  conc = conc_max / dilution_factor,\n  dilution_name = str_c('D', '0', dilution_power + 1)\n)\n\n\nwhich we must add to the sample data,\n\n\nsam <- sam %>%\n  left_join(dilution_df, by = 'dilution_name')\n\n\nNext, load the amplification data — the relative florescence values (Rn and Delta Rn) — and join the sample metadata.\n\n\namp <- path(data_path, '2022-06-29-trip01.xlsx') %>%\n  readxl::read_excel(\n    sheet = 'Amplification Data',\n    skip = 40,\n    col_types = c('numeric', 'text', 'numeric', 'text', 'numeric', 'numeric')\n  ) %>%\n  janitor::clean_names() %>%\n  glimpse\n\nRows: 3,840\nColumns: 6\n$ well          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ well_position <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ cycle         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ target_name   <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"…\n$ rn            <dbl> 5.125966, 5.172779, 5.221941, 5.282046, 5.3525…\n$ delta_rn      <dbl> -0.228727385, -0.190693140, -0.150309995, -0.0…\n\nNote, this table has the original, incorrect target name.\nI’ll drop that, and join the sample metadata table which has the corrected target names.\n\n\namp <- amp %>%\n  select(-target_name) %>%\n  left_join(sam, by = c('well', 'well_position')) %>%\n  glimpse\n\nRows: 3,840\nColumns: 23\n$ well            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ well_position   <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A…\n$ cycle           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ rn              <dbl> 5.125966, 5.172779, 5.221941, 5.282046, 5.35…\n$ delta_rn        <dbl> -0.228727385, -0.190693140, -0.150309995, -0…\n$ sample_name     <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\",…\n$ ww_triplicate   <chr> \"Blank\", \"Blank\", \"Blank\", \"Blank\", \"Blank\",…\n$ target_name     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ dilution_name   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ sample_color    <chr> \"RGB(0,139,69)\", \"RGB(0,139,69)\", \"RGB(0,139…\n$ biogroup_name   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ biogroup_color  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ target_color    <chr> \"RGB(0,139,69)\", \"RGB(0,139,69)\", \"RGB(0,139…\n$ task            <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", …\n$ reporter        <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"F…\n$ quencher        <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", …\n$ quantity        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ comments        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ row             <ord> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,…\n$ column          <ord> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ dilution_power  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ dilution_factor <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conc            <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\nTODO: Read in the baseline start/stop points, so can view and check where in the amplifying/not portion\n\nExplore sample metadata\n\n\nsam %>%\n  count(target_name)\n\n# A tibble: 3 × 2\n  target_name     n\n  <chr>       <int>\n1 009            24\n2 010            24\n3 <NA>           48\n\nPlot the plate layout\nTODO: use code from vivo vitro to do this nicely; need to first parse the row and column from the well position\nTODO: see if can flow the sample names so that they print nicer\nQC checks\nCheck the blanks\nCheck the NTCs and Neg controls (though this is target-specific)\n\nWe’ll want to do the analysis separately for each target.\nMaybe there’s value in first looking at everything\nTODO: Set a fixed color scheme for the targets, and use this in all plots.\n\n\ndelta_rn_min <- 1e-3\np1 <- amp %>%\n  filter(!is.na(target_name)) %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  scale_color_brewer(type = 'qual') +\n  geom_point() +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nTODO: Consider if a better way to deal with non-positive values in the log-scale plot. See what the software does. (I think it might just not show these points.)\nAnalysis of a single target\nWill ultimately do this for all targets; perhaps show in tabs?\nFor now, I’ll use target 009.\n\n\namp_cur <- amp %>%\n  filter(target_name == '009')\n\n\n\n\ndelta_rn_min <- 1e-3\np1 <- amp_cur %>%\n  ggplot(aes(cycle, pmax(delta_rn, delta_rn_min), color = target_name)) +\n  geom_line(aes(group = well)) +\n  scale_color_brewer(type = 'qual') +\n  geom_point() +\n  labs(y = 'Delta Rn', x = 'Cycle', color = 'Target')\np2 <- p1 +\n  scale_y_log10()\np1 / p2\n\n\n\nPick threshold\nFor now, will pick the threshold manually.\n\n\nthreshold <- 3e-1\np1 / p2 &\n  geom_hline(yintercept = threshold)\n\n\n\nTODO: implement chosen auto-threshold algorithm.\nCompute Cq values\nTODO Ask Anjali if wants to use Ct or Cq as the name\nTODO Separate out into distinct files the code where I’m testing for myself, and demonstrating for others\n\nI’ll define a custom function estimate_cq to estimate the Cq value for a trajectotry crossing a given quantification threshold.\nSee the appendix below for more info.\n\n\nestimate_cq <- function(.data, threshold) {\n  .data <- .data %>%\n    arrange(cycle) %>%\n    mutate(\n      # NOTE: log transformation, important for interpolation\n      across(delta_rn, log),\n      below = delta_rn < threshold,\n      above = delta_rn > threshold,\n    )\n  before <- .data %>%\n    filter(below) %>%\n    slice_tail(n = 1)\n  after <- .data %>%\n    filter(above) %>%\n    slice_head(n = 1)\n  # And find the intersection of the line passing between these two points, and the threshold.\n  slope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\n  delta_rn_diff <- threshold - before$delta_rn\n  cycle_diff <- delta_rn_diff / slope\n  ct <- before$cycle + cycle_diff\n  ct\n}\n\n\nNow use this function to compute the Cq values for each trajectory\n\n\nsample_vars <-  sam %>% colnames\ncqs <- amp_cur %>%\n  with_groups(all_of(sample_vars), nest) %>%\n  mutate(\n    cq = map_dbl(data, estimate_cq, threshold = threshold)\n  ) %>%\n  select(-data) %>%\n  glimpse\n\nRows: 24\nColumns: 21\n$ well            <dbl> 51, 52, 53, 54, 55, 56, 57, 58, 63, 64, 65, …\n$ well_position   <chr> \"E3\", \"E4\", \"E5\", \"E6\", \"E7\", \"E8\", \"E9\", \"E…\n$ sample_name     <chr> \"Trip1_009_Neg_Ctrl\", \"Trip1_009_D07\", \"Trip…\n$ ww_triplicate   <chr> \"Trip1\", \"Trip1\", \"Trip1\", \"Trip1\", \"Trip1\",…\n$ target_name     <chr> \"009\", \"009\", \"009\", \"009\", \"009\", \"009\", \"0…\n$ dilution_name   <chr> \"Neg_Ctrl\", \"D07\", \"D06\", \"D05\", \"D04\", \"D03…\n$ sample_color    <chr> \"RGB(238,44,44)\", \"RGB(51,161,201)\", \"RGB(23…\n$ biogroup_name   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ biogroup_color  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ target_color    <chr> \"RGB(51,161,201)\", \"RGB(238,18,137)\", \"RGB(1…\n$ task            <chr> \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", …\n$ reporter        <chr> \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"F…\n$ quencher        <chr> \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", …\n$ quantity        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ comments        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ row             <ord> E, E, E, E, E, E, E, E, F, F, F, F, F, F, F,…\n$ column          <ord> 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 7, 8, 9…\n$ dilution_power  <int> NA, 6, 5, 4, 3, 2, 1, 0, NA, 6, 5, 4, 3, 2, …\n$ dilution_factor <dbl> NA, 6.4e+07, 3.2e+06, 1.6e+05, 8.0e+03, 4.0e…\n$ conc            <dbl> NA, 1.0e-01, 2.0e+00, 4.0e+01, 8.0e+02, 1.6e…\n$ cq              <dbl> 33.15247, 32.86074, 33.07862, 32.43584, 30.9…\n\nEstimate and plot standard curve\nQuestions:\n\n\ncqs <- cqs %>%\n  mutate(\n    conc_log10 = log10(conc) \n  )\n\n\n\n\nfit <- lm(cq ~ conc_log10, data = cqs)\nfit %>% summary\n\n\nCall:\nlm(formula = cq ~ conc_log10, data = cqs)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.006 -2.612 -0.405  1.888  3.032 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  33.8227     0.7298   46.35  < 2e-16 ***\nconc_log10   -2.0442     0.1872  -10.92 1.25e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.232 on 19 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.8626,    Adjusted R-squared:  0.8553 \nF-statistic: 119.3 on 1 and 19 DF,  p-value: 1.251e-09\n\n\n\ncqs %>%\n  ggplot(aes(conc_log10, cq)) +\n  geom_point() +\n  geom_abline(\n    intercept = coef(fit)[1],\n    slope = coef(fit)[2]\n  )\n\n\n\nNote, better to plot in a way that shows the uncertainty.\nCan use stat smooth, but then not connected to the fit we did.\nCould be better to use some of the ggdist et al tools.\n\n\ncqs %>%\n  ggplot(aes(conc_log10, cq)) +\n  geom_point() +\n  stat_smooth(method = 'lm')\n\n\n\n\n\nFALSE\n\n[1] FALSE\n\nEstimate the efficiency from the standard curve\nWe can estimate the efficiency from the slope of the standard curve using the standard formula,\n\n\nx <- fit %>% broom::tidy()\nslope <- coef(fit)['conc_log10']\nefficiency_estimate <- 10^(-1/slope) - 1\n\n\nIn this case, the estimate is unreasonably large because the standard curve isn’t good.\n90% confidence interval:\n\n\nslope_ci <- confint(fit, parm = 'conc_log10', level = 0.9)\nefficiency_ci <- 10^(-1/slope_ci) - 1\nefficiency_ci\n\n                5 %     95 %\nconc_log10 1.644336 2.812536\n\nQuite a large range!\nBayesian version with rstanarm\n\n\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\n\nlibrary(ggdist)\n\n\n\n\nstan_fit <- stan_glm(\n  cq ~ conc_log10, \n  data = cqs,\n)\nstan_fit %>% summary\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      cq ~ conc_log10\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 21\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 33.8    0.8 32.8  33.8  34.8 \nconc_log10  -2.0    0.2 -2.3  -2.0  -1.8 \nsigma        2.4    0.4  1.9   2.3   3.0 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 27.9    0.8 26.9  27.9  28.9 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3052 \nconc_log10    0.0  1.0  3186 \nsigma         0.0  1.0  2274 \nmean_PPD      0.0  1.0  2947 \nlog-posterior 0.0  1.0  1453 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nstan_fit %>% plot\n\n\n\ntry to get the posterior samples, so that we can then get the posterior of the efficiency estimate.\nTODO: google a better way to do this\n\n\nslope_post <- rstan::extract(stan_fit$stanfit)$beta\nefficiency_post <- 10^(-1/slope_post) - 1\nefficiency_post %>% qplot\n\n\n\nnote, we could use a stronger prior since we have a lot of relevant domain info.\nDemo how to use the standard curve for calibration\nAppendix\nFunction for finding ct in a well\nStandard method is to compute the Ct for each well independentally.\nNeed to interpolate between points in the trajectory, and find when the trajectory crosses the threshold.\nI wonder what the software does; simplest method is perhaps linear interpolation of log Delta Rn.\nTODO: google linear interpolation in R. This is essentially what geom_line is doing. It would be handy if I could just get access to that output. Can also google how to create a piecewise linear function.\nFirst try\nSince I don’t have wifi, I will need to hack it myself.\nFor each well, find the cycles immediately before and after the theshold crossing\nfind the intersection between the line segment between those two points and the horizontal line at the threshold.\n\n\n\nFALSE\n\n[1] FALSE\n\n\n\n# Suppose we have the trajectory for a particular well\nx <- amp_cur %>%\n  filter(well == 52) %>%\n  select(cycle, delta_rn) %>%\n  arrange(cycle)\n# We can get the crossing point as follows\nx <- x %>%\n  mutate(\n    below = delta_rn < threshold,\n    above = delta_rn > threshold,\n  )\nbefore <- x %>%\n  filter(below) %>%\n  slice_tail(n = 1)\nafter <- x %>%\n  filter(above) %>%\n  slice_head(n = 1)\n\n\nAnd find the intersection.\nline passing between these two points, and the threshold.\n\n\nslope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\ndelta_rn_diff <- threshold - before$delta_rn\ncycle_diff <- delta_rn_diff / slope\nct <- before$cycle + cycle_diff\n\n\nwe can put this all together in a function,\n\n\nestimate_cq <- function(.data, threshold) {\n  .data <- .data %>%\n    arrange(cycle) %>%\n    mutate(\n      # NOTE: log transformation, important for interpolation\n      across(delta_rn, log),\n      below = delta_rn < threshold,\n      above = delta_rn > threshold,\n    )\n  before <- .data %>%\n    filter(below) %>%\n    slice_tail(n = 1)\n  after <- .data %>%\n    filter(above) %>%\n    slice_head(n = 1)\n  # And find the intersection of the line passing between these two points, and the threshold.\n  slope <- (after$delta_rn - before$delta_rn) / (after$cycle - before$cycle)\n  delta_rn_diff <- threshold - before$delta_rn\n  cycle_diff <- delta_rn_diff / slope\n  ct <- before$cycle + cycle_diff\n  ct\n}\n\n\nways we could improve\ncheck that there is only one crossing\ncheck that the crossing is not in the noise region\n\nTry 2, with wifi\n\n\n# Suppose we have the trajectory for a particular well\nx <- amp_cur %>%\n  filter(well == 52) %>%\n  select(cycle, delta_rn) %>%\n  arrange(cycle)\n# We can get the crossing point as follows\nx <- x %>%\n  mutate(\n    below = delta_rn < threshold,\n    above = delta_rn > threshold,\n  )\nbefore <- x %>%\n  filter(below) %>%\n  slice_tail(n = 1)\nafter <- x %>%\n  filter(above) %>%\n  slice_head(n = 1)\n\n\nWe can use approxfun() to define the interpolating function.\n\n\nf <- approxfun(x$cycle, x$delta_rn %>% log, rule = 1)\na <- seq(from = -5, to = 45, by = 0.1)\nqplot(a, f(a))\n\n\n\nand then the intersection with e.g. a root-finding function, uniroot(). however, this approach is a bit funny because of the non-monotonicity in the noise region, and the fact that once we know the cycle interval we can calculate the intersection of the interpolation manually.\nEfficiency estimate\nDerivation:\nThe slope of the standard curve tells us how many extra cycles correspond to a 10X decrease in starting concentration.\nWith perfect efficiency of \\(E=1\\), this would equal \\(\\log(10) / \\log(2)\\).\nMore generally, the number of extra cycles is \\(A = \\log(10) / \\log(1 + E)\\), for any log base; taking the log base to be 10 gives \\(A = 1 / \\log(1 + E)\\).\nThe slope of the standard curve corresponds to \\(-A\\).\nTherefore, we can estimate \\(E\\) by\n\\[\\begin{align}\n  \\hat E = 10^{1 / A} - 1\n\\end{align}\\]\nMethods for setting the threshold\nhttps://www.researchgate.net/post/How-can-I-set-the-threshold-in-a-Real-Time-PCR-result has some discussion.\nOne suggestion is to find candidate points based on the maximum of the second derivative of the amplification curves\nMethods for finding the baseline region\nNot needed right now since we’re using the software’s determination of this.\nNext steps\nmake an r package that can house helper functions\ndevelop an autothreshold method\nadd the ability to check the baseline calculation\nsave output\n\n\n\n\n",
    "preview": "posts/2022-11-05-developing-a-qpcr-data-analysis-workflow/developing-a-qpcr-data-analysis-workflow_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-27-czid-r-analysis-demo/",
    "title": "CZID-to-R data import and analysis demo",
    "description": "Demonstration of how to import taxonomic profiles from CZID into R and do a few basic analyses.",
    "author": [
      {
        "name": "Michael R. McLaren",
        "url": {}
      }
    ],
    "date": "2022-10-31",
    "categories": [
      "CZ ID",
      "hjelmso2019meta",
      "R"
    ],
    "contents": "\n\nContents\nR setup\nData import\nFrom a BIOM file (Hjelmsø et al. (2019) data)\nCreate a phyloseq object\n\n\nBasic data checks and stats\nTaxonomy\n\nAnalysis\nRecreating Figure 4 from Rothman et al. (2021) using the Hjelmso data\nBray-Curtis NMDS ordination (Panel A)\nAlpha diversity (Panel B)\nRelative abundances (Proportions) (Panel C)\nPut the panels together\n\n\n\nR setup\nStart by loading some useful R packages,\n\n\n# set of data analysis and viz tools\nlibrary(tidyverse)\n\n# file system helpers\nlibrary(fs)\n\n# specifying locations within a project\nlibrary(here)\n\n# microbiome analysis helpers\nlibrary(biomformat)\nlibrary(speedyseq)\n\n# plotting helpers\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\ntheme_set(theme_cowplot())\n\n\nHere I’ll work with the BIOM file generated from the Hjelmsø et al. (2019) taxonomic profiles.\n\n\nhjelmso_data_path <- here(\"_data/hjelmso2019meta/czid\")\ndir_ls(hjelmso_data_path) %>% path_file\n\n[1] \"2022-11-07_combined_microbiome_file_nt_r.biom\"      \n[2] \"2022-11-07_combined_microbiome_file_nt_r_fixed.biom\"\n\nData import\nFrom a BIOM file (Hjelmsø et al. (2019) data)\nThe BIOM format (https://biom-format.org/, McDonald et al. (2012)) is a file format for including the abundance matrix, taxonomy, and sample metadata all in one file.\nBIOM export from CZID is supported but listed as being in Beta.\nIf we try reading in the file as directly exported from CZID, we get an error\n\n\nbiom <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  read_biom()\n\nError in validObject(.Object): invalid class \"biom\" object: type field has unsupported value\n\n\n\n\n\nThis error arises because the ‘type’ of the data object defined in the JSON-formatted contents of the .biom file isn’t valid as per the biom format v1.0 specs, see https://biom-format.org/documentation/format_versions/biom-1.0.html.\nWe can see this by opening up the file and looking for the type argument towards the beginning; or looking at the top items in the list after reading in the file with a JSON parser.\n\n\nbiom_json <- path(hjelmso_data_path, \n  '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  jsonlite::read_json()\nbiom_json %>% head(9)\n\n$id\n[1] \"None\"\n\n$format\n[1] \"Biological Observation Matrix 1.0.0\"\n\n$format_url\n[1] \"http://biom-format.org\"\n\n$matrix_type\n[1] \"sparse\"\n\n$generated_by\n[1] \"BIOM-Format 2.1.12\"\n\n$date\n[1] \"2022-11-07T20:37:42.385642\"\n\n$type\n[1] \"Table\"\n\n$matrix_element_type\n[1] \"float\"\n\n$shape\n$shape[[1]]\n[1] 25495\n\n$shape[[2]]\n[1] 85\n\nWe can fix the file by changing the type from ‘Table’ to something valid.\nIt doesn’t actually matter what we use:\n\nWhile type is a required entry in BIOM tables, the BIOM format itself does not change for different data types (e.g., OTU Table, function table, metabolite table). This information is included to allow tools that use BIOM files to determine the data type, if desired. (Caption for Additional file 5 in McDonald et al. (2012))\n\nLet’s use ‘Taxon table’.\nThe following code chunk should do the trick but is very slow, apparently because the jsonlite package is slow to work with large lists/files.\n\n\nbiom_json$type <- 'Taxon table'\njsonlite::write_json(\n  biom_json,\n  path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom')\n)\n\n\nSo I’ll instead simply replace the offending text.\n\n\nbiom_text <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r.biom') %>%\n  read_file\nstopifnot( identical(biom_text %>% str_count('\"Table\"'), 1L) )\nbiom_text_fixed <- biom_text %>%\n  str_replace('\"Table\"', '\"Taxon table\"')\nwrite_file(\n  biom_text_fixed,\n  path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom')\n)\n\n\nThe above chunk reads in the BIOM file’s contents as a single string, checks that ‘“Table”’ appears only once (in the field where it is set as the type), then replaces it with ‘“Taxon table”’), then writes the string as a new BIOM file.\nWe should now be able to load the corrected BIOM file with the biomformat package,\n\n\nbiom <- path(hjelmso_data_path, '2022-11-07_combined_microbiome_file_nt_r_fixed.biom') %>%\n  read_biom() %>%\n  print\n\nbiom object. \ntype: Taxon table \nmatrix_type: sparse \n25495 rows and 85 columns \n\nNote: For most objects in R, the print() and glimpse() methods silently return the object as well as printing information about it.\nAdding a print or glimpse call at the end of a variable-assignment pipe chain is a succinct way to save an object and show some info about it.\nCreate a phyloseq object\nThe abundance (count) matrix, sample metadata table, and taxonomy table can be extracted with three corresponding functions functions from the biomformat package.\nWe’ll tackle these one at a time.\nFirst, the abundance matrix.\n\n\nabun <- biom %>% biom_data()\nabun %>% class\n\n[1] \"dgCMatrix\"\nattr(,\"package\")\n[1] \"Matrix\"\n\nabun %>% dim\n\n[1] 25495    85\n\nThe abundance matrix is stored as a sparse matrix from the Matrix package.\nThat is fine for now, though phyloseq will want a standard (dense) matrix.\nNext, the sample metadata.\n\n\nsam <- biom %>% sample_metadata()\nsam %>% class\n\n[1] \"data.frame\"\n\nsam %>% head\n\n                      sample_type nucleotide_type collection_date\nERR3026532:288969 Airplane sewage             RNA         2013-01\nERR3026500:288937 Airplane sewage             RNA         2013-01\nERR3026576:289013 Airplane sewage             RNA         2013-01\nERR3026559:288996 Airplane sewage             RNA         2013-01\nERR3026571:289008 Airplane sewage             RNA         2013-01\nERR3026512:288949 Airplane sewage             RNA         2013-01\n                  water_control collection_location isolate\nERR3026532:288969            No            Pakistan      No\nERR3026500:288937            No               China      No\nERR3026576:289013            No             Denmark      No\nERR3026559:288996            No              Canada      No\nERR3026571:289008            No             Denmark      No\nERR3026512:288949            No               Japan      No\n                   Study Sample Name\nERR3026532:288969      Islamabad_2_c\nERR3026500:288937        Beijing_2_e\nERR3026576:289013    Library_blank_a\nERR3026559:288996          Toronto_e\nERR3026571:289008 Negative_control_a\nERR3026512:288949          Tokyo_1_d\n\nsam %>% glimpse\n\nRows: 85\nColumns: 7\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ `Study Sample Name` <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n\nThe sample metadata is a standard data frame with rownames giving sample ids, and the taxonomy information is stored as a list.\nNotice how all the variable names are in snake case except for one.\nThis is apparently because the CZID BIOM exports its own standard variables as snake case (though shows them otherwise in the online interface), but leaves custom variables unchanged.\nIt is convinient to standardize all variable names to snake case; an easy way to do this is with the function janitor::clean_names().\n\n\nsam <- sam %>%\n  janitor::clean_names() %>%\n  glimpse\n\nRows: 85\nColumns: 7\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ study_sample_name   <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n\nNext, the taxonomy table, or what the biomformat package calls the ‘observation metadata’.\n\n\ntax <- biom %>% observation_metadata()\ntax %>% class\n\n[1] \"list\"\n\ntax %>% head(2)\n\n$`Bacteria;;Proteobacteria;Alphaproteobacteria;Rhizobiales;Xanthobacteraceae;Azorhizobium;Azorhizobium caulinodans`\n                 taxonomy1                  taxonomy2 \n                \"Bacteria\"                         \"\" \n                 taxonomy3                  taxonomy4 \n          \"Proteobacteria\"      \"Alphaproteobacteria\" \n                 taxonomy5                  taxonomy6 \n             \"Rhizobiales\"        \"Xanthobacteraceae\" \n                 taxonomy7                  taxonomy8 \n            \"Azorhizobium\" \"Azorhizobium caulinodans\" \n\n$`Bacteria;;Proteobacteria;Gammaproteobacteria;Enterobacterales;Erwiniaceae;Buchnera;Buchnera aphidicola`\n            taxonomy1             taxonomy2             taxonomy3 \n           \"Bacteria\"                    \"\"      \"Proteobacteria\" \n            taxonomy4             taxonomy5             taxonomy6 \n\"Gammaproteobacteria\"    \"Enterobacterales\"         \"Erwiniaceae\" \n            taxonomy7             taxonomy8 \n           \"Buchnera\" \"Buchnera aphidicola\" \n\nWe can see that here we have a list, with one element per taxon.\nThe documentation for biomformat::observation_metadata indicates that this function may return a ‘data.frame’ rather than a list, if it is able to, but does not say under what conditions that will be the case.\nUltimately we want a data frame (or tibble).\nThe following code chunk checks which we have, and if we have a list, tries to turn it into a data frame by spreading out the taxonomy vector of each list element into a table.\n\n\ntax_tmp <- biom %>% observation_metadata()\nif (is.data.frame(tax_tmp)) {\n  tax <- tax_tmp %>% as_tibble(rownames = '.otu')\n} else {\n  tax <- tax_tmp %>% \n    enframe(name = 'feature_id') %>% \n    unnest_wider(value)\n}\nrm(tax_tmp)\ntax %>% glimpse\n\nRows: 25,495\nColumns: 9\n$ feature_id <chr> \"Bacteria;;Proteobacteria;Alphaproteobacteria;Rhi…\n$ taxonomy1  <chr> \"Bacteria\", \"Bacteria\", \"Bacteria\", \"Bacteria\", \"…\n$ taxonomy2  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ taxonomy3  <chr> \"Proteobacteria\", \"Proteobacteria\", \"Actinobacter…\n$ taxonomy4  <chr> \"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Ac…\n$ taxonomy5  <chr> \"Rhizobiales\", \"Enterobacterales\", \"Micrococcales…\n$ taxonomy6  <chr> \"Xanthobacteraceae\", \"Erwiniaceae\", \"Cellulomonad…\n$ taxonomy7  <chr> \"Azorhizobium\", \"Buchnera\", \"Cellulomonas\", \"Dict…\n$ taxonomy8  <chr> \"Azorhizobium caulinodans\", \"Buchnera aphidicola\"…\n\nTo facilitate downstream analysis, it is helpful to so some cleanup:\nReplace the taxonomic ranks with the standard NCBI rank names (see an example NCBI taxonomic record)\nIn cases where the rank is missing/unassigned, replace the empty string with NA\n\n\n\nrnks <- c('superkingdom', 'kingdom', 'phylum', 'class', 'order', 'family',\n  'genus', 'species')\ncolnames(tax)[2:9] <- rnks\n# use NA for missing ranks\ntax <- tax %>%\n  mutate(\n    across(everything(), ~ifelse(. == \"\", NA_character_, .))\n  )\ntax %>% glimpse\n\nRows: 25,495\nColumns: 9\n$ feature_id   <chr> \"Bacteria;;Proteobacteria;Alphaproteobacteria;R…\n$ superkingdom <chr> \"Bacteria\", \"Bacteria\", \"Bacteria\", \"Bacteria\",…\n$ kingdom      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ phylum       <chr> \"Proteobacteria\", \"Proteobacteria\", \"Actinobact…\n$ class        <chr> \"Alphaproteobacteria\", \"Gammaproteobacteria\", \"…\n$ order        <chr> \"Rhizobiales\", \"Enterobacterales\", \"Micrococcal…\n$ family       <chr> \"Xanthobacteraceae\", \"Erwiniaceae\", \"Cellulomon…\n$ genus        <chr> \"Azorhizobium\", \"Buchnera\", \"Cellulomonas\", \"Di…\n$ species      <chr> \"Azorhizobium caulinodans\", \"Buchnera aphidicol…\n\nNow let’s import all three tables into a single phyloseq object.\nThis involves converting each individual table into the corresponding class from the phyloseq package, and then combiningg these into one phyloseq-class object.\n\n\nps <- phyloseq(\n  otu_table(abun %>% as.matrix, taxa_are_rows = TRUE),\n  sample_data(sam),\n  tax_table(tax)\n)\n\n\nNote that we had to first coerce the abundance matrix to a standard dense matrix; we also needed to tell phyloseq that taxa corresponded to rows in the matrix.\nBasic data checks and stats\nTODO: explain below\n\n\nps <- ps %>%\n  mutate_sample_data(., \n    sample_sum = sample_sums(.)\n  )\nsam <- ps %>% sample_data %>% as_tibble\ntax <- ps %>% tax_table %>% as_tibble\n\n\n\n\nps %>% t\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 25495 taxa and 85 samples ]:\nsample_data() Sample Data:        [ 85 samples by 8 sample variables ]:\ntax_table()   Taxonomy Table:     [ 25495 taxa by 8 taxonomic ranks ]:\ntaxa are columns\n\n\n\nps %>% sample_names %>% head\n\n[1] \"ERR3026532:288969\" \"ERR3026500:288937\" \"ERR3026576:289013\"\n[4] \"ERR3026559:288996\" \"ERR3026571:289008\" \"ERR3026512:288949\"\n\nps %>% sample_data %>% glimpse\n\nRows: 85\nColumns: 8\n$ sample_type         <chr> \"Airplane sewage\", \"Airplane sewage\", \"A…\n$ nucleotide_type     <chr> \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ collection_date     <chr> \"2013-01\", \"2013-01\", \"2013-01\", \"2013-0…\n$ water_control       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ collection_location <chr> \"Pakistan\", \"China\", \"Denmark\", \"Canada\"…\n$ isolate             <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ study_sample_name   <chr> \"Islamabad_2_c\", \"Beijing_2_e\", \"Library…\n$ sample_sum          <dbl> 83328, 188401, 313204, 234338, 767843, 5…\n\n\n\nsam %>%\n  ggplot(aes(sample_sum, fill = collection_location)) +\n  scale_x_log10() +\n  geom_histogram()\n\n\n\n\n\ntaxon_stats <- ps %>%\n  as_tibble %>%\n  mutate(across(superkingdom, fct_explicit_na)) %>%\n  with_groups(c(.otu, superkingdom), summarize, \n    prev_1 = sum(.abundance >= 1),\n    prev_10 = sum(.abundance >= 10),\n    total = sum(.abundance),\n    proportion = mean(.abundance / sample_sum)\n  )\n\n\n\n\ntaxon_stats %>%\n  pivot_longer(-c(.otu, superkingdom)) %>%\n  ggplot(aes(value, fill = superkingdom)) +\n  facet_wrap(~name, scales = 'free') +\n  scale_x_log10() +\n  scale_fill_brewer(type = 'qual') +\n  geom_histogram() \n\n\n\nTaxonomy\nNCBI taxonomy has recently received changes in some prokaryotic phylum names.\nLet’s check to see which version of phylum names are being used here, by seeing whether a Bacteroides species’ phylum is listed as ‘Bacteroidetes’ (old name) or ‘Bacteroidota’ (new name).\n\n\ntax %>%\n  filter(genus == 'Bacteroides') %>%\n  slice(1)%>%\n  glimpse\n\nRows: 1\nColumns: 9\n$ .otu         <chr> \"Bacteria;;Bacteroidetes;Bacteroidia;Bacteroida…\n$ superkingdom <chr> \"Bacteria\"\n$ kingdom      <chr> NA\n$ phylum       <chr> \"Bacteroidetes\"\n$ class        <chr> \"Bacteroidia\"\n$ order        <chr> \"Bacteroidales\"\n$ family       <chr> \"Bacteroidaceae\"\n$ genus        <chr> \"Bacteroides\"\n$ species      <chr> \"Bacteroides fragilis\"\n\nIf we look at this taxon in NCBI taxonomy, we can see that NCBI has adopted the new phylum name ‘Bacteroidota’; however, here we see the old phylum name.\nThis suggests that CZID is currently using an older version of NCBI prior to the name change.\nsee\n- https://ncbiinsights.ncbi.nlm.nih.gov/2021/12/10/ncbi-taxonomy-prokaryote-phyla-added/\n- https://www.the-scientist.com/news-opinion/newly-renamed-prokaryote-phyla-cause-uproar-69578\n\nCheck classification percentages\n\n\ntax %>%\n  pivot_longer(-.otu, names_to = 'rank') %>%\n  with_groups(rank, summarize,\n    features_classified = sum(!is.na(value)),\n    features_total = n()\n  ) %>%\n  mutate(\n    frac_classified = features_classified / features_total,\n    rank = factor(rank, rank_names(ps))\n  ) %>%\n  arrange(rank)\n\n# A tibble: 8 × 4\n  rank         features_classified features_total frac_classified\n  <fct>                      <int>          <int>           <dbl>\n1 superkingdom               25381          25495           0.996\n2 kingdom                     4995          25495           0.196\n3 phylum                     22667          25495           0.889\n4 class                      22119          25495           0.868\n5 order                      22199          25495           0.871\n6 family                     21744          25495           0.853\n7 genus                      20649          25495           0.810\n8 species                    25493          25495           1.00 \n\nThis analysis points to some notable features of the data.\nFor example, not every taxonomic feature has a superkingdom.\nLet’s take a look at some of those ‘species’ that don’t,\n\n\nset.seed(42)\ntax %>%\n  filter(is.na(superkingdom)) %>%\n  select(superkingdom, kingdom, genus, species) %>%\n  slice_sample(n=20)\n\n# A tibble: 20 × 4\n   superkingdom kingdom genus species                                 \n   <chr>        <chr>   <chr> <chr>                                   \n 1 <NA>         <NA>    <NA>  Cloning vector pPKm-230                 \n 2 <NA>         <NA>    <NA>  Cloning vector pMT450                   \n 3 <NA>         <NA>    <NA>  Vector pAAV-hSyn1-FLEX-GAP43-GCaMP6s-P2…\n 4 <NA>         <NA>    <NA>  uncultured microorganism                \n 5 <NA>         <NA>    <NA>  Cloning vector shRNA EYFP-P2A Puro      \n 6 <NA>         <NA>    <NA>  Cloning vector pMT449                   \n 7 <NA>         <NA>    <NA>  IncQ plasmid pIE1120                    \n 8 <NA>         <NA>    <NA>  Cloning vector pHal7-FAPG462VRFP        \n 9 <NA>         <NA>    <NA>  Shuttle vector pG106                    \n10 <NA>         <NA>    <NA>  uncultured gut microbe of Zootermopsis …\n11 <NA>         <NA>    <NA>  Vector EP-Pol                           \n12 <NA>         <NA>    <NA>  Cloning vector pRGPDuo4                 \n13 <NA>         <NA>    <NA>  Cloning vector IA5_YQR_DIMER            \n14 <NA>         <NA>    <NA>  Plasmid pMCBF1                          \n15 <NA>         <NA>    <NA>  Plasmid pM3                             \n16 <NA>         <NA>    <NA>  uncultured marine organism              \n17 <NA>         <NA>    <NA>  Transposon Tn4551                       \n18 <NA>         <NA>    <NA>  Sphinx1.76-related DNA                  \n19 <NA>         <NA>    <NA>  Cloning vector pMT451                   \n20 <NA>         <NA>    <NA>  uncultured marine microorganism         \n\nCan see that CZID report reference sequences that are in NT but don’t corresopnd to known organisms.\nWhat about ‘species’ without intermediate ranks?\n\n\ntax %>%\n  filter(!is.na(superkingdom), is.na(family)) %>%\n  select(superkingdom, kingdom, phylum, family, genus, species) %>%\n  slice_sample(n=20)\n\n# A tibble: 20 × 6\n   superkingdom kingdom phylum          family genus           species\n   <chr>        <chr>   <chr>           <chr>  <chr>           <chr>  \n 1 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n 2 Bacteria     <NA>    <NA>            <NA>   <NA>            swine …\n 3 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n 4 Bacteria     <NA>    <NA>            <NA>   <NA>            butyra…\n 5 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n 6 Bacteria     <NA>    Chloroflexi     <NA>   Dehalogenimonas Dehalo…\n 7 Bacteria     <NA>    Proteobacteria  <NA>   <NA>            alpha …\n 8 Eukaryota    <NA>    Bacillariophyta <NA>   <NA>            uncult…\n 9 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n10 Bacteria     <NA>    Actinobacteria  <NA>   <NA>            actino…\n11 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n12 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n13 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n14 Bacteria     <NA>    Proteobacteria  <NA>   <NA>            arseni…\n15 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n16 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n17 Bacteria     <NA>    <NA>            <NA>   <NA>            bacter…\n18 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n19 Archaea      <NA>    <NA>            <NA>   <NA>            uncult…\n20 Bacteria     <NA>    <NA>            <NA>   <NA>            uncult…\n\nCan also see cases where a species does not have an intermediate rank defined, such as family.\nHaving NA for intermediate ranks could cause issues, and we might consider replacing these with a new string such as ‘Enterobacterales_unclassified’.\nHow do features break down by superkingdom?\n\n\ntax %>% \n  count(superkingdom) %>%\n  mutate(fraction = n / sum(n))\n\n# A tibble: 5 × 3\n  superkingdom     n fraction\n  <chr>        <int>    <dbl>\n1 Archaea        391  0.0153 \n2 Bacteria     16583  0.650  \n3 Eukaryota     5655  0.222  \n4 Viruses       2752  0.108  \n5 <NA>           114  0.00447\n\nAnalysis\nRecreating Figure 4 from Rothman et al. (2021) using the Hjelmso data\nFigure 4 from Rothman et al. (2021) combines three common types of plots in microbiome analysis: An ordination plot to visualize the similarities and differences between samples, a plot showing the distribution of an alpha diversity metric (Shannon index) across samples, and the proportions (relative abundance) of particular species across samples (faceted by species).\nHere I’ll show how to (mostly) recreate this plot using the Hjelmso data.\nFirst, we’ll filter out some samples and taxa, which is a typical first step to any analysis.\nThere is a lot more to say about how you might do said filtering; but here I’ll\nRemove samples with very low read counts, since the low read counts can be a sign of experimental issues with those samples and can skew interpretation of some analyses\nSubset to just viruses (the Rothman analysis only considers viruses)\nRemove species not appearing in at least 2 samples and 10 reads, which will speed up calculations and likely make our results more meaningful since these identifications can easily be spurious.\nAggregate to the genus level\n\n\n\nps_plot <- ps %>%\n  filter_sample_data(sample_sum > 1e5) %>%\n  filter_tax_table(superkingdom == 'Viruses') %>%\n  filter_taxa2(~ sum(. > 0) > 2 & sum(.) >= 10) %>%\n  tax_glom('genus', NArm = TRUE) %>%\n  print\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:          [ 110 taxa and 84 samples ]:\nsample_data() Sample Data:        [ 84 samples by 8 sample variables ]:\ntax_table()   Taxonomy Table:     [ 110 taxa by 8 taxonomic ranks ]:\ntaxa are rows\n\nNow there are only 110 species being considered, compared to 25495 in the entire CZID output.\nNote that taxa with a missing genus name have been filtered out; this is the phyloseq default but it can have big effects so need to be aware and consider whether this is desired for a given analysis.\nNote, the names of the taxonomic features after aggregation are set to the name of the most abundant feature within the genus; they are not automatically set to the genus name.\nThat is easy to do manually, provided that you have unique genus names (which is not always the case).\nIt can be useful to check if the genus names are unique for plotting by genus later on,\n\n\nps_plot %>% tax_table %>% as_tibble %>% pull(genus) %>% anyDuplicated\n\n[1] 0\n\nThere are no duplicates, so we can uniquely refer to taxa by genus name.\nBray-Curtis NMDS ordination (Panel A)\nThere are many ways to do this; here I’ll use the ordinate() and plot_ordination() helper function from phyloseq to create the NMDS plot using the Bray-Curtis community dissimilarity metric.\nNote, that it is important to manually normalize the abundances to have the same total in each sample (e.g. by normalizing to proportions, as done here), otherwise the different total counts across samples will affect the results.\n\n\nnmds <- ps_plot %>%\n  transform_sample_counts(~ . / sum(.)) %>%\n  ordinate(method = \"NMDS\", distance = \"bray\", trymax = 50)\n\nRun 0 stress 0.1002992 \nRun 1 stress 0.1173969 \nRun 2 stress 0.1480709 \nRun 3 stress 0.1173968 \nRun 4 stress 0.1002992 \n... Procrustes: rmse 3.717243e-06  max resid 1.691263e-05 \n... Similar to previous best\nRun 5 stress 0.1424676 \nRun 6 stress 0.130377 \nRun 7 stress 0.1002992 \n... Procrustes: rmse 2.661887e-06  max resid 1.175533e-05 \n... Similar to previous best\nRun 8 stress 0.1002992 \n... Procrustes: rmse 1.104143e-05  max resid 6.17019e-05 \n... Similar to previous best\nRun 9 stress 0.1173976 \nRun 10 stress 0.1173975 \nRun 11 stress 0.1292318 \nRun 12 stress 0.1294527 \nRun 13 stress 0.1002992 \n... New best solution\n... Procrustes: rmse 5.355197e-06  max resid 1.53618e-05 \n... Similar to previous best\nRun 14 stress 0.1629023 \nRun 15 stress 0.117396 \nRun 16 stress 0.1417549 \nRun 17 stress 0.1417084 \nRun 18 stress 0.1002992 \n... Procrustes: rmse 1.162339e-05  max resid 7.001561e-05 \n... Similar to previous best\nRun 19 stress 0.1002992 \n... Procrustes: rmse 7.305224e-06  max resid 4.166893e-05 \n... Similar to previous best\nRun 20 stress 0.1292315 \n*** Best solution repeated 3 times\n\np_ord <- plot_ordination(ps_plot, nmds, \n  color = \"collection_location\", type = \"samples\"\n) +\n  labs(color = 'Country')\np_ord \n\n\n\nAlpha diversity (Panel B)\nWe can compute Shannon alpha diversity index for each sample in a variety of ways:\nphyloseq::estimate_richness()\nvegan::diversity()\nPerforming the calculation ourselves from the definition\n\n\n\nshannon_index <- otu_table(ps_plot) %>% \n  orient_taxa(as = 'cols') %>%\n  vegan::diversity()\nshannon_index %>% head\n\nERR3026500:288937 ERR3026576:289013 ERR3026559:288996 \n        1.3500820         0.6443310         1.3777211 \nERR3026571:289008 ERR3026512:288949 ERR3026526:288963 \n        0.9189514         2.0592347         1.6738584 \n\nNote that we needed to reorient the abundance matrix (i.e. OTU table) to have taxa corresponding to columns, as this is what functions in the vegan package expect.\nWe can tell that we used the correct orientation because the resulting diversity values are in a named vector where the names correspond to the sample names.\nIf we had passed the matrix in the incorrect orientation, then the vector names would be the taxa names.\nLet’s add the Shannon index to a copy of the sample data,\n\n\nsam_plot <- ps_plot %>% sample_data %>% as_tibble %>%\n  add_column(shannon_index = shannon_index)\n\n\nthen create the plot,\n\n\np_div <- sam_plot %>%\n  ggplot(aes(y = shannon_index, x = collection_location, \n      color = collection_location)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_quasirandom() +\n  expand_limits(y = 0) +\n  # scale_color_manual(values = colors_countries) +\n  labs(x = 'Country', y = 'Shannon index') +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    legend.position = 'none'\n  )\np_div\n\n\n\nNote, I’m plotting the data points over the box plots, since it is useful to see the scatter when we’re working with a relatively small number of points like this.\nI therefore turned off the plotting of outliers in the boxplot layer.\nNote, I suggest using the exponential of the Shannon index and plotting on a log scale), so that the numbers shown on the axis are in terms of effective number of species.\nRelative abundances (Proportions) (Panel C)\nIn the actual Rothman figure, the abundances for a set of viruses are shown; the particular viruses were picked based on an analysis to determine viruses that vary across treatment plant, using the ANCOM R package.\nI may do that in a future version of this script, but for now I’ll just pick the 10 most abundant viruses by average proportion.\nFirst, get a data frame for plotting, with the proportions of all taxa alongside the original read counts,\n\n\nx <- ps_plot %>%\n  as_tibble %>%\n  with_groups(.sample, mutate,\n    proportion = .abundance / sum(.abundance))\n\n\nNext, get the top 10 viruses by median proportion. We can do this various ways, e.g.\n\n\ntop_viruses1 <- ps_plot %>% \n  transform_sample_counts(~ . / sum(.)) %>%\n  orient_taxa(as = 'rows') %>%\n  otu_table %>%\n  apply(1, median) %>%\n  sort(decreasing = TRUE) %>%\n  head(10) %>%\n  names\n\n\nor\n\n\ntop_viruses2 <- x %>%\n  with_groups(.otu, summarize, across(proportion, median)) %>%\n  slice_max(proportion, n = 10) %>%\n  print %>%\n  pull(.otu)\n\n# A tibble: 10 × 2\n   .otu                                                        propo…¹\n   <chr>                                                         <dbl>\n 1 Viruses;Orthornavirae;Kitrinoviricota;Alsuviricetes;Martel… 0.282  \n 2 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.0821 \n 3 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Siphoviri… 0.0456 \n 4 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Siphoviri… 0.0127 \n 5 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Herellevi… 0.00844\n 6 Viruses;Orthornavirae;Pisuviricota;Duplopiviricetes;Durnav… 0.00821\n 7 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.00500\n 8 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Podovirid… 0.00478\n 9 Viruses;;Uroviricota;Caudoviricetes;Caudovirales;Myovirida… 0.00353\n10 Viruses;Orthornavirae;Pisuviricota;Pisoniviricetes;Picorna… 0.00323\n# … with abbreviated variable name ¹​proportion\n\nidentical(top_viruses1, top_viruses2)\n\n[1] TRUE\n\n\n\np_prop <- x %>%\n  filter(.otu %in% top_viruses1) %>%\n  mutate(\n    across(genus, fct_reorder, proportion, .fun = median, .desc = TRUE),\n  ) %>%\n  ggplot(aes(x = collection_location, y = proportion,\n      color = collection_location)) +\n  facet_wrap(~genus, nrow = 2, scales = 'free_y') +\n  # scale_y_log10() +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(y = 'Proportion', x = 'Country') +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    legend.position = 'none'\n  )\np_prop\n\n\n\nNote, I’ve ordered the facets as decreasing in median proportion.\nPut the panels together\nPutting multiple panels together is often very easy with the patchwork package loaded,\n\n\n(p_ord + p_div) / p_prop +\n  plot_annotation(tag_levels = 'A')\n\n\n\nThis plot could definitely benefit from some extra fiddling, to adjust the spacing and colors etc.\n\n\n\nHjelmsø, Mathis Hjort, Sarah Mollerup, Randi Holm Jensen, Carlotta Pietroni, Oksana Lukjancenko, Anna Charlotte Schultz, Frank M. Aarestrup, and Anders Johannes Hansen. 2019. “Metagenomic Analysis of Viruses in Toilet Waste from Long Distance Flights New Procedure for Global Infectious Disease Surveillance.” PLOS ONE 14 (1): e0210368. https://doi.org/10.1371/journal.pone.0210368.\n\n\nMcDonald, Daniel, Jose C Clemente, Justin Kuczynski, Jai Ram Rideout, Jesse Stombaugh, Doug Wendel, Andreas Wilke, et al. 2012. “The Biological Observation Matrix (BIOM) Format or: How I Learned to Stop Worrying and Love the Ome-Ome.” GigaScience 1 (1): 2047-217X-1-7. https://doi.org/10.1186/2047-217X-1-7.\n\n\nRothman, Jason A., Theresa B. Loveless, Joseph Kapcia, Eric D. Adams, Joshua A. Steele, Amity G. Zimmer-Faust, Kylie Langlois, et al. 2021. “RNA Viromics of Southern California Wastewater and Detection of SARS-CoV-2 Single-Nucleotide Variants.” Applied and Environmental Microbiology 87 (23): e01448–21. https://doi.org/10.1128/AEM.01448-21.\n\n\n\n\n",
    "preview": "posts/2022-10-27-czid-r-analysis-demo/czid-r-analysis-demo_files/figure-html5/unnamed-chunk-19-1.svg",
    "last_modified": "2023-06-24T19:40:40+00:00",
    "input_file": {}
  }
]
